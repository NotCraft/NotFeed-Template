{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-Register Projection for Headline Part of Speech Tagging. (arXiv:2109.07483v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07483","description":"<p>Part of speech (POS) tagging is a familiar NLP task. State of the art taggers\nroutinely achieve token-level accuracies of over 97% on news body text,\nevidence that the problem is well understood. However, the register of English\nnews headlines, \"headlinese\", is very different from the register of long-form\ntext, causing POS tagging models to underperform on headlines. In this work, we\nautomatically annotate news headlines with POS tags by projecting predicted\ntags from corresponding sentences in news bodies. We train a multi-domain POS\ntagger on both long-form and headline text and show that joint training on both\nregisters improves over training on just one or naively concatenating training\nsets. We evaluate on a newly-annotated corpus of over 5,248 English news\nheadlines from the Google sentence compression corpus, and show that our model\nyields a 23% relative error reduction per token and 19% per headline. In\naddition, we demonstrate that better headline POS tags can improve the\nperformance of a syntax-based open information extraction system. We make POSH,\nthe POS-tagged Headline corpus, available to encourage research in improved NLP\nmodels for news headlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benton_A/0/1/0/all/0/1\">Adrian Benton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malioutov_I/0/1/0/all/0/1\">Igor Malioutov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Euclidean and Hyperbolic Embeddings on the WordNet Nouns Hypernymy Graph. (arXiv:2109.07488v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07488","description":"<p>Nickel and Kiela (2017) present a new method for embedding tree nodes in the\nPoincare ball, and suggest that these hyperbolic embeddings are far more\neffective than Euclidean embeddings at embedding nodes in large, hierarchically\nstructured graphs like the WordNet nouns hypernymy tree. This is especially\ntrue in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek\nto reproduce their experiments on embedding and reconstructing the WordNet\nnouns hypernymy graph. Counter to what they report, we find that Euclidean\nembeddings are able to represent this tree at least as well as Poincare\nembeddings, when allowed at least 50 dimensions. We note that this does not\ndiminish the significance of their work given the impressive performance of\nhyperbolic embeddings in very low-dimensional settings. However, given the wide\ninfluence of their work, our aim here is to present an updated and more\naccurate comparison between the Euclidean and hyperbolic embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Sameer Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benton_A/0/1/0/all/0/1\">Adrian Benton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets. (arXiv:2109.07494v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07494","description":"<p>For interpreting the behavior of a probabilistic model, it is useful to\nmeasure a model's calibration--the extent to which it produces reliable\nconfidence scores. We address the open problem of calibration for tagging\nmodels with sparse tagsets, and recommend strategies to measure and reduce\ncalibration error (CE) in such models. We show that several post-hoc\nrecalibration techniques all reduce calibration error across the marginal\ndistribution for two existing sequence taggers. Moreover, we propose tag\nfrequency grouping (TFG) as a way to measure calibration error in different\nfrequency bands. Further, recalibrating each group separately promotes a more\nequitable reduction of calibration error across the tag frequency spectrum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1\">Michael Kranzlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue State Tracking with a Language Model using Schema-Driven Prompting. (arXiv:2109.07506v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07506","description":"<p>Task-oriented conversational systems often use dialogue state tracking to\nrepresent the user's intentions, which involves filling in values of\npre-defined slots. Many approaches have been proposed, often using\ntask-specific architectures with special-purpose classifiers. Recently, good\nresults have been obtained using more general architectures based on pretrained\nlanguage models. Here, we introduce a new variation of the language modeling\napproach that uses schema-driven prompting to provide task-aware history\nencoding that is used for both categorical and non-categorical slots. We\nfurther improve performance by augmenting the prompting with schema\ndescriptions, a naturally occurring source of in-domain knowledge. Our purely\ngenerative system achieves state-of-the-art performance on MultiWOZ 2.2 and\nachieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.\nThe data and code will be available at\nhttps://github.com/chiahsuan156/DST-as-Prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tied & Reduced RNN-T Decoder. (arXiv:2109.07513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07513","description":"<p>Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have\nshown that, under some conditions, it is possible to simplify its prediction\nnetwork with little or no loss in recognition accuracy (<a href=\"/abs/2003.07705\">arXiv:2003.07705</a>\n[eess.AS], [2], <a href=\"/abs/2012.06749\">arXiv:2012.06749</a> [cs.CL]). This is done by limiting the context\nsize of previous labels and/or using a simpler architecture for its layers\ninstead of LSTMs. The benefits of such changes include reduction in model size,\nfaster inference and power savings, which are all useful for on-device\napplications.\n</p>\n<p>In this work, we study ways to make the RNN-T decoder (prediction network +\njoint network) smaller and faster without degradation in recognition\nperformance. Our prediction network performs a simple weighted averaging of the\ninput embeddings, and shares its embedding matrix weights with the joint\nnetwork's output layer (a.k.a. weight tying, commonly used in language modeling\n<a href=\"/abs/1611.01462\">arXiv:1611.01462</a> [cs.LG]). This simple design, when used in conjunction with\nadditional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T\nDecoder from 23M parameters to just 2M, without affecting word-error rate\n(WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botros_R/0/1/0/all/0/1\">Rami Botros</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+David_R/0/1/0/all/0/1\">Robert David</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_E/0/1/0/all/0/1\">Emmanuel Guzman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a> (1) ((1) Google Inc. USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text as Causal Mediators: Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects. (arXiv:2109.07542v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07542","description":"<p>Using observed language to understand interpersonal interactions is important\nin high-stakes decision making. We propose a causal research design for\nobservational (non-experimental) data to estimate the natural direct and\nindirect effects of social group signals (e.g. race or gender) on speakers'\nresponses with separate aspects of language as causal mediators. We illustrate\nthe promises and challenges of this framework via a theoretical case study of\nthe effect of an advocate's gender on interruptions from justices during U.S.\nSupreme Court oral arguments. We also discuss challenges conceptualizing and\noperationalizing causal variables such as gender and language that comprise of\nmany components, and we articulate technical open challenges such as temporal\ndependence between language mediators in conversational settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rice_D/0/1/0/all/0/1\">Douglas Rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brendan O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"It doesn't look good for a date\": Transforming Critiques into Preferences for Conversational Recommendation Systems. (arXiv:2109.07576v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07576","description":"<p>Conversations aimed at determining good recommendations are iterative in\nnature. People often express their preferences in terms of a critique of the\ncurrent recommendation (e.g., \"It doesn't look good for a date\"), requiring\nsome degree of common sense for a preference to be inferred. In this work, we\npresent a method for transforming a user critique into a positive preference\n(e.g., \"I prefer more romantic\") in order to retrieve reviews pertaining to\npotentially better recommendations (e.g., \"Perfect for a romantic dinner\"). We\nleverage a large neural language model (LM) in a few-shot setting to perform\ncritique-to-preference transformation, and we test two methods for retrieving\nrecommendations: one that matches embeddings, and another that fine-tunes an LM\nfor the task. We instantiate this approach in the restaurant domain and\nevaluate it using a new dataset of restaurant critiques. In an ablation study,\nwe show that utilizing critique-to-preference transformation improves\nrecommendations, and that there are at least three general cases that explain\nthis improved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bursztyn_V/0/1/0/all/0/1\">Victor S. Bursztyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healey_J/0/1/0/all/0/1\">Jennifer Healey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_E/0/1/0/all/0/1\">Eunyee Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birnbaum_L/0/1/0/all/0/1\">Larry Birnbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An influencer-based approach to understanding radical right viral tweets. (arXiv:2109.07588v1 [cs.SI])","link":"http://arxiv.org/abs/2109.07588","description":"<p>Radical right influencers routinely use social media to spread highly\ndivisive, disruptive and anti-democratic messages. Assessing and countering the\nchallenge that such content poses is crucial for ensuring that online spaces\nremain open, safe and accessible. Previous work has paid little attention to\nunderstanding factors associated with radical right content that goes viral. We\ninvestigate this issue with a new dataset ROT which provides insight into the\ncontent, engagement and followership of a set of 35 radical right influencers.\nIt includes over 50,000 original entries and over 40 million retweets, quotes,\nreplies and mentions. We use a multilevel model to measure engagement with\ntweets, which are nested in each influencer. We show that it is crucial to\naccount for the influencer-level structure, and find evidence of the importance\nof both influencer- and content-level factors, including the number of\nfollowers each influencer has, the type of content (original posts, quotes and\nreplies), the length and toxicity of content, and whether influencers request\nretweets. We make ROT available for other researchers to use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sprejer_L/0/1/0/all/0/1\">Laila Sprejer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1\">Helen Margetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_K/0/1/0/all/0/1\">Kleber Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OSullivan_D/0/1/0/all/0/1\">David O&#x27;Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07589","description":"<p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity\ntagging in low resource domains. Existing approaches only learn class-specific\nsemantic features and intermediate representations from source domains. This\naffects generalizability to unseen target domains, resulting in suboptimal\nperformances. To this end, we present CONTaiNER, a novel contrastive learning\ntechnique that optimizes the inter-token distribution distance for Few-Shot\nNER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a\ngeneralized objective of differentiating between token categories based on\ntheir Gaussian-distributed embeddings. This effectively alleviates overfitting\nissues originating from training domains. Our experiments in several\ntraditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large\nscale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER\noutperforms previous methods by 3%-13% absolute F1 points while showing\nconsistent performance trends, even in challenging scenarios where previous\napproaches could not achieve appreciable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katiyar_A/0/1/0/all/0/1\">Arzoo Katiyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation. (arXiv:2109.07591v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07591","description":"<p>Domain adaptation of neural networks commonly relies on three training\nphases: pretraining, selected data training and then fine tuning. Data\nselection improves target domain generalization by training further on\npretraining data identified by relying on a small sample of target domain data.\nThis work examines the benefit of data selection for language modeling and\nmachine translation. Our experiments assess the complementarity of selection\nwith fine tuning and result in practical recommendations: (i) selected data\nmust be similar to the fine-tuning domain but not so much as to erode the\ncomplementary effect of fine-tuning; (ii) there is a trade-off between\nselecting little data for fast but limited progress or much data for slow but\nlong lasting progress; (iii) data selection can be applied early during\npretraining, with performance gains comparable to long pretraining session;\n(iv) data selection from domain classifiers is often more effective than the\npopular contrastive data selection method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification. (arXiv:2109.07604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07604","description":"<p>Traditional hand-crafted linguistically-informed features have often been\nused for distinguishing between translated and original non-translated texts.\nBy contrast, to date, neural architectures without manual feature engineering\nhave been less explored for this task. In this work, we (i) compare the\ntraditional feature-engineering-based approach to the feature-learning-based\none and (ii) analyse the neural architectures in order to investigate how well\nthe hand-crafted features explain the variance in the neural models'\npredictions. We use pre-trained neural word embeddings, as well as several\nend-to-end neural architectures in both monolingual and multilingual settings\nand compare them to feature-engineering-based SVM classifiers. We show that (i)\nneural architectures outperform other approaches by more than 20 accuracy\npoints, with the BERT-based model performing the best in both the monolingual\nand multilingual settings; (ii) while many individual hand-crafted\ntranslationese features correlate with neural model predictions, feature\nimportance analysis shows that the most important features for neural and\nclassical architectures differ; and (iii) our multilingual experiments provide\nempirical evidence for translationese universals across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pylypenko_D/0/1/0/all/0/1\">Daria Pylypenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amponsah_Kaakyire_K/0/1/0/all/0/1\">Kwabena Amponsah-Kaakyire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_K/0/1/0/all/0/1\">Koel Dutta Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis. (arXiv:2109.07672v1 [cs.AI])","link":"http://arxiv.org/abs/2109.07672","description":"<p>We propose an Ontology-Based Information Extraction (OBIE) system to automate\nthe extraction of the criteria and values applied in Land Use Suitability\nAnalysis (LUSA) from bylaw and regulation documents related to the geographic\narea of interest. The results obtained by our proposed LUSA OBIE system (land\nuse suitability criteria and their values) are presented as an ontology\npopulated with instances of the extracted criteria and property values. This\nlatter output ontology is incorporated into a Multi-Criteria Decision Making\n(MCDM) model applied for constructing suitability maps for different kinds of\nland uses. The resulting maps may be the final desired product or can be\nincorporated into the cellular automata urban modeling and simulation for\npredicting future urban growth. A case study has been conducted where the\noutput from LUSA OBIE is applied to help produce a suitability map for the City\nof Regina, Saskatchewan, to assist in the identification of suitable areas for\nresidential development. A set of Saskatchewan bylaw and regulation documents\nwere downloaded and input to the LUSA OBIE system. We accessed the extracted\ninformation using both the populated LUSA ontology and the set of annotated\ndocuments. In this regard, the LUSA OBIE system was effective in producing a\nfinal suitability map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Ageili_M/0/1/0/all/0/1\">Munira Al-Ageili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1\">Malek Mouhoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset. (arXiv:2109.07679v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07679","description":"<p>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the\nform of free-text is an important yet hard task in NLP. While CSKB completion\nonly fills the missing links within the domain of the CSKB, CSKB population is\nalternatively proposed with the goal of reasoning unseen assertions from\nexternal resources. In this task, CSKBs are grounded to a large-scale\neventuality (activity, state, and event) graph to discriminate whether novel\ntriples from the eventuality graph are plausible or not. However, existing\nevaluations on the population task are either not accurate (automatic\nevaluation with randomly sampled negative examples) or of small scale (human\nannotation). In this paper, we benchmark the CSKB population task with a new\nlarge-scale dataset by first aligning four popular CSKBs, and then presenting a\nhigh-quality human-annotated evaluation set to probe neural models' commonsense\nreasoning ability. We also propose a novel inductive commonsense reasoning\nmodel that reasons over graphs. Experimental results show that generalizing\ncommonsense reasoning on unseen assertions is inherently a hard task. Models\nachieving high accuracy during training perform poorly on the evaluation set,\nwith a large gap between human performance. We will make the data publicly\navailable for future contributions. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07680","description":"<p>Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razmara_J/0/1/0/all/0/1\">Jafar Razmara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-shot Multilingual Learners. (arXiv:2109.07684v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07684","description":"<p>General-purpose language models have demonstrated impressive capabilities,\nperforming on par with state-of-the-art approaches on a range of downstream\nnatural language processing (NLP) tasks and benchmarks when inferring\ninstructions from very few examples. Here, we evaluate the multilingual skills\nof the GPT and T5 models in conducting multi-class classification on\nnon-English languages without any parameter updates. We show that, given a few\nEnglish examples as context, pre-trained language models can predict not only\nEnglish test samples but also non-English ones. Finally, we find the in-context\nfew-shot cross-lingual prediction results of language models are significantly\nbetter than random prediction, and they are competitive compared to the\nexisting state-of-the-art cross-lingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1\">Jason Yosinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable Persona-Grounded Dialogues via Grounded Minimal Edits. (arXiv:2109.07713v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07713","description":"<p>Grounded dialogue models generate responses that are grounded on certain\nconcepts. Limited by the distribution of grounded dialogue data, models trained\non such data face the transferability challenges in terms of the data\ndistribution and the type of grounded concepts. To address the challenges, we\npropose the grounded minimal editing framework, which minimally edits existing\nresponses to be grounded on the given concept. Focusing on personas, we propose\nGrounded Minimal Editor (GME), which learns to edit by disentangling and\nrecombining persona-related and persona-agnostic parts of the response. To\nevaluate persona-grounded minimal editing, we present the PersonaMinEdit\ndataset, and experimental results show that GME outperforms competitive\nbaselines by a large margin. To evaluate the transferability, we experiment on\nthe test set of BlendedSkillTalk and show that GME can edit dialogue models'\nresponses to largely improve their persona consistency while preserving the use\nof knowledge and empathy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sister Help: Data Augmentation for Frame-Semantic Role Labeling. (arXiv:2109.07725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07725","description":"<p>While FrameNet is widely regarded as a rich resource of semantics in natural\nlanguage processing, a major criticism concerns its lack of coverage and the\nrelative paucity of its labeled data compared to other commonly used lexical\nresources such as PropBank and VerbNet. This paper reports on a pilot study to\naddress these gaps. We propose a data augmentation approach, which uses\nexisting frame-specific annotation to automatically annotate other lexical\nunits of the same frame which are unannotated. Our rule-based approach defines\nthe notion of a sister lexical unit and generates frame-specific augmented data\nfor training. We present experiments on frame-semantic role labeling which\ndemonstrate the importance of this data augmentation: we obtain a large\nimprovement to prior results on frame identification and argument\nidentification for FrameNet, utilizing both full-text and lexicographic\nannotations under FrameNet. Our findings on data augmentation highlight the\nvalue of automatic resource creation for improved models in frame-semantic\nparsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pancholy_A/0/1/0/all/0/1\">Ayush Pancholy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petruck_M/0/1/0/all/0/1\">Miriam R. L. Petruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07726","description":"<p>Despite being a common figure of speech, hyperbole is under-researched with\nonly a few studies addressing its identification task. In this paper, we\nintroduce a new task of hyperbole generation to transfer a literal sentence\ninto its hyperbolic paraphrase. To tackle the lack of available hyperbolic\nsentences, we construct HYPO-XL, the first large-scale hyperbole corpus\ncontaining 17,862 hyperbolic sentences in a non-trivial way. Based on our\ncorpus, we propose an unsupervised method for hyperbole generation with no need\nfor parallel literal-hyperbole pairs. During training, we fine-tune BART to\ninfill masked hyperbolic spans of sentences from HYPO-XL. During inference, we\nmask part of an input literal sentence and over-generate multiple possible\nhyperbolic versions. Then a BERT-based ranker selects the best candidate by\nhyperbolicity and paraphrase quality. Human evaluation results show that our\nmodel is capable of generating hyperbolic paraphrase sentences and outperforms\nseveral baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for Neural Machine Translation. (arXiv:2109.07740v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07740","description":"<p>We present an empirical study of scaling properties of encoder-decoder\nTransformer models used in neural machine translation (NMT). We show that\ncross-entropy loss as a function of model size follows a certain scaling law.\nSpecifically (i) We propose a formula which describes the scaling behavior of\ncross-entropy loss as a bivariate function of encoder and decoder size, and\nshow that it gives accurate predictions under a variety of scaling approaches\nand languages; we show that the total number of parameters alone is not\nsufficient for such purposes. (ii) We observe different power law exponents\nwhen scaling the decoder vs scaling the encoder, and provide recommendations\nfor optimal allocation of encoder/decoder capacity based on this observation.\n(iii) We also report that the scaling behavior of the model is acutely\ninfluenced by composition bias of the train/test sets, which we define as any\ndeviation from naturally generated text (either via machine generated or human\ntranslated text). We observe that natural text on the target side enjoys\nscaling, which manifests as successful reduction of the cross-entropy loss.\n(iv) Finally, we investigate the relationship between the cross-entropy loss\nand the quality of the generated translations. We find two different behaviors,\ndepending on the nature of the test data. For test sets which were originally\ntranslated from target language to source language, both loss and BLEU score\nimprove as model size increases. In contrast, for test sets originally\ntranslated from source language to target language, the loss improves, but the\nBLEU score stops improving after a certain threshold. We release generated text\nfrom all models used in this study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1\">Ciprian Chelba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models. (arXiv:2109.07765v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07765","description":"<p>We introduce CoWeSe (the Corpus Web Salud Espa\\~nol), the largest Spanish\nbiomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean\nplain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains\nexecuted in 2020. The corpus is openly available and already preprocessed.\nCoWeSe is an important resource for biomedical and health NLP in Spanish and\nhas already been employed to train domain-specific language models and to\nproduce word embbedings. We released the CoWeSe corpus under a Creative Commons\nAttribution 4.0 International license, both in Zenodo\n(\\url{https://zenodo.org/record/4561971\\#.YTI5SnVKiEA}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1\">Ona de Gibert Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07779","description":"<p>Researches on dialogue empathy aim to endow an agent with the capacity of\naccurate understanding and proper responding for emotions. Existing models for\nempathetic dialogue generation focus on the emotion flow in one direction, that\nis, from the context to response. We argue that conducting an empathetic\nconversation is a bidirectional process, where empathy occurs when the emotions\nof two interlocutors could converge on the same point, i.e., reaching an\nemotion consensus. Besides, we also find that the empathetic dialogue corpus is\nextremely limited, which further restricts the model performance. To address\nthe above issues, we propose a dual-generative model, Dual-Emp, to\nsimultaneously construct the emotion consensus and utilize some external\nunpaired data. Specifically, our model integrates a forward dialogue model, a\nbackward dialogue model, and a discrete latent variable representing the\nemotion consensus into a unified architecture. Then, to alleviate the\nconstraint of paired data, we extract unpaired emotional data from open-domain\nconversations and employ Dual-Emp to produce pseudo paired empathetic samples,\nwhich is more efficient and low-cost than the human annotation. Automatic and\nhuman evaluations demonstrate that our method outperforms competitive baselines\nin producing coherent and empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation by Bidirectional Training. (arXiv:2109.07780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07780","description":"<p>We present a simple and effective pretraining strategy -- bidirectional\ntraining (BiT) for neural machine translation. Specifically, we bidirectionally\nupdate the model parameters at the early stage and then tune the model\nnormally. To achieve bidirectional updating, we simply reconstruct the training\nsamples from \"src$\\rightarrow$tgt\" to \"src+tgt$\\rightarrow$tgt+src\" without any\ncomplicated model modifications. Notably, our approach does not increase any\nparameters or training steps, requiring the parallel data merely. Experimental\nresults show that BiT pushes the SOTA neural machine translation performance\nacross 15 translation tasks on 8 language pairs (data sizes range from 160K to\n38M) significantly higher. Encouragingly, our proposed model can complement\nexisting data manipulation strategies, i.e. back translation, data\ndistillation, and data diversification. Extensive analyses show that our\napproach functions as a novel bilingual code-switcher, obtaining better\nbilingual alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transductive Learning for Unsupervised Text Style Transfer. (arXiv:2109.07812v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07812","description":"<p>Unsupervised style transfer models are mainly based on an inductive learning\napproach, which represents the style as embeddings, decoder parameters, or\ndiscriminator parameters and directly applies these general rules to the test\ncases. However, the lacking of parallel corpus hinders the ability of these\ninductive learning methods on this task. As a result, it is likely to cause\nsevere inconsistent style expressions, like `the salad is rude`. To tackle this\nproblem, we propose a novel transductive learning approach in this paper, based\non a retrieval-based context-aware style representation. Specifically, an\nattentional encoder-decoder with a retriever framework is utilized. It involves\ntop-K relevant sentences in the target style in the transfer process. In this\nway, we can learn a context-aware style embedding to alleviate the above\ninconsistency problem. In this paper, both sparse (BM25) and dense retrieval\nfunctions (MIPS) are used, and two objective functions are designed to\nfacilitate joint learning. Experimental results show that our method\noutperforms several strong baselines. The proposed transductive learning\napproach is general and effective to the task of unsupervised style transfer,\nand we will apply it to the other two typical methods in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07830","description":"<p>How can model designers turn task instructions into effective prompts for\nlanguage models? Backed by extensive empirical analysis on GPT3, we observe\nimportant features for successful instructional prompts, and propose several\nreframing techniques for model designers to create such prompts. For example, a\ncomplex task can be decomposed into multiple simpler tasks. We experiment over\n12 NLP tasks across 6 diverse categories (question generation, classification,\netc.). Our results show that reframing improves few-shot learning performance\nby 14\\% while reducing sample complexity over existing few-shot baselines. The\nperformance gains are particularly important on large language models, such as\nGPT3 where tuning models or prompts on large datasets is not feasible.\nFurthermore, we observe that such gains are not limited to GPT3; the reframed\ntasks remain superior over raw instructions across different model\narchitectures, underscoring the cross-model generality of these guidelines. We\nhope these empirical-driven techniques will pave way for more effective ways to\nprompt LMs in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07833","description":"<p>Natural language inference (NLI) requires models to learn and apply\ncommonsense knowledge. These reasoning abilities are particularly important for\nexplainable NLI systems that generate a natural language explanation in\naddition to their label prediction. The integration of external knowledge has\nbeen shown to improve NLI systems, here we investigate whether it can also\nimprove their explanation capabilities. For this, we investigate different\nsources of external knowledge and evaluate the performance of our models on\nin-domain data as well as on special transfer datasets that are designed to\nassess fine-grained reasoning capabilities. We find that different sources of\nknowledge have a different effect on reasoning abilities, for example, implicit\nknowledge stored in language models can hinder reasoning on numbers and\nnegations. Finally, we conduct the largest and most fine-grained explainable\nNLI crowdsourcing study to date. It reveals that even large differences in\nautomatic performance scores do neither reflect in human ratings of label,\nexplanation, commonsense nor grammar correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsiu-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation. (arXiv:2109.07848v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07848","description":"<p>Temporary syntactic ambiguities arise when the beginning of a sentence is\ncompatible with multiple syntactic analyses. We inspect to which extent neural\nlanguage models (LMs) exhibit uncertainty over such analyses when processing\ntemporarily ambiguous inputs, and how that uncertainty is modulated by\ndisambiguating cues. We probe the LM's expectations by generating from it: we\nuse stochastic decoding to derive a set of sentence completions, and estimate\nthe probability that the LM assigns to each interpretation based on the\ndistribution of parses across completions. Unlike scoring-based methods for\ntargeted syntactic evaluation, this technique makes it possible to explore\ncompletions that are not hypothesized in advance by the researcher. We apply\nthis method to study the behavior of two LMs (GPT2 and an LSTM) on three types\nof temporary ambiguity, using materials from human sentence processing\nexperiments. We find that LMs can track multiple analyses simultaneously; the\ndegree of uncertainty varies across constructions and contexts. As a response\nto disambiguating cues, the LMs often select the correct interpretation, but\noccasional errors point to potential areas of improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aina_L/0/1/0/all/0/1\">Laura Aina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation Transformers Rediscover Inherent Data Domains. (arXiv:2109.07864v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07864","description":"<p>Many works proposed methods to improve the performance of Neural Machine\nTranslation (NMT) models in a domain/multi-domain adaptation scenario. However,\nan understanding of how NMT baselines represent text domain information\ninternally is still lacking. Here we analyze the sentence representations\nlearned by NMT Transformers and show that these explicitly include the\ninformation on text domains, even after only seeing the input sentences without\ndomains labels. Furthermore, we show that this internal information is enough\nto cluster sentences by their underlying domains without supervision. We show\nthat NMT models produce clusters better aligned to the actual domains compared\nto pre-trained language models (LMs). Notably, when computed on document-level,\nNMT cluster-to-domain correspondence nears 100%. We use these findings together\nwith an approach to NMT domain adaptation using automatically extracted\ndomains. Whereas previous work relied on external LMs for text clustering, we\npropose re-using the NMT model as a source of unsupervised clusters. We perform\nan extensive experimental study comparing two approaches across two data\nscenarios, three language pairs, and both sentence-level and document-level\nclustering, showing equal or significantly superior performance compared to\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotkova_E/0/1/0/all/0/1\">Elizaveta Korotkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07867","description":"<p>Estimating the performance of a machine learning system is a longstanding\nchallenge in artificial intelligence research. Today, this challenge is\nespecially relevant given the emergence of systems which appear to increasingly\noutperform human beings. In some cases, this \"superhuman\" performance is\nreadily demonstrated; for example by defeating legendary human players in\ntraditional two player games. On the other hand, it can be challenging to\nevaluate classification models that potentially surpass human performance.\nIndeed, human annotations are often treated as a ground truth, which implicitly\nassumes the superiority of the human over any models trained on human\nannotations. In reality, human annotators can make mistakes and be subjective.\nEvaluating the performance with respect to a genuine oracle may be more\nobjective and reliable, even when querying the oracle is expensive or\nimpossible. In this paper, we first raise the challenge of evaluating the\nperformance of both humans and models with respect to an oracle which is\nunobserved. We develop a theory for estimating the accuracy compared to the\noracle, using only imperfect human annotations for reference. Our analysis\nprovides a simple recipe for detecting and certifying superhuman performance in\nthis setting, which we believe will assist in understanding the stage of\ncurrent research on classification. We validate the convergence of the bounds\nand the assumptions of our theory on carefully designed toy experiments with\nknown oracles. Moreover, we demonstrate the utility of our theory by\nmeta-analyzing large-scale natural language processing tasks, for which an\noracle does not exist, and show that under our assumptions a number of models\nfrom recent years are with high probability superhuman.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition. (arXiv:2109.07877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07877","description":"<p>Pre-trained language models lead Named Entity Recognition (NER) into a new\nera, while some more knowledge is needed to improve their performance in\nspecific problems. In Chinese NER, character substitution is a complicated\nlinguistic phenomenon. Some Chinese characters are quite similar for sharing\nthe same components or having similar pronunciations. People replace characters\nin a named entity with similar characters to generate a new collocation but\nreferring to the same object. It becomes even more common in the Internet age\nand is often used to avoid Internet censorship or just for fun. Such character\nsubstitution is not friendly to those pre-trained language models because the\nnew collocations are occasional. As a result, it always leads to unrecognizable\nor recognition errors in the NER task. In this paper, we propose a new method,\nMulti-Feature Fusion Embedding for Chinese Named Entity Recognition (MFE-NER),\nto strengthen the language pattern of Chinese and handle the character\nsubstitution problem in Chinese Named Entity Recognition. MFE fuses semantic,\nglyph, and phonetic features together. In the glyph domain, we disassemble\nChinese characters into components to denote structure features so that\ncharacters with similar structures can have close embedding space\nrepresentation. Meanwhile, an improved phonetic system is also proposed in our\nwork, making it reasonable to calculate phonetic similarity among Chinese\ncharacters. Experiments demonstrate that our method improves the overall\nperformance of Chinese NER and especially performs well in informal language\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kui Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveying the Research on Fake News in Social Media: a Tale of Networks and Language. (arXiv:2109.07909v1 [cs.CY])","link":"http://arxiv.org/abs/2109.07909","description":"<p>The history of journalism and news diffusion is tightly coupled with the\neffort to dispel hoaxes, misinformation, propaganda, unverified rumours, poor\nreporting, and messages containing hate and divisions. With the explosive\ngrowth of online social media and billions of individuals engaged with\nconsuming, creating, and sharing news, this ancient problem has surfaced with a\nrenewed intensity threatening our democracies, public health, and news outlets\ncredibility. This has triggered many researchers to develop new methods for\nstudying, understanding, detecting, and preventing fake-news diffusion; as a\nconsequence, thousands of scientific papers have been published in a relatively\nshort period, making researchers of different disciplines to struggle in search\nof open problems and most relevant trends. The aim of this survey is threefold:\nfirst, we want to provide the researchers interested in this multidisciplinary\nand challenging area with a network-based analysis of the existing literature\nto assist them with a visual exploration of papers that can be of interest;\nsecond, we present a selection of the main results achieved so far adopting the\nnetwork as an unifying framework to represent and make sense of data, to model\ndiffusion processes, and to evaluate different debunking strategies. Finally,\nwe present an outline of the most relevant research trends focusing on the\nmoving target of fake-news, bots, and trolls identification by means of data\nmining and text technologies; despite scholars working on computational\nlinguistics and networks traditionally belong to different scientific\ncommunities, we expect that forthcoming computational approaches to prevent\nfake news from polluting the social media must be developed using hybrid and\nup-to-date methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruffo_G/0/1/0/all/0/1\">Giancarlo Ruffo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Semeraro_A/0/1/0/all/0/1\">Alfonso Semeraro</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a> (3) ((1) Universit&#xe0; degli Studi di Torino, (2) Utrecht University, (3) Universitat Polit&#xe8;cnica de Val&#xe8;ncia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07926","description":"<p>Recently more attention has been given to adversarial attacks on neural\nnetworks for natural language processing (NLP). A central research topic has\nbeen the investigation of search algorithms and search constraints, accompanied\nby benchmark algorithms and tasks. We implement an algorithm inspired by zeroth\norder optimization-based attacks and compare with the benchmark results in the\nTextAttack framework. Surprisingly, we find that optimization-based methods do\nnot yield any improvement in a constrained setup and slightly benefit from\napproximate gradient information only in unconstrained setups where search\nspaces are larger. In contrast, simple heuristics exploiting nearest neighbors\nwithout querying the target function yield substantial success rates in\nconstrained setups, and nearly full success rate in unconstrained setups, at an\norder of magnitude fewer queries. We conclude from these results that current\nTextAttack benchmark tasks are too easy and constraints are too strict,\npreventing meaningful research on black-box adversarial text attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_N/0/1/0/all/0/1\">Nathaniel Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization. (arXiv:2109.07943v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07943","description":"<p>Existing summarization systems mostly generate summaries purely relying on\nthe content of the source document. However, even for humans, we usually need\nsome references or exemplars to help us fully understand the source document\nand write summaries in a particular format. But how to find the high-quality\nexemplars and incorporate them into summarization systems is still challenging\nand worth exploring. In this paper, we propose RetrievalSum, a novel retrieval\nenhanced abstractive summarization framework consisting of a dense Retriever\nand a Summarizer. At first, several closely related exemplars are retrieved as\nsupplementary input to help the generation model understand the text more\ncomprehensively. Furthermore, retrieved exemplars can also play a role in\nguiding the model to capture the writing style of a specific corpus. We\nvalidate our method on a wide range of summarization datasets across multiple\ndomains and two backbone models: BERT and BART. Results show that our framework\nobtains significant improvement by 1.38~4.66 in ROUGE-1 score when compared\nwith the powerful pre-trained models, and achieve new state-of-the-art on\nBillSum. Human evaluation demonstrates that our retrieval enhanced model can\nbetter capture the domain-specific writing style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianqiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attribute Injection for Pretrained Language Models. (arXiv:2109.07953v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07953","description":"<p>Metadata attributes (e.g., user and product IDs from reviews) can be\nincorporated as additional inputs to neural-based NLP models, by modifying the\narchitecture of the models, in order to improve their performance. Recent\nmodels however rely on pretrained language models (PLMs), where previously used\ntechniques for attribute injection are either nontrivial or ineffective. In\nthis paper, we propose a lightweight and memory-efficient method to inject\nattributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules,\nto include attributes both independently of or jointly with the text. To limit\nthe increase of parameters especially when the attribute vocabulary is large,\nwe use low-rank approximations and hypercomplex multiplications, significantly\ndecreasing the total parameters. We also introduce training mechanisms to\nhandle domains in which attributes can be multi-labeled or sparse. Extensive\nexperiments and analyses on eight datasets from different domains show that our\nmethod outperforms previous attribute injection methods and achieves\nstate-of-the-art performance on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Unsupervised Question Answering via Summarization-Informed Question Generation. (arXiv:2109.07954v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07954","description":"<p>Question Generation (QG) is the task of generating a plausible question for a\ngiven &lt;passage, answer&gt; pair. Template-based QG uses linguistically-informed\nheuristics to transform declarative sentences into interrogatives, whereas\nsupervised QG uses existing Question Answering (QA) datasets to train a system\nto generate a question given a passage and an answer. A disadvantage of the\nheuristic approach is that the generated questions are heavily tied to their\ndeclarative counterparts. A disadvantage of the supervised approach is that\nthey are heavily tied to the domain/language of the QA dataset used as training\ndata. In order to overcome these shortcomings, we propose an unsupervised QG\nmethod which uses questions generated heuristically from summaries as a source\nof training data for a QG system. We make use of freely available news summary\ndata, transforming declarative summary sentences into appropriate questions\nusing heuristics informed by dependency parsing, named entity recognition and\nsemantic role labeling. The resulting questions are then combined with the\noriginal news articles to train an end-to-end neural QG model. We extrinsically\nevaluate our approach using unsupervised QA: our QG model is used to generate\nsynthetic QA pairs for training a QA model. Experimental results show that,\ntrained with only 20k English Wikipedia-based synthetic QA pairs, the QA model\nsubstantially outperforms previous unsupervised models on three in-domain\ndatasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain\ndatasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07958","description":"<p>We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. For example, the 6B-parameter GPT-J model\nwas 17% less truthful than its 125M-parameter counterpart. This contrasts with\nother NLP tasks, where performance improves with model size. However, this\nresult is expected if false answers are learned from the training distribution.\nWe suggest that scaling up models alone is less promising for improving\ntruthfulness than fine-tuning using training objectives other than imitation of\ntext from the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephanie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization. (arXiv:2109.07968v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07968","description":"<p>The open domain-dialogue system Alquist has a goal to conduct a coherent and\nengaging conversation that can be considered as one of the benchmarks of social\nintelligence. The fourth version of the system, developed within the Alexa\nPrize Socialbot Grand Challenge 4, brings two main innovations. The first\naddresses coherence, and the second addresses the engagingness of the\nconversation. For innovations regarding coherence, we propose a novel hybrid\napproach combining hand-designed responses and a generative model. The proposed\napproach utilizes hand-designed dialogues, out-of-domain detection, and a\nneural response generator. Hand-designed dialogues walk the user through\nhigh-quality conversational flows. The out-of-domain detection recognizes that\nthe user diverges from the predefined flow and prevents the system from\nproducing a scripted response that might not make sense for unexpected user\ninput. Finally, the neural response generator generates a response based on the\ncontext of the dialogue that correctly reacts to the unexpected user input and\nreturns the dialogue to the boundaries of hand-designed dialogues. The\ninnovations for engagement that we propose are mostly inspired by the famous\nexploration-exploitation dilemma. To conduct an engaging conversation with the\ndialogue partners, one has to learn their preferences and interests --\nexploration. Moreover, to engage the partner, we have to utilize the knowledge\nwe have already learned -- exploitation. In this work, we present the\nprinciples and inner workings of individual components of the open-domain\ndialogue system Alquist developed within the Alexa Prize Socialbot Grand\nChallenge 4 and the experiments we have conducted to evaluate them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konrad_J/0/1/0/all/0/1\">Jakub Konr&#xe1;d</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichl_J/0/1/0/all/0/1\">Jan Pichl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marek_P/0/1/0/all/0/1\">Petr Marek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ta_V/0/1/0/all/0/1\">Van Duy Ta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobza_O/0/1/0/all/0/1\">Ond&#x159;ej Kobza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hylova_L/0/1/0/all/0/1\">Lenka H&#xfd;lov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedivy_J/0/1/0/all/0/1\">Jan &#x160;ediv&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Know the Way to Rome?. (arXiv:2109.07971v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07971","description":"<p>The global geometry of language models is important for a range of\napplications, but language model probes tend to evaluate rather local\nrelations, for which ground truths are easily obtained. In this paper we\nexploit the fact that in geography, ground truths are available beyond local\nrelations. In a series of experiments, we evaluate the extent to which language\nmodel representations of city and country names are isomorphic to real-world\ngeography, e.g., if you tell a language model where Paris and Berlin are, does\nit know the way to Rome? We find that language models generally encode limited\ngeographic information, but with larger models performing the best, suggesting\nthat geographic knowledge can be induced from higher-order co-occurrence\nstatistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lietard_B/0/1/0/all/0/1\">Bastien Li&#xe9;tard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07983","description":"<p>Contrastive explanations for understanding the behavior of black box models\nhas gained a lot of attention recently as they provide potential for recourse.\nIn this paper, we propose a method Contrastive Attributed explanations for Text\n(CAT) which provides contrastive explanations for natural language text data\nwith a novel twist as we build and exploit attribute classifiers leading to\nmore semantically meaningful explanations. To ensure that our contrastive\ngenerated text has the fewest possible edits with respect to the original text,\nwhile also being fluent and close to a human generated contrastive, we resort\nto a minimal perturbation approach regularized using a BERT language model and\nattribute classifiers trained on available attributes. We show through\nqualitative examples and a user study that our method not only conveys more\ninsight because of these attributes, but also leads to better quality\n(contrastive) text. Moreover, quantitatively we show that our method is more\nefficient than other state-of-the-art methods with it also scoring higher on\nbenchmark metrics such as flip rate, (normalized) Levenstein distance, fluency\nand content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1\">Amar Prakash Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Entity Typing in Knowledge Graphs. (arXiv:2109.07990v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07990","description":"<p>Knowledge graph entity typing aims to infer entities' missing types in\nknowledge graphs which is an important but under-explored issue. This paper\nproposes a novel method for this task by utilizing entities' contextual\ninformation. Specifically, we design two inference mechanisms: i) N2T:\nindependently use each neighbor of an entity to infer its type; ii) Agg2T:\naggregate the neighbors of an entity to infer its type. Those mechanisms will\nproduce multiple inference results, and an exponentially weighted pooling\nmethod is used to generate the final inference result. Furthermore, we propose\na novel loss function to alleviate the false-negative problem during training.\nExperiments on two real-world KGs demonstrate the effectiveness of our method.\nThe source code and data of this paper can be obtained from\nhttps://github.com/CCIIPLab/CET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowMAN: Weakly Supervised Multinomial Adversarial Networks. (arXiv:2109.07994v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07994","description":"<p>The absence of labeled data for training neural models is often addressed by\nleveraging knowledge about the specific task, resulting in heuristic but noisy\nlabels. The knowledge is captured in labeling functions, which detect certain\nregularities or patterns in the training samples and annotate corresponding\nlabels for training. This process of weakly supervised training may result in\nan over-reliance on the signals captured by the labeling functions and hinder\nmodels to exploit other signals or to generalize well. We propose KnowMAN, an\nadversarial scheme that enables to control influence of signals associated with\nspecific labeling functions. KnowMAN forces the network to learn\nrepresentations that are invariant to those signals and to pick up other\nsignals that are more generally associated with an output label. KnowMAN\nstrongly improves results compared to direct weakly supervised learning with a\npre-trained transformer language model and a feature-based baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1\">Ehsaneddin Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braune_F/0/1/0/all/0/1\">Fabienne Braune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_F/0/1/0/all/0/1\">Franziska Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans System for the WMT21 Efficiency Task. (arXiv:2109.08003v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08003","description":"<p>This paper describes the NiuTrans system for the WMT21 translation efficiency\ntask (<a href=\"http://statmt.org/wmt21/efficiency-task.html\">this http URL</a>). Following last year's\nwork, we explore various techniques to improve efficiency while maintaining\ntranslation quality. We investigate the combinations of lightweight Transformer\narchitectures and knowledge distillation strategies. Also, we improve the\ntranslation efficiency with graph optimization, low precision, dynamic\nbatching, and parallel pre/post-processing. Our system can translate 247,000\nwords per second on an NVIDIA A100, being 3$\\times$ faster than last year's\nsystem. Our system is the fastest and has the lowest memory consumption on the\nGPU-throughput track. The code, model, and pipeline will be available at\nNiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yongyu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhongxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans System for WNGT 2020 Efficiency Task. (arXiv:2109.08008v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08008","description":"<p>This paper describes the submissions of the NiuTrans Team to the WNGT 2020\nEfficiency Shared Task. We focus on the efficient implementation of deep\nTransformer models \\cite{wang-etal-2019-learning, li-etal-2019-niutrans} using\nNiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP\ntasks. We explored the combination of deep encoder and shallow decoder in\nTransformer models via model compression and knowledge distillation. The neural\nmachine translation decoding also benefits from FP16 inference, attention\ncaching, dynamic batching, and batch pruning. Our systems achieve promising\nresults in both translation quality and efficiency, e.g., our fastest system\ncan translate more than 40,000 tokens per second with an RTX 2080 Ti while\nmaintaining 42.9 BLEU on \\textit{newstest2018}. The code, models, and docker\nimages are available at NiuTrans.NMT\n(https://github.com/NiuTrans/NiuTrans.NMT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08013","description":"<p>Propaganda can be defined as a form of communication that aims to influence\nthe opinions or the actions of people towards a specific goal; this is achieved\nby means of well-defined rhetorical and psychological devices. Propaganda, in\nthe form we know it today, can be dated back to the beginning of the 17th\ncentury. However, it is with the advent of the Internet and the social media\nthat it has started to spread on a much larger scale than before, thus becoming\nmajor societal and political issue. Nowadays, a large fraction of propaganda in\nsocial media is multimodal, mixing textual with visual content. With this in\nmind, here we propose a new multi-label multimodal task: detecting the type of\npropaganda techniques used in memes. We further create and release a new corpus\nof 950 memes, carefully annotated with 22 propaganda techniques, which can\nappear in the text, in the image, or in both. Our analysis of the corpus shows\nthat understanding both modalities together is essential for detecting these\ntechniques. This is further confirmed in our experiments with several\nstate-of-the-art multimodal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_B/0/1/0/all/0/1\">Bishr Bin Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Concept of Semantic Value in Social Network Analysis: an Application to Comparative Mythology. (arXiv:2109.08023v1 [cs.SI])","link":"http://arxiv.org/abs/2109.08023","description":"<p>Human sciences have traditionally relied on human reasoning and intelligence\nto infer knowledge from a wide range of sources, such as oral and written\nnarrations, reports, and traditions. Here we develop an extension of classical\nsocial network analysis approaches to incorporate the concept of meaning in\neach actor, as a mean to quantify and infer further knowledge from the original\nsource of the network. This extension is based on a new affinity function, the\nsemantic affinity, that establishes fuzzy-like relationships between the\ndifferent actors in the network, using combinations of affinity functions. We\nalso propose a new heuristic algorithm based on the shortest capacity problem\nto compute this affinity function. We use these concept of meaning and semantic\naffinity to analyze and compare the gods and heroes from three different\nclassical mythologies: Greek, Celtic and Nordic. We study the relationships of\neach individual mythology and those of common structure that is formed when we\nfuse the three of them. We show a strong connection between the Celtic and\nNordic gods and that Greeks put more emphasis on heroic characters rather than\ndeities. Our approach provides a technique to highlight and quantify important\nrelationships in the original domain of the network not deducible from its\nstructural properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fumanal_Idocin_J/0/1/0/all/0/1\">Javier Fumanal-Idocin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordon_O/0/1/0/all/0/1\">Oscar Cord&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimuro_G/0/1/0/all/0/1\">Gra&#xe7;aliz Dimuro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minarova_M/0/1/0/all/0/1\">Mar&#xed;a Min&#xe1;rov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1\">Humberto Bustince</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating Language-Specific Information in Contextualized Embeddings. (arXiv:2109.08040v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08040","description":"<p>Multilingual pretrained language models (MPLMs) exhibit multilinguality and\nare well suited for transfer across languages. Most MPLMs are trained in an\nunsupervised fashion and the relationship between their objective and\nmultilinguality is unclear. More specifically, the question whether MPLM\nrepresentations are language-agnostic or they simply interleave well with\nlearned task prediction heads arises. In this work, we locate language-specific\ninformation in MPLMs and identify its dimensionality and the layers where this\ninformation occurs. We show that language-specific information is scattered\nacross many dimensions, which can be projected into a linear subspace. Our\nstudy contributes to a better understanding of MPLM representations, going\nbeyond treating them as unanalyzable blobs of information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Error Type Annotation for Arabic. (arXiv:2109.08068v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08068","description":"<p>We present ARETA, an automatic error type annotation system for Modern\nStandard Arabic. We design ARETA to address Arabic's morphological richness and\northographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus\n(ALC) Error Tagset with some modifications. ARETA achieves a performance of\n85.8% (micro average F1 score) on a manually annotated blind test portion of\nALC. We also demonstrate ARETA's usability by applying it to a number of\nsubmissions from the QALB 2014 shared task for Arabic grammatical error\ncorrection. The resulting analyses give helpful insights on the strengths and\nweaknesses of different submissions, which is more useful than the opaque M2\nscoring metrics used in the shared task. ARETA employs a large Arabic\nmorphological analyzer, but is completely unsupervised otherwise. We make ARETA\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belkebir_R/0/1/0/all/0/1\">Riadh Belkebir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Open Information Extraction using Question Generation and Reading Comprehension. (arXiv:2109.08079v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08079","description":"<p>Typically, Open Information Extraction (OpenIE) focuses on extracting\ntriples, representing a subject, a relation, and the object of the relation.\nHowever, most of the existing techniques are based on a predefined set of\nrelations in each domain which limits their applicability to newer domains\nwhere these relations may be unknown such as financial documents. This paper\npresents a zero-shot open information extraction technique that extracts the\nentities (value) and their descriptions (key) from a sentence, using off the\nshelf machine reading comprehension (MRC) Model. The input questions to this\nmodel are created using a novel noun phrase generation method. This method\ntakes the context of the sentence into account and can create a wide variety of\nquestions making our technique domain independent. Given the questions and the\nsentence, our technique uses the MRC model to extract entities (value). The\nnoun phrase corresponding to the question, with the highest confidence, is\ntaken as the description (key).\n</p>\n<p>This paper also introduces the EDGAR10-Q dataset which is based on publicly\navailable financial documents from corporations listed in US securities and\nexchange commission (SEC). The dataset consists of paragraphs, tagged values\n(entities), and their keys (descriptions) and is one of the largest among\nentity extraction datasets. This dataset will be a valuable addition to the\nresearch community, especially in the financial domain. Finally, the paper\ndemonstrates the efficacy of the proposed technique on the EDGAR10-Q and Ade\ncorpus drug dosage datasets, where it obtained 86.84 % and 97% accuracy,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badugu_A/0/1/0/all/0/1\">Amogh Badugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tamanna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_H/0/1/0/all/0/1\">Himanshu Sharad Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection. (arXiv:2109.08113v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08113","description":"<p>Much of natural language processing is focused on leveraging large capacity\nlanguage models, typically trained over single messages with a task of\npredicting one or more tokens. However, modeling human language at\nhigher-levels of context (i.e., sequences of messages) is under-explored. In\nstance detection and other social media tasks where the goal is to predict an\nattribute of a message, we have contextual data that is loosely semantically\nconnected by authorship. Here, we introduce Message-Level Transformer (MeLT) --\na hierarchical message-encoder pre-trained over Twitter and applied to the task\nof stance prediction. We focus on stance prediction as a task benefiting from\nknowing the context of the message (i.e., the sequence of previous messages).\nThe model is trained using a variant of masked-language modeling; where instead\nof predicting tokens, it seeks to generate an entire masked (aggregated)\nmessage vector via reconstruction loss. We find that applying this pre-trained\nmasked message-level transformer to the downstream task of stance detection\nachieves F1 performance of 67%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matero_M/0/1/0/all/0/1\">Matthew Matero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Online Hate Speech through the Causal Lens. (arXiv:2109.08120v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08120","description":"<p>The societal issue of digital hostility has previously attracted a lot of\nattention. The topic counts an ample body of literature, yet remains prominent\nand challenging as ever due to its subjective nature. We posit that a better\nunderstanding of this problem will require the use of causal inference\nframeworks. This survey summarises the relevant research that revolves around\nestimations of causal effects related to online hate speech. Initially, we\nprovide an argumentation as to why re-establishing the exploration of hate\nspeech in causal terms is of the essence. Following that, we give an overview\nof the leading studies classified with respect to the direction of their\noutcomes, as well as an outline of all related research, and a summary of open\nresearch problems that can influence future work on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Founta_A/0/1/0/all/0/1\">Antigoni-Maria Founta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Tri-training of Dependency Parsers. (arXiv:2109.08122v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08122","description":"<p>We compare two orthogonal semi-supervised learning techniques, namely\ntri-training and pretrained word embeddings, in the task of dependency parsing.\nWe explore language-specific FastText and ELMo embeddings and multilingual BERT\nembeddings. We focus on a low resource scenario as semi-supervised learning can\nbe expected to have the most impact here. Based on treebank size and available\nELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and\nVietnamese. Furthermore, we include English in a simulated low-resource\nsetting. We find that pretrained word embeddings make more effective use of\nunlabelled data than tri-training but that the two approaches can be\nsuccessfully combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Summary Evaluation Survive Translation to Other Languages?. (arXiv:2109.08129v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08129","description":"<p>The creation of a large summarization quality dataset is a considerable,\nexpensive, time-consuming effort, requiring careful planning and setup. It\nincludes producing human-written and machine-generated summaries and evaluation\nof the summaries by humans, preferably by linguistic experts, and by automatic\nevaluation tools. If such effort is made in one language, it would be\nbeneficial to be able to use it in other languages. To investigate how much we\ncan trust the translation of such dataset without repeating human annotations\nin another language, we translated an existing English summarization dataset,\nSummEval dataset, to four different languages and analyzed the scores from the\nautomatic evaluation metrics in translated languages, as well as their\ncorrelation with human annotations in the source language. Our results reveal\nthat although translation changes the absolute value of automatic scores, the\nscores keep the same rank order and approximately the same correlations with\nhuman annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iskender_N/0/1/0/all/0/1\">Neslihan Iskender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polzehl_T/0/1/0/all/0/1\">Tim Polzehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase Retrieval Learns Passage Retrieval, Too. (arXiv:2109.08133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08133","description":"<p>Dense retrieval methods have shown great promise over sparse retrieval\nmethods in a range of NLP problems. Among them, dense phrase retrieval-the most\nfine-grained retrieval unit-is appealing because phrases can be directly used\nas the output for question answering and slot filling tasks. In this work, we\nfollow the intuition that retrieving phrases naturally entails retrieving\nlarger text blocks and study whether phrase retrieval can serve as the basis\nfor coarse-level retrieval including passages and documents. We first observe\nthat a dense phrase-retrieval system, without any retraining, already achieves\nbetter passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage\nretrievers, which also helps achieve superior end-to-end QA performance with\nfewer passages. Then, we provide an interpretation for why phrase-level\nsupervision helps learn better fine-grained entailment compared to\npassage-level supervision, and also show that phrase retrieval can be improved\nto achieve competitive performance in document-retrieval tasks such as entity\nlinking and knowledge-grounded dialogue. Finally, we demonstrate how phrase\nfiltering and vector quantization can reduce the size of our index by 4-10x,\nmaking dense phrase retrieval a practical and versatile solution in\nmulti-granularity retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank and the BoAT Annotation Tool. (arXiv:2002.10416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.10416","description":"<p>In this paper, we introduce the resources that we developed for Turkish\ndependency parsing, which include a novel manually annotated treebank (BOUN\nTreebank), along with the guidelines we adopted, and a new annotation tool\n(BoAT). The manual annotation process we employed was shaped and implemented by\na team of four linguists and five Natural Language Processing (NLP)\nspecialists. Decisions regarding the annotation of the BOUN Treebank were made\nin line with the Universal Dependencies (UD) framework as well as our recent\nefforts for unifying the Turkish UD treebanks through manual re-annotation. To\nthe best of our knowledge, BOUN Treebank is the largest Turkish treebank. It\ncontains a total of 9,761 sentences from various topics including biographical\ntexts, national newspapers, instructional texts, popular culture articles, and\nessays. In addition, we report the parsing results of a state-of-the-art\ndependency parser obtained over the BOUN Treebank as well as two other\ntreebanks in Turkish. Our results demonstrate that the unification of the\nTurkish annotation scheme and the introduction of a more comprehensive treebank\nlead to improved performance with regard to dependency parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turk_U/0/1/0/all/0/1\">Utku T&#xfc;rk</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Atmaca_F/0/1/0/all/0/1\">Furkan Atmaca</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ozates_S/0/1/0/all/0/1\">&#x15e;aziye Bet&#xfc;l &#xd6;zate&#x15f;</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Berk_G/0/1/0/all/0/1\">G&#xf6;zde Berk</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bedir_S/0/1/0/all/0/1\">Seyyit Talha Bedir</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Basaran_B/0/1/0/all/0/1\">Balk&#x131;z &#xd6;zt&#xfc;rk Ba&#x15f;aran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gungor_T/0/1/0/all/0/1\">Tunga G&#xfc;ng&#xf6;r</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a> (2) ((1) Department of Linguistics Bo&#x11f;azi&#xe7;i University, (2) Department of Computer Engineering Bo&#x11f;azi&#xe7;i University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regex Queries over Incomplete Knowledge Bases. (arXiv:2005.00480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00480","description":"<p>We propose the novel task of answering regular expression queries (containing\ndisjunction ($\\vee$) and Kleene plus ($+$) operators) over incomplete KBs. The\nanswer set of these queries potentially has a large number of entities, hence\nprevious works for single-hop queries in KBC that model a query as a point in\nhigh-dimensional space are not as effective. In response, we develop RotatE-Box\n-- a novel combination of RotatE and box embeddings. It can model more\nrelational inference patterns compared to existing embedding based models.\nFurthermore, we define baseline approaches for embedding based KBC models to\nhandle regex operators. We demonstrate performance of RotatE-Box on two new\nregex-query datasets introduced in this paper, including one where the queries\nare harvested based on actual user query logs. We find that our final\nRotatE-Box model significantly outperforms models based on just RotatE and just\nbox embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Parth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning for End-to-End Automatic Speech Recognition. (arXiv:2005.04288v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2005.04288","description":"<p>In this paper, we propose an incremental learning method for end-to-end\nAutomatic Speech Recognition (ASR) which enables an ASR system to perform well\non new tasks while maintaining the performance on its originally learned ones.\nTo mitigate catastrophic forgetting during incremental learning, we design a\nnovel explainability-based knowledge distillation for ASR models, which is\ncombined with a response-based knowledge distillation to maintain the original\nmodel's predictions and the \"reason\" for the predictions. Our method works\nwithout access to the training data of original tasks, which addresses the\ncases where the previous data is no longer available or joint training is\ncostly. Results on a multi-stage sequential training task show that our method\noutperforms existing ones in mitigating forgetting. Furthermore, in two\npractical scenarios, compared to the target-reference joint training method,\nthe performance drop of our method is 0.02% Character Error Rate (CER), which\nis 97% smaller than the drops of the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_L/0/1/0/all/0/1\">Li Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zi_L/0/1/0/all/0/1\">Libo Zi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengchen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04480","description":"<p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality\nEstimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven\nlanguage pairs, with human labels for up to 10,000 translations per language\npair in the following formats: sentence-level direct assessments and\npost-editing effort, and word-level good/bad labels. It also contains the\npost-edited sentences, as well as titles of the articles where the sentences\nwere extracted from, and the neural MT models used to translate the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopatina_N/0/1/0/all/0/1\">Nina Lopatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders. (arXiv:2011.01097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.01097","description":"<p>Current end-to-end approaches to Spoken Language Translation (SLT) rely on\nlimited training resources, especially for multilingual settings. On the other\nhand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on\nhigher-quality and more massive data sets. Our proposed method extends a\nMultiNMT architecture based on language-specific encoders-decoders to the task\nof Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency\nfrom MultiSLT data and it is able to translate while training only on ASR and\nMultiNMT data.\n</p>\n<p>Our experiments on four different languages show that coupling the speech\nencoder to the MultiNMT architecture produces similar quality translations\ncompared to a bilingual baseline ($\\pm 0.2$ BLEU) while effectively allowing\nfor zero-shot MultiSLT. Additionally, we propose using an Adapter module for\ncoupling the speech inputs. This Adapter module produces consistent\nimprovements up to +6 BLEU points on the proposed architecture and +1 BLEU\npoint on the end-to-end baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_C/0/1/0/all/0/1\">Carlos Segura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Wish I Would Have Loved This One, But I Didn't -- A Multilingual Dataset for Counterfactual Detection in Product Reviews. (arXiv:2104.06893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06893","description":"<p>Counterfactual statements describe events that did not or cannot take place.\nWe consider the problem of counterfactual detection (CFD) in product reviews.\nFor this purpose, we annotate a multilingual CFD dataset from Amazon product\nreviews covering counterfactual statements written in English, German, and\nJapanese languages. The dataset is unique as it contains counterfactuals in\nmultiple languages, covers a new application area of e-commerce reviews, and\nprovides high quality professional annotations. We train CFD models using\ndifferent text representation methods and classifiers. We find that these\nmodels are robust against the selectional biases introduced due to cue\nphrase-based sentence selection. Moreover, our CFD dataset is compatible with\nprior datasets and can be merged to learn accurate CFD models. Applying machine\ntranslation on English counterfactual examples to create multilingual data\nperforms poorly, demonstrating the language-specificity of this problem, which\nhas been ignored so far.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_J/0/1/0/all/0/1\">James O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozenshtein_P/0/1/0/all/0/1\">Polina Rozenshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiryo_R/0/1/0/all/0/1\">Ryuichi Kiryo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubota_M/0/1/0/all/0/1\">Motoko Kubota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TWEAC: Transformer with Extendable QA Agent Classifiers. (arXiv:2104.07081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07081","description":"<p>Question answering systems should help users to access knowledge on a broad\nrange of topics and to answer a wide array of different questions. Most systems\nfall short of this expectation as they are only specialized in one particular\nsetting, e.g., answering factual questions with Wikipedia data. To overcome\nthis limitation, we propose composing multiple QA agents within a meta-QA\nsystem. We argue that there exist a wide range of specialized QA agents in\nliterature. Thus, we address the central research question of how to\neffectively and efficiently identify suitable QA agents for any given question.\nWe study both supervised and unsupervised approaches to address this challenge,\nshowing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves\nthe best performance overall with 94% accuracy. We provide extensive insights\non the scalability of TWEAC, demonstrating that it scales robustly to over 100\nQA agents with each providing just 1000 examples of questions they can answer.\nOur code and data is available:\nhttps://github.com/UKPLab/TWEAC-qa-agent-selection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08678","description":"<p>Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete representations in neural models of spoken language. (arXiv:2105.05582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05582","description":"<p>The distributed and continuous representations used by neural networks are at\nodds with representations employed in linguistics, which are typically\nsymbolic. Vector quantization has been proposed as a way to induce discrete\nneural representations that are closer in nature to their linguistic\ncounterparts. However, it is not clear which metrics are the best-suited to\nanalyze such discrete representations. We compare the merits of four commonly\nused metrics in the context of weakly supervised models of spoken language. We\ncompare the results they show when applied to two different models, while\nsystematically studying the effect of the placement and size of the\ndiscretization layer. We find that different evaluation regimes can give\ninconsistent results. While we can attribute them to the properties of the\ndifferent metrics in most cases, one point of concern remains: the use of\nminimal pairs of phoneme triples as stimuli disadvantages larger discrete unit\ninventories, unlike metrics applied to complete utterances. Furthermore, while\nin general vector quantization induces representations that correlate with\nunits posited in linguistics, the strength of this correlation is only\nmoderate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1\">Bertrand Higy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1\">Lieke Gelderloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divided We Rule: Influencer Polarization on Twitter During Political Crises in India. (arXiv:2105.08361v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2105.08361","description":"<p>Influencers are key to the nature and networks of information propagation on\nsocial media. Influencers are particularly important in political discourse\nthrough their engagement with issues, and may derive their legitimacy either\nsolely or in large part through online operation, or have an offline sphere of\nexpertise such as entertainers, journalists etc. To quantify influencers'\npolitical engagement and polarity, we use Google's Universal Sentence Encoder\n(USE) to encode the tweets of 6k influencers and 26k Indian politicians during\npolitical crises in India. We then obtain aggregate vector representations of\nthe influencers based on their tweet embeddings, which alongside retweet graphs\nhelp compute their stance and polarity with respect to these political issues.\nWe find that influencers engage with the topics in a partisan manner, with\npolarized influencers being rewarded with increased retweeting and following.\nMoreover, we observe that specific groups of influencers are consistently\npolarized across all events. We conclude by discussing how our study provides\ninsights into the political schisms of present-day India, but also offers a\nmeans to study the role of influencers in exacerbating political polarization\nin other contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Saloni Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Dibyendu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhawat_G/0/1/0/all/0/1\">Gazal Shekhawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_J/0/1/0/all/0/1\">Joyojeet Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directed Acyclic Graph Network for Conversational Emotion Recognition. (arXiv:2105.12907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12907","description":"<p>The modeling of conversational context plays a vital role in emotion\nrecognition from conversation (ERC). In this paper, we put forward a novel idea\nof encoding the utterances with a directed acyclic graph (DAG) to better model\nthe intrinsic structure within a conversation, and design a directed acyclic\nneural network, namely DAG-ERC, to implement this idea. In an attempt to\ncombine the strengths of conventional graph-based neural models and\nrecurrence-based neural models, DAG-ERC provides a more intuitive way to model\nthe information flow between long-distance conversation background and nearby\ncontext. Extensive experiments are conducted on four ERC benchmarks with\nstate-of-the-art models employed as baselines for comparison. The empirical\nresults demonstrate the superiority of this new model and confirm the\nmotivation of the directed acyclic graph architecture for ERC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05707","description":"<p>Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1\">Rami Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Arpit Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06922","description":"<p>How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fu-An Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07340","description":"<p>Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of transfer learning. Due\nto the nature of mathematical texts, which often use domain specific vocabulary\nalong with equations and math symbols, we posit that the development of a new\nBERT model for mathematics would be useful for many mathematical downstream\ntasks. In this resource paper, we introduce our multi-institutional effort\n(i.e., two learning platforms and three academic institutions in the US) toward\nthis need: MathBERT, a model created by pre-training the BASE BERT model on a\nlarge mathematical corpus ranging from pre-kindergarten (pre-k), to\nhigh-school, to college graduate level mathematical content. In addition, we\nselect three general NLP tasks that are often used in mathematics education:\nprediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge\ntracing, to demonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods by 1.2-22% and\nBASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific\nvocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT\npre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the\nparticipated leaning platforms: Stride, Inc, a commercial educational resource\nprovider, and ASSISTments.org, a free online educational platform. We release\nMathBERT for public usage at: https://github.com/tbs17/MathBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_B/0/1/0/all/0/1\">Ben Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01366","description":"<p>Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07610","description":"<p>This paper improves the robustness of the pretrained language model, BERT,\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated \\textit{on the fly} during training. Experimental results\nshow that our method improves the robustness of BERT against four different\nword substitution-based adversarial attacks. Additionally, combining our method\nwith adversarial training gives higher robustness than adversarial training\nalone. Furthermore, to understand why our method can improve the model\nrobustness against adversarial attacks, we study vector representations of\nclean examples and their corresponding adversarial examples before and after\napplying our method. As our method improves model robustness with unlabeled raw\ndata, it opens up the possibility of using large text datasets to train robust\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00412","description":"<p>In multimodal sentiment analysis (MSA), the performance of a model highly\ndepends on the quality of synthesized embeddings. These embeddings are\ngenerated from the upstream process called multimodal fusion, which aims to\nextract and combine the input unimodal raw data to produce a richer multimodal\nrepresentation. Previous work either back-propagates the task loss or\nmanipulates the geometric property of feature spaces to produce favorable\nfusion results, which neglects the preservation of critical task-related\ninformation that flows from input to the fusion results. In this work, we\npropose a framework named MultiModal InfoMax (MMIM), which hierarchically\nmaximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)\nand between multimodal fusion result and unimodal input in order to maintain\ntask-related information through multimodal fusion. The framework is jointly\ntrained with the main task (MSA) to improve the performance of the downstream\nMSA task. To address the intractable issue of MI bounds, we further formulate a\nset of computationally simple parametric and non-parametric methods to\napproximate their truth value. Experimental results on the two widely used\ndatasets demonstrate the efficacy of our approach. The implementation of this\nwork is publicly available at\nhttps://github.com/declare-lab/Multimodal-Infomax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01207","description":"<p>Large multilingual language models show remarkable zero-shot cross-lingual\ntransfer performance on a range of tasks. Follow-up works hypothesized that\nthese models internally project representations of different languages into a\nshared interlingual space. However, they produced contradictory results. In\nthis paper, we correct the famous prior work claiming that \"BERT is not an\nInterlingua\" and show that with the proper choice of sentence representation\ndifferent languages actually do converge to a shared space in such language\nmodels. Furthermore, we demonstrate that this convergence pattern is robust\nacross four measures of correlation similarity and six mBERT-like models. We\nthen extend our analysis to 28 diverse languages and find that the interlingual\nspace exhibits a particular structure similar to the linguistic relatedness of\nlanguages. We also highlight a few outlier languages that seem to fail to\nconverge to the shared space. The code for replicating our results is available\nat the following URL: https://github.com/maksym-del/interlingua.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03772","description":"<p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03910","description":"<p>In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as \"make this\nmelodramatic\" or \"insert a metaphor.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05494","description":"<p>Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Anoop C S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_P/0/1/0/all/0/1\">Prathosh A P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">A G Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05611","description":"<p>We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06554","description":"<p>This paper concerns the intersection of natural language and the physical\nspace around us in which we live, that we observe and/or imagine things within.\nMany important features of language have spatial connotations, for example,\nmany prepositions (like in, next to, after, on, etc.) are fundamentally\nspatial. Space is also a key factor of the meanings of many\nwords/phrases/sentences/text, and space is a, if not the key, context for\nreferencing (e.g. pointing) and embodiment.\n</p>\n<p>We propose a mechanism for how space and linguistic structure can be made to\ninteract in a matching compositional fashion. Examples include Cartesian space,\nsubway stations, chesspieces on a chess-board, and Penrose's staircase. The\nstarting point for our construction is the DisCoCat model of compositional\nnatural language meaning, which we relax to accommodate physical space. We\naddress the issue of having multiple agents/objects in a space, including the\ncase that each agent has different capabilities with respect to that space,\ne.g., the specific moves each chesspiece can make, or the different velocities\none may be able to reach.\n</p>\n<p>Once our model is in place, we show how inferences drawing from the structure\nof physical space can be made. We also how how linguistic model of space can\ninteract with other such models related to our senses and/or embodiment, such\nas the conceptual spaces of colour, taste and smell, resulting in a rich\ncompositional model of meaning that is close to human experience and embodiment\nin the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Mascianica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07222","description":"<p>Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop. (arXiv:1907.11184v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/1907.11184","description":"<p>While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandogan_E/0/1/0/all/0/1\">Eser Kandogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasecki_W/0/1/0/all/0/1\">Walter S. Lasecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}