{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"From Pivots to Graphs: Augmented CycleDensity as a Generalization to One Time InverseConsultation. (arXiv:2108.12459v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12459","description":"<p>This paper describes an approach used to generate new translations using raw\nbilingual dictionaries as part of the 4th Task Inference Across Dictionaries\n(TIAD 2021) shared task. We propose Augmented Cycle Density (ACD) as a\nframework that combines insights from two state of the art methods that require\nno sense information and parallel corpora: Cycle Density (CD) and One Time\nInverse Consultation (OTIC). The task results show that across 3 unseen\nlanguage pairs, ACD's predictions, has more than double (74%) the coverage of\nOTIC at almost the same precision (76%). ACD combines CD's scalability -\nleveraging rich multilingual graphs for better predictions, and OTIC's data\nefficiency - producing good results with the minimum possible resource of one\npivot language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shashwat Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_K/0/1/0/all/0/1\">Kunwar Shaanjeet Singh Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12463","description":"<p>A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings (\\textit{e.g.}, BERT, Roberta, ELMo) is introduced.\nThis metric is motivated by a new framework relying on optimal transport tools,\n\\textit{i.e.}, Wasserstein distance and barycenter. By modelling the layer\noutput of deep contextualized embeddings as a probability distribution rather\nthan by a vector embedding; this framework provides a natural way to aggregate\nthe different outputs through the Wasserstein space topology. In addition, it\nprovides theoretical grounds to our metric and offers an alternative to\navailable solutions (\\textit{e.g.}, MoverScore and BertScore). Numerical\nevaluation is performed on four different tasks: machine translation,\nsummarization, data2text generation and image captioning. Our results show that\n\\texttt{BaryScore} outperforms other BERT based metrics and exhibits more\nconsistent behaviour in particular for text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12465","description":"<p>Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clave_C/0/1/0/all/0/1\">Chloe Clave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models. (arXiv:2108.12472v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12472","description":"<p>Automatic construction of relevant Knowledge Bases (KBs) from text, and\ngeneration of semantically meaningful text from KBs are both long-standing\ngoals in Machine Learning. In this paper, we present ReGen, a bidirectional\ngeneration of text and graph leveraging Reinforcement Learning (RL) to improve\nperformance. Graph linearization enables us to re-frame both tasks as a\nsequence to sequence generation problem regardless of the generative direction,\nwhich in turn allows the use of Reinforcement Learning for sequence training\nwhere the model itself is employed as its own critic leading to Self-Critical\nSequence Training (SCST). We present an extensive investigation demonstrating\nthat the use of RL via SCST benefits graph and text generation on WebNLG+ 2020\nand TekGen datasets. Our system provides state-of-the-art results on WebNLG+\n2020 by significantly improving upon published results from the WebNLG 2020+\nChallenge for both text-to-graph and graph-to-text generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1\">Pierre L. Dognin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinions are Made to be Changed: Temporally Adaptive Stance Classification. (arXiv:2108.12476v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12476","description":"<p>Given the rapidly evolving nature of social media and people's views, word\nusage changes over time. Consequently, the performance of a classifier trained\non old textual data can drop dramatically when tested on newer data. While\nresearch in stance classification has advanced in recent years, no effort has\nbeen invested in making these classifiers have persistent performance over\ntime. To study this phenomenon we introduce two novel large-scale, longitudinal\nstance datasets. We then evaluate the performance persistence of stance\nclassifiers over time and demonstrate how it decays as the temporal gap between\ntraining and testing data increases. We propose a novel approach to mitigate\nthis performance drop, which is based on temporal adaptation of the word\nembeddings used for training the stance classifier. This enables us to make use\nof readily available unlabelled data from the current time period instead of\nexpensive annotation efforts. We propose and compare several approaches to\nembedding adaptation and find that the Incremental Temporal Alignment (ITA)\nmodel leads to the best results in reducing performance drop over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12516","description":"<p>Neural table-to-text generation models have achieved remarkable progress on\nan array of tasks. However, due to the data-hungry nature of neural models,\ntheir performances strongly rely on large-scale training examples, limiting\ntheir applicability in real-world applications. To address this, we propose a\nnew framework: Prototype-to-Generate (P2G), for table-to-text generation under\nthe few-shot scenario. The proposed framework utilizes the retrieved\nprototypes, which are jointly selected by an IR system and a novel prototype\nselector to help the model bridging the structural gap between tables and\ntexts. Experimental results on three benchmark datasets with three\nstate-of-the-art models demonstrate that the proposed framework significantly\nimproves the model performance across various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Factuality of Reporting of News Media Using Observations About User Attention in Their YouTube Channels. (arXiv:2108.12519v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12519","description":"<p>We propose a novel framework for predicting the factuality of reporting of\nnews media outlets by studying the user attention cycles in their YouTube\nchannels. In particular, we design a rich set of features derived from the\ntemporal evolution of the number of views, likes, dislikes, and comments for a\nvideo, which we then aggregate to the channel level. We develop and release a\ndataset for the task, containing observations of user attention on YouTube\nchannels for 489 news media. Our experiments demonstrate both complementarity\nand sizable improvements over state-of-the-art textual representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bozhanova_K/0/1/0/all/0/1\">Krasimira Bozhanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkov_Y/0/1/0/all/0/1\">Yoan Dinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castaldo_M/0/1/0/all/0/1\">Maria Castaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturini_T/0/1/0/all/0/1\">Tommaso Venturini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetBLM: A Hate Speech Dataset and Analysis of Black Lives Matter-related Microblogs on Twitter. (arXiv:2108.12521v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12521","description":"<p>In the past few years, there has been a significant rise in toxic and hateful\ncontent on various social media platforms. Recently Black Lives Matter movement\ncame into the picture, causing an avalanche of user generated responses on the\ninternet. In this paper, we have proposed a Black Lives Matter related tweet\nhate speech dataset TweetBLM. Our dataset comprises 9165 manually annotated\ntweets that target the Black Lives Matter movement. We annotated the tweets\ninto two classes, i.e., HATE and NONHATE based on their content related to\nracism erupted from the movement for the black community. In this work, we also\ngenerated useful statistical insights on our dataset and performed a systematic\nanalysis of various machine learning models such as Random Forest, CNN, LSTM,\nBiLSTM, Fasttext, BERTbase, and BERTlarge for the classification task on our\ndataset. Through our work, we aim at contributing to the substantial efforts of\nthe research community for the identification and mitigation of hate speech on\nthe internet. The dataset is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pranesh_R/0/1/0/all/0/1\">Raj Ratn Pranesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Energy-Based Approximate Inference Networks for Structured Applications in NLP. (arXiv:2108.12522v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12522","description":"<p>Structured prediction in natural language processing (NLP) has a long\nhistory. The complex models of structured application come at the difficulty of\nlearning and inference. These difficulties lead researchers to focus more on\nmodels with simple structure components (e.g., local classifier). Deep\nrepresentation learning has become increasingly popular in recent years. The\nstructure components of their method, on the other hand, are usually relatively\nsimple. We concentrate on complex structured models in this dissertation. We\nprovide a learning framework for complicated structured models as well as an\ninference method with a better speed/accuracy/search error trade-off. The\ndissertation begins with a general introduction to energy-based models. In NLP\nand other applications, an energy function is comparable to the concept of a\nscoring function. In this dissertation, we discuss the concept of the energy\nfunction and structured models with different energy functions. Then, we\npropose a method in which we train a neural network to do argmax inference\nunder a structured energy function, referring to the trained networks as\n\"inference networks\" or \"energy-based inference networks\". We then develop ways\nof jointly learning energy functions and inference networks using an\nadversarial learning framework. Despite the inference and learning difficulties\nof energy-based models, we present approaches in this thesis that enable\nenergy-based models more easily to be applied in structured NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Representations and Phoneme Classification for Preserving the Endangered Language of Ladin. (arXiv:2108.12531v1 [eess.AS])","link":"http://arxiv.org/abs/2108.12531","description":"<p>A vast majority of the world's 7,000 spoken languages are predicted to become\nextinct within this century, including the endangered language of Ladin from\nthe Italian Alps. Linguists who work to preserve a language's phonetic and\nphonological structure can spend hours transcribing each minute of speech from\nnative speakers. To address this problem in the context of Ladin, our paper\npresents the first analysis of speech representations and machine learning\nmodels for classifying 32 phonemes of Ladin. We experimented with a novel\ndataset of the Fascian dialect of Ladin, collected from native speakers in\nItaly. We created frame-level and segment-level speech feature extraction\napproaches and conducted extensive experiments with 8 different classifiers\ntrained on 9 different speech representations. Our speech representations\nranged from traditional features (MFCC, LPC) to features learned with deep\nneural network models (autoencoders, LSTM autoencoders, and WaveNet). Our\nhighest-performing classifier, trained on MFCC representations of speech\nsignals, achieved an 86% average accuracy across all Ladin phonemes. We also\nobtained average accuracies above 77% for all Ladin phoneme subgroups examined.\nOur findings contribute insights for learning discriminative Ladin phoneme\nrepresentations and demonstrate the potential for leveraging machine learning\nand speech signal processing to preserve Ladin and other endangered languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durante_Z/0/1/0/all/0/1\">Zane Durante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_E/0/1/0/all/0/1\">Eric Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sichong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramdas_T/0/1/0/all/0/1\">Tejas Ramdas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iskarous_K/0/1/0/all/0/1\">Khalil Iskarous</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QACE: Asking Questions to Evaluate an Image Caption. (arXiv:2108.12560v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12560","description":"<p>In this paper, we propose QACE, a new metric based on Question Answering for\nCaption Evaluation. QACE generates questions on the evaluated caption and\nchecks its content by asking the questions on either the reference caption or\nthe source image. We first develop QACE-Ref that compares the answers of the\nevaluated caption to its reference, and report competitive results with the\nstate-of-the-art metrics. To go further, we propose QACE-Img, which asks the\nquestions directly on the image, instead of reference. A Visual-QA system is\nnecessary for QACE-Img. Unfortunately, the standard VQA models are framed as a\nclassification among only a few thousand categories. Instead, we propose\nVisual-T5, an abstractive VQA system. The resulting metric, QACE-Img is\nmulti-modal, reference-less, and explainable. Our experiments show that\nQACE-Img compares favorably w.r.t. other reference-less metrics. We will\nrelease the pre-trained models to compute QACE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-driven text descriptions for images. (arXiv:2108.12575v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12575","description":"<p>A big part of achieving Artificial General Intelligence(AGI) is to build a\nmachine that can see and listen like humans. Much work has focused on designing\nmodels for image classification, video classification, object detection, pose\nestimation, speech recognition, etc., and has achieved significant progress in\nrecent years thanks to deep learning. However, understanding the world is not\nenough. An AI agent also needs to know how to talk, especially how to\ncommunicate with a human. While perception (vision, for example) is more common\nacross animal species, the use of complicated language is unique to humans and\nis one of the most important aspects of intelligence.\n</p>\n<p>In this thesis, we focus on generating textual output given visual input. In\nChapter 3, we focus on generating the referring expression, a text description\nfor an object in the image so that a receiver can infer which object is being\ndescribed. We use a comprehension machine to directly guide the generated\nreferring expressions to be more discriminative. In Chapter 4, we introduce a\nmethod that encourages discriminability in image caption generation. We show\nthat more discriminative captioning models generate more descriptive captions.\nIn Chapter 5, we study how training objectives and sampling methods affect the\nmodels' ability to generate diverse captions. We find that a popular captioning\ntraining strategy will be detrimental to the diversity of generated captions.\nIn Chapter 6, we propose a model that can control the length of generated\ncaptions. By changing the desired length, one can influence the style and\ndescriptiveness of the captions. Finally, in Chapter 7, we rank/generate\ninformative image tags according to their information utility. The proposed\nmethod better matches what humans think are the most important tags for the\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruotian Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2108.12589v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12589","description":"<p>As the labeling cost for different modules in task-oriented dialog (ToD)\nsystems is expensive, a major challenge is to train different modules with the\nleast amount of labeled data. Recently, large-scale pre-trained language\nmodels, have shown promising results for few-shot learning in ToD. In this\npaper, we devise a self-training approach to utilize the abundant unlabeled\ndialog data to further improve state-of-the-art pre-trained models in few-shot\nlearning scenarios for ToD systems. Specifically, we propose a self-training\napproach that iteratively labels the most confident unlabeled data to train a\nstronger Student model. Moreover, a new text augmentation technique (GradAug)\nis proposed to better train the Student by replacing non-crucial tokens using a\nmasked language model. We conduct extensive experiments and present analyses on\nfour downstream tasks in ToD, including intent classification, dialog state\ntracking, dialog act prediction, and response selection. Empirical results\ndemonstrate that the proposed self-training approach consistently improves\nstate-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number\nof labeled data are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Fengyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Model Pruning based on Mutual Information. (arXiv:2108.12594v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12594","description":"<p>The proposed pruning strategy offers merits over weight-based pruning\ntechniques: (1) it avoids irregular memory access since representations and\nmatrices can be squeezed into their smaller but dense counterparts, leading to\ngreater speedup; (2) in a manner of top-down pruning, the proposed method\noperates from a more global perspective based on training signals in the top\nlayer, and prunes each layer by propagating the effect of global signals\nthrough layers, leading to better performances at the same sparsity level.\nExtensive experiments show that at the same sparsity level, the proposed\nstrategy offers both greater speedup and higher performances than weight-based\npruning methods (e.g., magnitude pruning, movement pruning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigation of Diachronic Bias in Fake News Detection Dataset. (arXiv:2108.12601v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12601","description":"<p>Fake news causes significant damage to society.To deal with these fake news,\nseveral studies on building detection models and arranging datasets have been\nconducted. Most of the fake news datasets depend on a specific time period.\nConsequently, the detection models trained on such a dataset have difficulty\ndetecting novel fake news generated by political changes and social changes;\nthey may possibly result in biased output from the input, including specific\nperson names and organizational names. We refer to this problem as\n\\textbf{Diachronic Bias} because it is caused by the creation date of news in\neach dataset. In this study, we confirm the bias, especially proper nouns\nincluding person names, from the deviation of phrase appearances in each\ndataset. Based on these findings, we propose masking methods using Wikidata to\nmitigate the influence of person names and validate whether they make fake news\ndetection models robust through experiments with in-domain and out-of-domain\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murayama_T/0/1/0/all/0/1\">Taichi Murayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WALNUT: A Benchmark on Weakly Supervised Learning for Natural Language Understanding. (arXiv:2108.12603v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12603","description":"<p>Building quality machine learning models for natural language understanding\n(NLU) tasks relies heavily on labeled data. Weak supervision has been shown to\nprovide valuable supervision when large amount of labeled data is unavailable\nor expensive to obtain. Existing works studying weak supervision for NLU either\nmostly focus on a specific task or simulate weak supervision signals from\nground-truth labels. To date a benchmark for NLU with real world weak\nsupervision signals for a collection of NLU tasks is still not available. In\nthis paper, we propose such a benchmark, named WALNUT, to advocate and\nfacilitate research on weak supervision for NLU. WALNUT consists of NLU tasks\nwith different types, including both document-level prediction tasks and\ntoken-level prediction tasks and for each task contains weak labels generated\nby multiple real-world weak sources. We conduct baseline evaluations on the\nbenchmark to systematically test the value of weak supervision for NLU tasks,\nwith various weak supervision methods and model architectures. We demonstrate\nthe benefits of weak supervision for low-resource NLU tasks and expect WALNUT\nto stimulate further research on methodologies to best leverage weak\nsupervision. The benchmark and code for baselines will be publicly available at\naka.ms/walnut_benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadlineCause: A Dataset of News Headlines for Detecting Casualties. (arXiv:2108.12626v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12626","description":"<p>Detecting implicit causal relations in texts is a task that requires both\ncommon sense and world knowledge. Existing datasets are focused either on\ncommonsense causal reasoning or explicit causal relations. In this work, we\npresent HeadlineCause, a dataset for detecting implicit causal relations\nbetween pairs of news headlines. The dataset includes over 5000 headline pairs\nfrom English news and over 9000 headline pairs from Russian news labeled\nthrough crowdsourcing. The pairs vary from totally unrelated or belonging to\nthe same general topic to the ones including causation and refutation\nrelations. We also present a set of models and experiments that demonstrates\nthe dataset validity, including a multilingual XLM-RoBERTa based model for\ncausality detection and a GPT-2 based model for possible effects prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oh My Mistake!: Toward Realistic Dialogue State Tracking including Turnback Utterances. (arXiv:2108.12637v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12637","description":"<p>The primary purpose of dialogue state tracking (DST), a critical component of\nan end-to-end conversational system, is to build a model that responds well to\nreal-world situations. Although we often change our minds during ordinary\nconversations, current benchmark datasets do not adequately reflect such\noccurrences and instead consist of over-simplified conversations, in which no\none changes their mind during a conversation. As the main question inspiring\nthe present study,``Are current benchmark datasets sufficiently diverse to\nhandle casual conversations in which one changes their mind?'' We found that\nthe answer is ``No'' because simply injecting template-based turnback\nutterances significantly degrades the DST model performance. The test joint\ngoal accuracy on the MultiWOZ decreased by over 5\\%p when the simplest form of\nturnback utterance was injected. Moreover, the performance degeneration worsens\nwhen facing more complicated turnback situations. However, we also observed\nthat the performance rebounds when a turnback is appropriately included in the\ntraining dataset, implying that the problem is not with the DST models but\nrather with the construction of the benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction as Natural Language Generation. (arXiv:2108.12724v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction (EE), the task that identifies event triggers and their\narguments in text, is usually formulated as a classification or structured\nprediction problem. Such models usually reduce labels to numeric identifiers,\nmaking them unable to take advantage of label semantics (e.g. an event type\nnamed Arrest is related to words like arrest, detain, or apprehend). This\nprevents the generalization to new event types. In this work, we formulate EE\nas a natural language generation task and propose GenEE, a model that not only\ncaptures complex dependencies within an event but also generalizes well to\nunseen or rare event types. Given a passage and an event type, GenEE is trained\nto generate a natural sentence following a predefined template for that event\ntype. The generated output is then decoded into trigger and argument\npredictions. The autoregressive generation process naturally models the\ndependencies among the predictions -- each new word predicted depends on those\nalready generated in the output sentence. Using carefully designed input\nprompts during generation, GenEE is able to capture label semantics, which\nenables the generalization to new event types. Empirical results show that our\nmodel achieves strong performance on event extraction tasks under all\nzero-shot, few-shot, and high-resource scenarios. Especially, in the\nhigh-resource setting, GenEE outperforms the state-of-the-art model on argument\nextraction and gets competitive results with the current best on end-to-end EE\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$k$Folden: $k$-Fold Ensemble for Out-Of-Distribution Detection. (arXiv:2108.12731v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12731","description":"<p>Out-of-Distribution (OOD) detection is an important problem in natural\nlanguage processing (NLP). In this work, we propose a simple yet effective\nframework $k$Folden, which mimics the behaviors of OOD detection during\ntraining without the use of any external data. For a task with $k$ training\nlabels, $k$Folden induces $k$ sub-models, each of which is trained on a subset\nwith $k-1$ categories with the left category masked unknown to the sub-model.\nExposing an unknown label to the sub-model during training, the model is\nencouraged to learn to equally attribute the probability to the seen $k-1$\nlabels for the unknown label, enabling this framework to simultaneously resolve\nin- and out-distribution examples in a natural way via OOD simulations. Taking\ntext classification as an archetype, we develop benchmarks for OOD detection\nusing existing text classification datasets. By conducting comprehensive\ncomparisons and analyses on the developed benchmarks, we demonstrate the\nsuperiority of $k$Folden against current methods in terms of improving OOD\ndetection performances while maintaining improved in-domain classification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12738","description":"<p>Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutuma_M/0/1/0/all/0/1\">Mutethia Mutuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Troy Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Structure and Word Relationship Modeling for Emphasis Selection. (arXiv:2108.12750v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12750","description":"<p>Emphasis Selection is a newly proposed task which focuses on choosing words\nfor emphasis in short sentences. Traditional methods only consider the sequence\ninformation of a sentence while ignoring the rich sentence structure and word\nrelationship information. In this paper, we propose a new framework that\nconsiders sentence structure via a sentence structure graph and word\nrelationship via a word similarity graph. The sentence structure graph is\nderived from the parse tree of a sentence. The word similarity graph allows\nnodes to share information with their neighbors since we argue that in emphasis\nselection, similar words are more likely to be emphasized together. Graph\nneural networks are employed to learn the representation of each node of these\ntwo graphs. Experimental results demonstrate that our framework can achieve\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution. (arXiv:2108.12777v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12777","description":"<p>Recent studies have shown that deep neural networks are vulnerable to\nintentionally crafted adversarial examples, and various methods have been\nproposed to defend against adversarial word-substitution attacks for neural NLP\nmodels. However, there is a lack of systematic study on comparing different\ndefense approaches under the same attacking setting. In this paper, we seek to\nfill the gap of systematic studies through comprehensive researches on\nunderstanding the behavior of neural text classifiers trained by various\ndefense methods under representative adversarial attacks. In addition, we\npropose an effective method to further improve the robustness of neural text\nclassifiers against such attacks and achieved the highest accuracy on both\nclean and adversarial examples on AGNEWS and IMDB datasets by a significant\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Propaganda Detection in News Articles. (arXiv:2108.12802v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12802","description":"<p>Online users today are exposed to misleading and propagandistic news articles\nand media posts on a daily basis. To counter thus, a number of approaches have\nbeen designed aiming to achieve a healthier and safer online news and media\nconsumption. Automatic systems are able to support humans in detecting such\ncontent; yet, a major impediment to their broad adoption is that besides being\naccurate, the decisions of such systems need also to be interpretable in order\nto be trusted and widely adopted by users. Since misleading and propagandistic\ncontent influences readers through the use of a number of deception techniques,\nwe propose to detect and to show the use of such techniques as a way to offer\ninterpretability. In particular, we define qualitatively descriptive features\nand we analyze their suitability for detecting deception techniques. We further\nshow that our interpretable features can be easily combined with pre-trained\nlanguage models, yielding state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seunghak Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohtarami_M/0/1/0/all/0/1\">Mitra Mohtarami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks. (arXiv:2108.12805v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12805","description":"<p>Adversarial training has been proven to be a powerful regularization method\nto improve the generalization of models. However, current adversarial training\nmethods only attack the original input sample or the embedding vectors, and\ntheir attacks lack coverage and diversity. To further enhance the breadth and\ndepth of attack, we propose a novel masked weight adversarial training method\ncalled DropAttack, which enhances generalization of model by adding\nintentionally worst-case adversarial perturbations to both the input and hidden\nlayers in different dimensions and minimize the adversarial risks generated by\neach layer. DropAttack is a general technique and can be adopt to a wide\nvariety of neural networks with different architectures. To validate the\neffectiveness of the proposed method, we used five public datasets in the\nfields of natural language processing (NLP) and computer vision (CV) for\nexperimental evaluating. We compare the proposed method with other adversarial\ntraining methods and regularization methods, and our method achieves\nstate-of-the-art on all datasets. In addition, Dropattack can achieve the same\nperformance when it use only a half training data compared to other standard\ntraining method. Theoretical analysis reveals that DropAttack can perform\ngradient regularization at random on some of the input and wight parameters of\nthe model. Further visualization experiments show that DropAttack can push the\nminimum risk of the model to a lower and flatter loss landscapes. Our source\ncode is publicly available on https://github.com/nishiwen1214/DropAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12821","description":"<p>Weight sharing has become the \\textit{de facto} approach to reduce the\ntraining cost of neural architecture search (NAS) by reusing the weights of\nshared operators from previously trained child models. However, the estimated\naccuracy of those child models has a low rank correlation with the ground truth\naccuracy due to the interference among different child models caused by weight\nsharing. In this paper, we investigate the interference issue by sampling\ndifferent child models and calculating the gradient similarity of shared\noperators, and observe that: 1) the interference on a shared operator between\ntwo child models is positively correlated to the number of different operators\nbetween them; 2) the interference is smaller when the inputs and outputs of the\nshared operator are more similar. Inspired by these two observations, we\npropose two approaches to mitigate the interference: 1) rather than randomly\nsampling child models for optimization, we propose a gradual modification\nscheme by modifying one operator between adjacent optimization steps to\nminimize the interference on the shared operators; 2) forcing the inputs and\noutputs of the operator across all child models to be similar to reduce the\ninterference. Experiments on a BERT search space verify that mitigating\ninterference via each of our proposed methods improves the rank correlation of\nsuper-pet and combining both methods can achieve better results. Our searched\narchitecture outperforms RoBERTa$_{\\rm base}$ by 1.1 and 0.6 scores and\nELECTRA$_{\\rm base}$ by 1.6 and 1.1 scores on the dev and test set of GLUE\nbenchmark. Extensive results on the BERT compression task, SQuAD datasets and\nother search spaces also demonstrate the effectiveness and generality of our\nproposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive and Abstractive Sentence Labelling of Sentiment-bearing Topics. (arXiv:2108.12822v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12822","description":"<p>This paper tackles the problem of automatically labelling sentiment-bearing\ntopics with descriptive sentence labels. We propose two approaches to the\nproblem, one extractive and the other abstractive. Both approaches rely on a\nnovel mechanism to automatically learn the relevance of each sentence in a\ncorpus to sentiment-bearing topics extracted from that corpus. The extractive\napproach uses a sentence ranking algorithm for label selection which for the\nfirst time jointly optimises topic--sentence relevance as well as\naspect--sentiment co-coverage. The abstractive approach instead addresses\naspect--sentiment co-coverage by using sentence fusion to generate a sentential\nlabel that includes relevant content from multiple sentences. To our knowledge,\nwe are the first to study the problem of labelling sentiment-bearing topics.\nOur experimental results on three real-world datasets show that both the\nextractive and abstractive approaches outperform four strong baselines in terms\nof facilitating topic understanding and interpretation. In addition, when\ncomparing extractive and abstractive labels, our evaluation shows that our best\nperforming abstractive method is able to provide more topic information\ncoverage in fewer words, at the cost of generating less grammatical labels than\nthe extractive method. We conclude that abstractive methods can effectively\nsynthesise the rich information contained in sentiment-bearing topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barawi_M/0/1/0/all/0/1\">Mohamad Hardyman Barawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharthan_A/0/1/0/all/0/1\">Advaith Siddharthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinbin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behind the Scenes: An Exploration of Trigger Biases Problem in Few-Shot Event Classification. (arXiv:2108.12844v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12844","description":"<p>Few-Shot Event Classification (FSEC) aims at developing a model for event\nprediction, which can generalize to new event types with a limited number of\nannotated data. Existing FSEC studies have achieved high accuracy on different\nbenchmarks. However, we find they suffer from trigger biases that signify the\nstatistical homogeneity between some trigger words and target event types,\nwhich we summarize as trigger overlapping and trigger separability. The biases\ncan result in context-bypassing problem, i.e., correct classifications can be\ngained by looking at only the trigger words while ignoring the entire context.\nTherefore, existing models can be weak in generalizing to unseen data in real\nscenarios. To further uncover the trigger biases and assess the generalization\nability of the models, we propose two new sampling methods, Trigger-Uniform\nSampling (TUS) and COnfusion Sampling (COS), for the meta tasks construction\nduring evaluation. Besides, to cope with the context-bypassing problem in FSEC\nmodels, we introduce adversarial training and trigger reconstruction\ntechniques. Experiments show these techniques help not only improve the\nperformance, but also enhance the generalization ability of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Fine-tuning for Pre-trained Language Models. (arXiv:2108.12848v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12848","description":"<p>Pre-trained language models (PrLM) have to carefully manage input units when\ntraining on a very large text with a vocabulary consisting of millions of\nwords. Previous works have shown that incorporating span-level information over\nconsecutive words in pre-training could further improve the performance of\nPrLMs. However, given that span-level clues are introduced and fixed in\npre-training, previous methods are time-consuming and lack of flexibility. To\nalleviate the inconvenience, this paper presents a novel span fine-tuning\nmethod for PrLMs, which facilitates the span setting to be adaptively\ndetermined by specific downstream tasks during the fine-tuning phase. In\ndetail, any sentences processed by the PrLM will be segmented into multiple\nspans according to a pre-sampled dictionary. Then the segmentation information\nwill be sent through a hierarchical CNN module together with the representation\noutputs of the PrLM and ultimately generate a span-enhanced representation.\nExperiments on GLUE benchmark show that the proposed span fine-tuning method\nsignificantly enhances the PrLM, and at the same time, offer more flexibility\nin an efficient way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rongzhou Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12870","description":"<p>Extractive text summarization aims at extracting the most representative\nsentences from a given document as its summary. To extract a good summary from\na long text document, sentence embedding plays an important role. Recent\nstudies have leveraged graph neural networks to capture the inter-sentential\nrelationship (e.g., the discourse graph) to learn contextual sentence\nembedding. However, those approaches neither consider multiple types of\ninter-sentential relationships (e.g., semantic similarity &amp; natural\nconnection), nor model intra-sentential relationships (e.g, semantic &amp;\nsyntactic relationship among words). To address these problems, we propose a\nnovel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model\ndifferent types of relationships among sentences and words. Based on Multi-GCN,\nwe propose a Multiplex Graph Summarization (Multi-GraS) model for extractive\ntext summarization. Finally, we evaluate the proposed models on the\nCNN/DailyMail benchmark dataset to demonstrate the effectiveness and\nsuperiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zeyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigations on Speech Recognition Systems for Low-Resource Dialectal Arabic-English Code-Switching Speech. (arXiv:2108.12881v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12881","description":"<p>Code-switching (CS), defined as the mixing of languages in conversations, has\nbecome a worldwide phenomenon. The prevalence of CS has been recently met with\na growing demand and interest to build CS ASR systems. In this paper, we\npresent our work on code-switched Egyptian Arabic-English automatic speech\nrecognition (ASR). We first contribute in filling the huge gap in resources by\ncollecting, analyzing and publishing our spontaneous CS Egyptian Arabic-English\nspeech corpus. We build our ASR systems using DNN-based hybrid and\nTransformer-based end-to-end models. In this paper, we present a thorough\ncomparison between both approaches under the setting of a low-resource,\northographically unstandardized, and morphologically rich language pair. We\nshow that while both systems give comparable overall recognition results, each\nsystem provides complementary sets of strength points. We show that recognition\ncan be improved by combining the outputs of both systems. We propose several\neffective system combination approaches, where hypotheses of both systems are\nmerged on sentence- and word-levels. Our approaches result in overall WER\nrelative improvement of 4.7%, over a baseline performance of 32.1% WER. In the\ncase of intra-sentential CS sentences, we achieve WER relative improvement of\n4.8%. Our best performing system achieves 30.6% WER on ArzEn test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chia-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmahdy_M/0/1/0/all/0/1\">Mohamed Elmahdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Answer Candidates for Quizzes and Answer-Aware Question Generators. (arXiv:2108.12898v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12898","description":"<p>In education, open-ended quiz questions have become an important tool for\nassessing the knowledge of students. Yet, manually preparing such questions is\na tedious task, and thus automatic question generation has been proposed as a\npossible alternative. So far, the vast majority of research has focused on\ngenerating the question text, relying on question answering datasets with\nreadily picked answers, and the problem of how to come up with answer\ncandidates in the first place has been largely ignored. Here, we aim to bridge\nthis gap. In particular, we propose a model that can generate a specified\nnumber of answer candidates for a given passage of text, which can then be used\nby instructors to write questions manually or can be passed as an input to\nautomatic answer-aware question generators. Our experiments show that our\nproposed answer candidate generation model outperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vachev_K/0/1/0/all/0/1\">Kristiyan Vachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Chemical Entity Typing with Multimodal Knowledge Representation. (arXiv:2108.12899v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12899","description":"<p>Automated knowledge discovery from trending chemical literature is essential\nfor more efficient biomedical research. How to extract detailed knowledge about\nchemical reactions from the core chemistry literature is a new emerging\nchallenge that has not been well studied. In this paper, we study the new\nproblem of fine-grained chemical entity typing, which poses interesting new\nchallenges especially because of the complex name mentions frequently occurring\nin chemistry literature and graphic representation of entities. We introduce a\nnew benchmark data set (CHEMET) to facilitate the study of the new task and\npropose a novel multi-modal representation learning framework to solve the\nproblem of fine-grained chemical entity typing by leveraging external resources\nwith chemical structures and using cross-modal attention to learn effective\nrepresentation of text in the chemistry domain. Experiment results show that\nthe proposed framework outperforms multiple state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinfeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulian_N/0/1/0/all/0/1\">Nikolaus Nova Parulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mischievous Nominal Constructions in Universal Dependencies. (arXiv:2108.12928v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12928","description":"<p>While the highly multilingual Universal Dependencies (UD) project provides\nextensive guidelines for clausal structure as well as structure within\ncanonical nominal phrases, a standard treatment is lacking for many\n\"mischievous\" nominal phenomena that break the mold. As a result, numerous\ninconsistencies within and across corpora can be found, even in languages with\nextensive UD treebanking work, such as English. This paper surveys the kinds of\nmischievous nominal expressions attested in English UD corpora and proposes\nsolutions primarily with English in mind, but which may offer paths to\nsolutions for a variety of UD languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations. (arXiv:2108.12941v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12941","description":"<p>Retrofitting is a technique used to move word vectors closer together or\nfurther apart in their space to reflect their relationships in a Knowledge Base\n(KB). However, retrofitting only works on concepts that are present in that KB.\nRetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a\none-to-one mapping between concepts and their retrofitted counterparts. It\napplies that mapping (post-specializes) to handle concepts that do not appear\nin the original KB in a manner similar to how some natural language systems\nhandle out-of-vocabulary entries. We test our system on three word-similarity\nbenchmarks and a downstream sentence simplification task and achieve the state\nof the art (CARD-660). Altogether, our results demonstrate our system's\neffectiveness for out-of-knowledge and rare word generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colon_Hernandez_P/0/1/0/all/0/1\">Pedro Colon-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yida Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_H/0/1/0/all/0/1\">Henry Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havasi_C/0/1/0/all/0/1\">Catherine Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1\">Cynthia Breazeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12944","description":"<p>With the increasing adoption of language models in applications involving\nsensitive data, it has become crucial to protect these models from leaking\nprivate information. Previous work has attempted to tackle this challenge by\ntraining RNN-based language models with differential privacy guarantees.\nHowever, applying classical differential privacy to language models leads to\npoor model performance as the underlying privacy notion is over-pessimistic and\nprovides undifferentiated protection for all tokens of the data. Given that the\nprivate information in natural language is sparse (for example, the bulk of an\nemail might not carry personally identifiable information), we propose a new\nprivacy notion, selective differential privacy, to provide rigorous privacy\nguarantees on the sensitive portion of the data to improve model utility. To\nrealize such a new notion, we develop a corresponding privacy mechanism,\nSelective-DPSGD, for RNN-based language models. Besides language modeling, we\nalso apply the method to a more concrete application -- dialog systems.\nExperiments on both language modeling and dialog system building show that the\nproposed privacy-preserving mechanism achieves better utilities while remaining\nsafe under various privacy attacks compared to the baselines. The data, code\nand models are available at https://github.com/wyshi/lm_privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Aiqi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Evan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation. (arXiv:2108.12960v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12960","description":"<p>Standard multi-task benchmarks are essential for driving the progress of\ngeneral pretraining models to generalize to various downstream tasks. However,\nexisting benchmarks such as GLUE and GLGE tend to focus on short text\nunderstanding and generation tasks, without considering long text modeling,\nwhich requires many distinct capabilities such as modeling long-range\ncommonsense and discourse relations, as well as the coherence and\ncontrollability of generation. The lack of standardized benchmarks makes it\ndifficult to fully evaluate these capabilities of a model and fairly compare\ndifferent models, especially Chinese pretraining models. Therefore, we propose\nLOT, a benchmark including two understanding and two generation tasks for\nChinese long text modeling evaluation. We construct the datasets for the tasks\nbased on various kinds of human-written Chinese stories. Besides, we release an\nencoder-decoder Chinese long text pretraining model named LongLM with up to 1\nbillion parameters. We pretrain LongLM on 120G Chinese novels with two\ngenerative tasks including text infilling and conditional continuation.\nExtensive experiments on LOT demonstrate that LongLM matches the performance of\nsimilar-sized pretraining models on the understanding tasks and outperforms\nstrong baselines substantially on the generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuoer Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yamei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruilin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12963","description":"<p>Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning. (arXiv:2108.13032v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13032","description":"<p>The highly popular Transformer architecture, based on self-attention, is the\nfoundation of large pretrained models such as BERT, that have become an\nenduring paradigm in NLP. While powerful, the computational resources and time\nrequired to pretrain such models can be prohibitive. In this work, we present\nan alternative self-attention architecture, Shatter, that more efficiently\nencodes sequence information by softly partitioning the space of relative\npositions and applying different value matrices to different parts of the\nsequence. This mechanism further allows us to simplify the multi-headed\nattention in Transformer to single-headed. We conduct extensive experiments\nshowing that Shatter achieves better performance than BERT, with pretraining\nbeing faster per step (15% on TPU), converging in fewer steps, and offering\nconsiderable memory savings (&gt;50%). Put together, Shatter can be pretrained on\n8 V100 GPUs in 7 days, and match the performance of BERT_Base -- making the\ncost of pretraining much more affordable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Ran Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding. (arXiv:2108.13048v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13048","description":"<p>Language understanding in speech-based systems have attracted much attention\nin recent years with the growing demand for voice interface applications.\nHowever, the robustness of natural language understanding (NLU) systems to\nerrors introduced by automatic speech recognition (ASR) is under-examined. %To\nfacilitate the research on ASR-robust general language understanding, In this\npaper, we propose ASR-GLUE benchmark, a new collection of 6 different NLU tasks\nfor evaluating the performance of models under ASR error across 3 different\nlevels of background noise and 6 speakers with various voice characteristics.\nBased on the proposed benchmark, we systematically investigate the effect of\nASR error on NLU tasks in terms of noise intensity, error type and speaker\nvariants. We further purpose two ways, correction-based method and data\naugmentation-based method to improve robustness of the NLU systems. Extensive\nexperimental results and analysises show that the proposed methods are\neffective to some extent, but still far from human performance, demonstrating\nthat NLU under ASR error is still very challenging and requires further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lingyun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Completion Meets Transfer Learning. (arXiv:2108.13073v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13073","description":"<p>The aim of knowledge base completion is to predict unseen facts from existing\nfacts in knowledge bases. In this work, we introduce the first approach for\ntransfer of knowledge from one collection of facts to another without the need\nfor entity or relation matching. The method works for both canonicalized\nknowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge\nbases where more than one copy of a real-world entity or relation may exist.\nSuch knowledge bases are a natural output of automated information extraction\ntools that extract structured data from unstructured text. Our main\ncontribution is a method that can make use of a large-scale pre-training on\nfacts, collected from unstructured text, to improve predictions on structured\ndata from a specific domain. The introduced method is the most impactful on\nsmall datasets such as ReVerb20K, where we obtained 6% absolute increase of\nmean reciprocal rank and 65% relative decrease of mean rank over the previously\nbest method, despite not relying on large pre-trained models like BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEREL: A Russian Dataset with Nested Named Entities and Relations. (arXiv:2108.13112v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13112","description":"<p>In this paper, we present NEREL, a Russian dataset for named entity\nrecognition and relation extraction. NEREL is significantly larger than\nexisting Russian datasets: to date it contains 56K annotated named entities and\n39K annotated relations. Its important difference from previous datasets is\nannotation of nested named entities, as well as relations within nested\nentities and at the discourse level. NEREL can facilitate development of novel\nmodels that can extract relations between nested named entities, as well as\nrelations on both sentence and document levels. NEREL also contains the\nannotation of events involving named entities and their roles in the events.\nThe NEREL collection is available via https://github.com/nerel-ds/NEREL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_I/0/1/0/all/0/1\">Ilia Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_V/0/1/0/all/0/1\">Vladimir Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1\">Suresh Manandhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugachev_A/0/1/0/all/0/1\">Alexander Pugachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13134","description":"<p>Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuron-level Interpretation of Deep NLP Models: A Survey. (arXiv:2108.13138v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13138","description":"<p>The proliferation of deep neural networks in various domains has seen an\nincreased need for interpretability of these methods. A plethora of research\nhas been carried out to analyze and understand components of the deep neural\nnetwork models. Preliminary work done along these lines and papers that\nsurveyed such, were focused on a more high-level representation analysis.\nHowever, a recent branch of work has concentrated on interpretability at a more\ngranular level, analyzing neurons and groups of neurons in these large models.\nIn this paper, we survey work done on fine-grained neuron analysis including:\ni) methods developed to discover and understand neurons in a network, ii) their\nlimitations and evaluation, iii) major findings including cross architectural\ncomparison that such analyses unravel and iv) direct applications of neuron\nanalysis such as model behavior control and domain adaptation along with\npotential directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSDS: A Fine-grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13139","description":"<p>Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customers' issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure. Neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13140","description":"<p>While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?. (arXiv:1905.10617v9 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1905.10617","description":"<p>Exposure bias has been regarded as a central problem for auto-regressive\nlanguage models (LM). It claims that teacher forcing would cause the test-time\ngeneration to be incrementally distorted due to the training-generation\ndiscrepancy. Although a lot of algorithms have been proposed to avoid teacher\nforcing and therefore alleviate exposure bias, there is little work showing how\nserious the exposure bias problem actually is. In this work, we focus on the\ntask of open-ended language generation, propose metrics to quantify the impact\nof exposure bias in the aspects of quality, diversity, and consistency. Our key\nintuition is that if we feed ground-truth data prefixes (instead of prefixes\ngenerated by the model itself) into the model and ask it to continue the\ngeneration, the performance should become much better because the\ntraining-generation discrepancy in the prefix is removed. Both automatic and\nhuman evaluations are conducted in our experiments. On the contrary to the\npopular belief in exposure bias, we find that the the distortion induced by the\nprefix discrepancy is limited, and does not seem to be incremental during the\ngeneration. Moreover, our analysis reveals an interesting self-recovery ability\nof the LM, which we hypothesize to be countering the harmful effects from\nexposure bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keeping it simple: Implementation and performance of the proto-principle of adaptation and learning in the language sciences. (arXiv:2003.03813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.03813","description":"<p>In this paper we present the Widrow-Hoff rule and its applications to\nlanguage data. After contextualizing the rule historically and placing it in\nthe chain of neurally inspired artificial learning models, we explain its\nrationale and implementational considerations. Using a number of case studies\nwe illustrate how the Widrow-Hoff rule offers unexpected opportunities for the\ncomputational simulation of a range of language phenomena that make it possible\nto approach old problems from a novel perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milin_P/0/1/0/all/0/1\">Petar Milin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croucher_M/0/1/0/all/0/1\">Michael Croucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divjak_D/0/1/0/all/0/1\">Dagmar Divjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Learning with Common Sense Knowledge Graphs. (arXiv:2006.10713v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.10713","description":"<p>Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations by embedding nodes\nfrom common sense knowledge graphs in a vector space. Common sense knowledge\ngraphs are an untapped source of explicit high-level knowledge that requires\nlittle human effort to apply to a range of tasks. To capture the knowledge in\nthe graph, we introduce ZSL-KG, a general-purpose framework with a novel\ntransformer graph convolutional network (TrGCN) for generating class\nrepresentations. Our proposed TrGCN architecture computes non-linear\ncombinations of node neighbourhoods. Our results show that ZSL-KG improves over\nexisting WordNet-based methods on five out of six zero-shot benchmark datasets\nin language and vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best-First Beam Search. (arXiv:2007.03909v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.03909","description":"<p>Decoding for many NLP tasks requires an effective heuristic algorithm for\napproximating exact search since the problem of searching the full output space\nis often intractable, or impractical in many settings. The default algorithm\nfor this job is beam search -- a pruned version of breadth-first search. Quite\nsurprisingly, beam search often returns better results than exact inference due\nto beneficial search bias for NLP tasks. In this work, we show that the\nstandard implementation of beam search can be made up to 10x faster in\npractice. Our method assumes that the scoring function is monotonic in the\nsequence length, which allows us to safely prune hypotheses that cannot be in\nthe final set of hypotheses early on. We devise effective monotonic\napproximations to popular nonmonontic scoring functions, including length\nnormalization and mutual information decoding. Lastly, we propose a\nmemory-reduced variant of Best-First Beam Search, which has a similar\nbeneficial search bias in terms of downstream performance, but runs in a\nfraction of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiovisual Speech Synthesis using Tacotron2. (arXiv:2008.00620v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2008.00620","description":"<p>Audiovisual speech synthesis is the problem of synthesizing a talking face\nwhile maximizing the coherency of the acoustic and visual speech. In this\npaper, we propose and compare two audiovisual speech synthesis systems for 3D\nface models. The first system is the AVTacotron2, which is an end-to-end\ntext-to-audiovisual speech synthesizer based on the Tacotron2 architecture.\nAVTacotron2 converts a sequence of phonemes representing the sentence to\nsynthesize into a sequence of acoustic features and the corresponding\ncontrollers of a face model. The output acoustic features are used to condition\na WaveRNN to reconstruct the speech waveform, and the output facial controllers\nare used to generate the corresponding video of the talking face. The second\naudiovisual speech synthesis system is modular, where acoustic speech is\nsynthesized from text using the traditional Tacotron2. The reconstructed\nacoustic speech signal is then used to drive the facial controls of the face\nmodel using an independently trained audio-to-facial-animation neural network.\nWe further condition both the end-to-end and modular approaches on emotion\nembeddings that encode the required prosody to generate emotional audiovisual\nspeech. We analyze the performance of the two systems and compare them to the\nground truth videos using subjective evaluation tests. The end-to-end and\nmodular systems are able to synthesize close to human-like audiovisual speech\nwith mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS\nof 4.1 for the ground truth generated from professionally recorded videos.\nWhile the end-to-end system gives a better overall quality, the modular\napproach is more flexible and the quality of acoustic speech and visual speech\nsynthesis is almost independent of each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdelaziz_A/0/1/0/all/0/1\">Ahmed Hussen Abdelaziz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anushree Prasanna Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seivwright_C/0/1/0/all/0/1\">Chloe Seivwright</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fanelli_G/0/1/0/all/0/1\">Gabriele Fanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Binder_J/0/1/0/all/0/1\">Justin Binder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stylianou_Y/0/1/0/all/0/1\">Yannis Stylianou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank over Class: The Untapped Potential of Ranking in Natural Language Processing. (arXiv:2009.05160v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05160","description":"<p>Text classification has long been a staple within Natural Language Processing\n(NLP) with applications spanning across diverse areas such as sentiment\nanalysis, recommender systems and spam detection. With such a powerful\nsolution, it is often tempting to use it as the go-to tool for all NLP problems\nsince when you are holding a hammer, everything looks like a nail. However, we\nargue here that many tasks which are currently addressed using classification\nare in fact being shoehorned into a classification mould and that if we instead\naddress them as a ranking problem, we not only improve the model, but we\nachieve better performance. We propose a novel end- to-end ranking approach\nconsisting of a Transformer network responsible for producing representations\nfor a pair of text sequences, which are in turn passed into a context\naggregating network outputting ranking scores used to determine an ordering to\nthe sequences based on some notion of relevance. We perform numerous\nexperiments on publicly-available datasets and investigate the applications of\nranking in problems often solved using classification. In an experiment on a\nheavily-skewed sentiment analysis dataset, converting ranking results to\nclassification labels yields an approximately 22% improvement over\nstate-of-the-art text classification, demonstrating the efficacy of text\nranking over text classification in certain scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1\">Andrew Stephen McGough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight Squeezing: Reparameterization for Knowledge Transfer and Model Compression. (arXiv:2010.06993v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.06993","description":"<p>In this work, we present a novel approach for simultaneous knowledge transfer\nand model compression called Weight Squeezing. With this method, we perform\nknowledge transfer from a teacher model by learning the mapping from its\nweights to smaller student model weights.\n</p>\n<p>We applied Weight Squeezing to a pre-trained text classification model based\non BERT-Medium model and compared our method to various other knowledge\ntransfer and model compression methods on GLUE multitask benchmark. We observed\nthat our approach produces better results while being significantly faster than\nother methods for training student models.\n</p>\n<p>We also proposed a variant of Weight Squeezing called Gated Weight Squeezing,\nfor which we combined fine-tuning of BERT-Medium model and learning mapping\nfrom BERT-Base weights. We showed that fine-tuning with Gated Weight Squeezing\noutperforms plain fine-tuning of BERT-Medium model as well as other concurrent\nSoTA approaches while much being easier to implement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chumachenko_A/0/1/0/all/0/1\">Artem Chumachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalaidin_P/0/1/0/all/0/1\">Pavel Kalaidin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Guided Abstractive Text Summarization: a Joint Learning Approach. (arXiv:2010.10323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10323","description":"<p>We introduce a new approach for abstractive text summarization, Topic-Guided\nAbstractive Summarization, which calibrates long-range dependencies from\ntopic-level features with globally salient content. The idea is to incorporate\nneural topic modeling with a Transformer-based sequence-to-sequence (seq2seq)\nmodel in a joint learning framework. This design can learn and preserve the\nglobal semantics of the document, which can provide additional contextual\nguidance for capturing important ideas of the document, thereby enhancing the\ngeneration of summary. We conduct extensive experiments on two datasets and the\nresults show that our proposed model outperforms many extractive and\nabstractive systems in terms of both ROUGE measurements and human evaluation.\nOur code is available at: https://github.com/chz816/tas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Ling Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11524","description":"<p>Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding. (arXiv:2101.00204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00204","description":"<p>In this paper, we introduce ``Embedding Barrier'', a phenomenon that limits\nthe monolingual performance of multilingual models on low-resource languages\nhaving unique typologies. We build `BanglaBERT', a Bangla language model\npretrained on 18.6 GB Internet-crawled data and benchmark on five standard NLU\ntasks. We discover a significant drop in the performance of the\nstate-of-the-art multilingual model (XLM-R) from BanglaBERT and attribute this\nto the Embedding Barrier through comprehensive experiments. We identify that a\nmultilingual model's performance on a low-resource language is hurt when its\nwriting script is not similar to any of the high-resource languages. To tackle\nthe barrier, we propose a straightforward solution by transcribing languages to\na common script, which can effectively improve the performance of a\nmultilingual model for the Bangla language. As a bi-product of the standard NLU\nbenchmarks, we introduce a new downstream dataset on natural language inference\n(NLI) and show that BanglaBERT outperforms previous state-of-the-art results on\nall tasks by up to 3.5%. We are making the BanglaBERT language model and the\nnew Bangla NLI dataset publicly available in the hope of advancing the\ncommunity. The resources can be found at\n\\url{https://github.com/csebuetnlp/banglabert}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1\">Anindya Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast End-to-End Speech Recognition via Non-Autoregressive Models and Cross-Modal Knowledge Transferring from BERT. (arXiv:2102.07594v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.07594","description":"<p>Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because the decoder predicts text\ntokens (such as characters or words) in an autoregressive manner, it is\ndifficult for an AED model to predict all tokens in parallel. This makes the\ninference speed relatively slow. We believe that because the encoder already\ncaptures the whole speech utterance, which has the token-level relationship\nimplicitly, we can predict a token without explicitly autoregressive language\nmodeling. When the prediction of a token does not rely on other tokens, the\nparallel prediction of all tokens in the sequence is realizable. Based on this\nidea, we propose a non-autoregressive speech recognition model called LASO\n(Listen Attentively, and Spell Once). The model consists of an encoder, a\ndecoder, and a position dependent summarizer (PDS). The three modules are based\non basic attention blocks. The encoder extracts high-level representations from\nthe speech. The PDS uses positional encodings corresponding to tokens to\nconvert the acoustic representations into token-level representations. The\ndecoder further captures token-level relationships with the self-attention\nmechanism. At last, the probability distribution on the vocabulary is computed\nfor each token position. Therefore, speech recognition is re-formulated as a\nposition-wise classification problem. Further, we propose a cross-modal\ntransfer learning method to refine semantics from a large-scale pre-trained\nlanguage model BERT for improving the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Ye Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A More Fine-Grained Aspect-Sentiment-Opinion Triplet Extraction Task. (arXiv:2103.15255v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.15255","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences and tries to provide a\ncomplete solution for aspect-based sentiment analysis (ABSA). However, some\ntriplets extracted by ASTE are confusing, since the sentiment in a triplet\nextracted by ASTE is the sentiment that the sentence expresses toward the\naspect term rather than the sentiment of the aspect term and opinion term pair.\nIn this paper, we introduce a more fine-grained Aspect-Sentiment-Opinion\nTriplet Extraction (ASOTE) Task. ASOTE also extracts aspect term, sentiment and\nopinion term triplets. However, the sentiment in a triplet extracted by ASOTE\nis the sentiment of the aspect term and opinion term pair. We build four\ndatasets for ASOTE based on several popular ABSA benchmarks. We propose a\nPosition-aware BERT-based Framework (PBF) to address this task. PBF first\nextracts aspect terms from sentences. For each extracted aspect term, PBF first\ngenerates aspect term-specific sentence representations considering both the\nmeaning and the position of the aspect term, then extracts associated opinion\nterms and predicts the sentiments of the aspect term and opinion term pairs\nbased on the sentence representations. Experimental results on the four\ndatasets show the effectiveness of PBF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-hua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Cunxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03969","description":"<p>The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nThis study successfully applied learning representation and machine learning\nalgorithms to detect heart failure from clinical natural language in a single\nFrench institution. Further work is needed to use the same methodology in other\ninstitutions and other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., templates, into the input and transform a classification task into a\nmasked language modeling problem. However, as for relation extraction,\ndetermining the appropriate prompt template requires domain expertise. Single\nlabel word handcrafted or auto-searched is cumbersome and time-consuming to\nverify their effectiveness in non-few-shot scenarios. Further, there exist\nabundant semantic knowledge among the entities and relation labels which cannot\nbe ignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a Knowledge-aware prompt-tuning with\nsynergistic optimization (KNIGHT) approach. Specifically, we inject entity and\nrelation knowledge into prompt construction with learnable virtual template\nwords and answer words and jointly optimize their representation with knowledge\nconstraints. Extensive experimental results on five datasets with standard and\nlow-resource settings demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-effective End-to-end Information Extraction for Semi-structured Document Images. (arXiv:2104.08041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08041","description":"<p>A real-world information extraction (IE) system for semi-structured document\nimages often involves a long pipeline of multiple modules, whose complexity\ndramatically increases its development and maintenance cost. One can instead\nconsider an end-to-end model that directly maps the input to the target output\nand simplify the entire process. However, such generation approach is known to\nlead to unstable performance if not designed carefully. Here we present our\nrecent effort on transitioning from our existing pipeline-based IE system to an\nend-to-end system focusing on practical challenges that are associated with\nreplacing and deploying the system in real, large-scale production. By\ncarefully formulating document IE as a sequence generation task, we show that a\nsingle end-to-end IE system can be built and still achieve competent\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geewook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Evaluation Beyond Perplexity. (arXiv:2106.00085v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00085","description":"<p>We propose an alternate approach to quantifying how well language models\nlearn natural language: we ask how well they match the statistical tendencies\nof natural language. To answer this question, we analyze whether text generated\nfrom language models exhibits the statistical tendencies present in the\nhuman-generated text on which they were trained. We provide a framework--paired\nwith significance tests--for evaluating the fit of language models to these\ntrends. We find that neural language models appear to learn only a subset of\nthe tendencies considered, but align much more closely with empirical trends\nthan proposed theoretical distributions (when present). Further, the fit to\ndifferent distributions is highly-dependent on both model architecture and\ngeneration strategy. As concrete examples, text generated under the nucleus\nsampling scheme adheres more closely to the type--token relationship of natural\nlanguage than text produced using standard ancestral sampling; text from LSTMs\nreflects the natural language distributions over length, stopwords, and symbols\nsurprisingly well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08087","description":"<p>Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08264","description":"<p>Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02314","description":"<p>Enormous hope in the efficacy of vaccines became recently a successful\nreality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,\nfueled by exposure to social media misinformation about COVID-19 vaccines\nbecame a major hurdle. Therefore, it is essential to automatically detect where\nmisinformation about COVID-19 vaccines on social media is spread and what kind\nof misinformation is discussed, such that inoculation interventions can be\ndelivered at the right time and in the right place, in addition to\ninterventions designed to address vaccine hesitancy. This paper is addressing\nthe first step in tackling hesitancy against COVID-19 vaccines, namely the\nautomatic detection of known misinformation about the vaccines on Twitter, the\nsocial media platform that has the highest volume of conversations about\nCOVID-19 and its vaccines. We present CoVaxLies, a new dataset of tweets judged\nrelevant to several misinformation targets about COVID-19 vaccines on which a\nnovel method of detecting misinformation was developed. Our method organizes\nCoVaxLies in a Misinformation Knowledge Graph as it casts misinformation\ndetection as a graph link prediction problem. The misinformation detection\nmethod detailed in this paper takes advantage of the link scoring functions\nprovided by several knowledge embedding methods. The experimental results\ndemonstrate the superiority of this method when compared with\nclassification-based methods, widely used currently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell A. Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda M. Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing. (arXiv:2108.05542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05542","description":"<p>Transformer-based pretrained language models (T-PTLMs) have achieved great\nsuccess in almost every NLP task. The evolution of these models started with\nGPT and BERT. These models are built on the top of transformers,\nself-supervised learning and transfer learning. Transformed-based PTLMs learn\nuniversal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These\nmodels provide good background knowledge to downstream tasks which avoids\ntraining of downstream models from scratch. In this comprehensive survey paper,\nwe initially give a brief overview of self-supervised learning. Next, we\nexplain various core concepts like pretraining, pretraining methods,\npretraining tasks, embeddings and downstream adaptation methods. Next, we\npresent a new taxonomy of T-PTLMs and then give brief overview of various\nbenchmarks including both intrinsic and extrinsic. We present a summary of\nvarious useful libraries to work with T-PTLMs. Finally, we highlight some of\nthe future research directions which will further improve these models. We\nstrongly believe that this comprehensive survey paper will serve as a good\nreference to learn the core concepts as well as to stay updated with the recent\nhappenings in T-PTLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06712","description":"<p>Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset to study question answering (QA) and natural\nlanguage generation (NLG) over hierarchical tables. HiTab is a cross-domain\ndataset constructed from a wealth of statistical reports (analyses) and\nWikipedia pages, and has unique characteristics: (1) nearly all tables are\nhierarchical, and (2) both target sentences for NLG and questions for QA are\nrevised from original, meaningful, and diverse descriptive sentences authored\nby analysts and professions of reports. (3) to reveal complex numerical\nreasoning in statistical analyses, we provide fine-grained annotations of\nentity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive\nsentences with well-annotated quantity and entity alignment on 3,597 tables\nwith broad coverage of table hierarchies and numerical reasoning types.\n</p>\n<p>Targeting hierarchical structure, we devise a novel hierarchy-aware logical\nform for symbolic reasoning over tables, which shows high effectiveness.\nTargeting complex numerical reasoning, we propose partially supervised training\ngiven annotations of entity and quantity alignment, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pretrained Language Models with Label Attention for Explainable Biomedical Text Classification. (arXiv:2108.11809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11809","description":"<p>The massive growth of digital biomedical data is making biomedical text\nindexing and classification increasingly important. Accordingly, previous\nresearch has devised numerous deep learning techniques focused on using\nfeedforward, convolutional or recurrent neural architectures. More recently,\nfine-tuned transformers-based pretrained models (PTMs) have demonstrated\nsuperior performance compared to such models in many natural language\nprocessing tasks. However, the direct use of PTMs in the biomedical domain is\nonly limited to the target documents, ignoring the rich semantic information in\nthe label descriptions. In this paper, we develop an improved label\nattention-based architecture to inject semantic label description into the\nfine-tuning process of PTMs. Results on two public medical datasets show that\nthe proposed fine-tuning scheme outperforms the conventionally fine-tuned PTMs\nand prior state-of-the-art models. Furthermore, we show that fine-tuning with\nthe label attention mechanism is interpretable in the interpretability study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bruce Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features are dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. The source code can be\nfound in https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}