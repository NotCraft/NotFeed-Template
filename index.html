<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-22T01:30:00Z">09-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection. (arXiv:2109.09777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09777">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the DISRPT2021 Shared Task on
Discourse Unit Segmentation, Connective Detection, and Relation Classification.
Our system, called DisCoDisCo, is a Transformer-based neural classifier which
enhances contextualized word embeddings (CWEs) with hand-crafted features,
relying on tokenwise sequence tagging for discourse segmentation and connective
detection, and a feature-rich, encoder-less sentence pair classifier for
relation classification. Our results for the first two tasks outperform SOTA
scores from the previous 2019 shared task, and results on relation
classification suggest strong performance on the new 2021 benchmark. Ablation
tests show that including features beyond CWEs are helpful for both tasks, and
a partial evaluation of multiple pre-trained Transformer-based language models
indicates that models pre-trained on the Next Sentence Prediction (NSP) task
are optimal for relation classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology. (arXiv:2109.09780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09780">
<div class="article-summary-box-inner">
<span><p>An important question concerning contextualized word embedding (CWE) models
like BERT is how well they can represent different word senses, especially
those in the long tail of uncommon senses. Rather than build a WSD system as in
previous work, we investigate contextualized embedding neighborhoods directly,
formulating a query-by-example nearest neighbor retrieval task and examining
ranking performance for words and senses in different frequency bands. In an
evaluation on two English sense-annotated corpora, we find that several popular
CWE models all outperform a random baseline even for proportionally rare
senses, without explicit sense supervision. However, performance varies
considerably even among models with similar architectures and pretraining
regimes, with especially large differences for rare word senses, revealing that
CWE models are not all created equal when it comes to approximating word senses
in their native representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. (arXiv:2109.09784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09784">
<div class="article-summary-box-inner">
<span><p>State-of-the-art abstractive summarization systems often generate
\emph{hallucinations}; i.e., content that is not directly inferable from the
source text. Despite being assumed incorrect, many of the hallucinated contents
are consistent with world knowledge (factual hallucinations). Including these
factual hallucinations into a summary can be beneficial in providing additional
background information. In this work, we propose a novel detection approach
that separates factual from non-factual hallucinations of entities. Our method
is based on an entity's prior and posterior probabilities according to
pre-trained and finetuned masked language models, respectively. Empirical
results suggest that our method vastly outperforms three strong baselines in
both accuracy and F1 scores and has a strong correlation with human judgments
on factuality classification tasks. Furthermore, our approach can provide
insight into whether a particular hallucination is caused by the summarizer's
pre-training or fine-tuning step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Induction Through the Lens of Visual Perception. (arXiv:2109.09790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09790">
<div class="article-summary-box-inner">
<span><p>Most previous work on grammar induction focuses on learning phrasal or
dependency structure purely from text. However, because the signal provided by
text alone is limited, recently introduced visually grounded syntax models make
use of multimodal information leading to improved performance in constituency
grammar induction. However, as compared to dependency grammars, constituency
grammars do not provide a straightforward way to incorporate visual information
without enforcing language-specific heuristics. In this paper, we propose an
unsupervised grammar induction model that leverages word concreteness and a
structural vision-based heuristic to jointly learn constituency-structure and
dependency-structure grammars. Our experiments find that concreteness is a
strong indicator for learning dependency grammars, improving the direct
attachment score (DAS) by over 50\% as compared to state-of-the-art models
trained on pure text. Next, we propose an extension of our model that leverages
both word concreteness and visual semantic role labels in constituency and
dependency parsing. Our experiments show that the proposed extension
outperforms the current state-of-the-art visually grounded models in
constituency parsing even with a smaller grammar size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Fake News: Robust Generalisable News Classification Using Transformers. (arXiv:2109.09796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09796">
<div class="article-summary-box-inner">
<span><p>As online news has become increasingly popular and fake news increasingly
prevalent, the ability to audit the veracity of online news content has become
more important than ever. Such a task represents a binary classification
challenge, for which transformers have achieved state-of-the-art results. Using
the publicly available ISOT and Combined Corpus datasets, this study explores
transformers' abilities to identify fake news, with particular attention given
to investigating generalisation to unseen datasets with varying styles, topics
and class distributions. Moreover, we explore the idea that opinion-based news
articles cannot be classified as real or fake due to their subjective nature
and often sensationalised language, and propose a novel two-step classification
pipeline to remove such articles from both model training and the final
deployed inference system. Experiments over the ISOT and Combined Corpus
datasets show that transformers achieve an increase in F1 scores of up to 4.9%
for out of distribution generalisation compared to baseline approaches, with a
further increase of 10.1% following the implementation of our two-step
classification pipeline. To the best of our knowledge, this study is the first
to investigate generalisation of transformers in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Span Representation for Domain-adapted Coreference Resolution. (arXiv:2109.09811v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09811">
<div class="article-summary-box-inner">
<span><p>Recent work has shown fine-tuning neural coreference models can produce
strong performance when adapting to different domains. However, at the same
time, this can require a large amount of annotated target examples. In this
work, we focus on supervised domain adaptation for clinical notes, proposing
the use of concept knowledge to more efficiently adapt coreference models to a
new domain. We develop methods to improve the span representations via (1) a
retrofitting loss to incentivize span representations to satisfy a
knowledge-based distance function and (2) a scaffolding loss to guide the
recovery of knowledge from the span representation. By integrating these
losses, our model is able to improve our baseline precision and F-1 score. In
particular, we show that incorporating knowledge with end-to-end coreference
models results in better performance on the most challenging, domain-specific
spans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Methods for Anaphoric Zero Pronouns. (arXiv:2109.09825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09825">
<div class="article-summary-box-inner">
<span><p>In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and
many others, unrealized (null) arguments in certain syntactic positions can
refer to a previously introduced entity, and are thus called anaphoric zero
pronouns. The existing resources for studying anaphoric zero pronoun
interpretation are however still limited. In this paper, we use five data
augmentation methods to generate and detect anaphoric zero pronouns
automatically. We use the augmented data as additional training materials for
two anaphoric zero pronoun systems for Arabic. Our experimental results show
that data augmentation improves the performance of the two systems, surpassing
the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamSide: A Fully-Customizable Open-Source Toolkit for Efficient Annotation of Meaning Representations. (arXiv:2109.09853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09853">
<div class="article-summary-box-inner">
<span><p>This demonstration paper presents StreamSide, an open-source toolkit for
annotating multiple kinds of meaning representations. StreamSide supports
frame-based annotation schemes e.g., Abstract Meaning Representation (AMR) and
frameless annotation schemes e.g., Widely Interpretable Semantic Representation
(WISeR). Moreover, it supports both sentence-level and document-level
annotation by allowing annotators to create multi-rooted graphs for input text.
It can open and automatically convert between several types of input formats
including plain text, Penman notation, and its own JSON format enabling richer
annotation. It features reference frames for AMR predicate argument structures,
and also concept-to-text alignment. StreamSide is released under the Apache 2.0
license, and is completely open-source so that it can be customized to annotate
enriched meaning representations in different languages (e.g., Uniform Meaning
Representations). All StreamSide resources are publicly distributed through our
open source project at: https://github.com/emorynlp/StreamSide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. (arXiv:2109.09858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09858">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) is a graphical meaning representation
language designed to represent propositional information about argument
structure. However, at present it is unable to satisfyingly represent
non-veridical intensional contexts, often licensing inappropriate inferences.
In this paper, we show how to resolve the problem of non-veridicality without
appealing to layered graphs through a mapping from AMRs into Simply-Typed
Lambda Calculus (STLC). At least for some cases, this requires the introduction
of a new role :content which functions as an intensional operator. The
translation proposed is inspired by the formal linguistics literature on the
event semantics of attitude reports. Next, we address the interaction of
quantifier scope and intensional operators in so-called de re/de dicto
ambiguities. We adopt a scope node from the literature and provide an explicit
multidimensional semantics utilizing Cooper storage which allows us to derive
the de re and de dicto scope readings as well as intermediate scope readings
which prove difficult for accounts without a scope node.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Identification with a Reciprocal Rank Classifier. (arXiv:2109.09862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09862">
<div class="article-summary-box-inner">
<span><p>Language identification is a critical component of language processing
pipelines (Jauhiainen et al.,2019) and is not a solved problem in real-world
settings. We present a lightweight and effective language identifier that is
robust to changes of domain and to the absence of copious training data.
</p>
<p>The key idea for classification is that the reciprocal of the rank in a
frequency table makes an effective additive feature score, hence the term
Reciprocal Rank Classifier (RRC). The key finding for language classification
is that ranked lists of words and frequencies of characters form a sufficient
and robust representation of the regularities of key languages and their
orthographies.
</p>
<p>We test this on two 22-language data sets and demonstrate zero-effort domain
adaptation from a Wikipedia training set to a Twitter test set. When trained on
Wikipedia but applied to Twitter the macro-averaged F1-score of a
conventionally trained SVM classifier drops from 90.9% to 77.7%. By contrast,
the macro F1-score of RRC drops only from 93.1% to 90.6%. These classifiers are
compared with those from fastText and langid. The RRC performs better than
these established systems in most experiments, especially on short Wikipedia
texts and Twitter.
</p>
<p>The RRC classifier can be improved for particular domains and conversational
situations by adding words to the ranked lists. Using new terms learned from
such conversations, we demonstrate a further 7.9% increase in accuracy of
sample message classification, and 1.7% increase for conversation
classification. Surprisingly, this made results on Twitter data slightly worse.
</p>
<p>The RRC classifier is available as an open source Python package
(https://github.com/LivePersonInc/lplangid).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Short Text Clustering. (arXiv:2109.09894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09894">
<div class="article-summary-box-inner">
<span><p>Effective representation learning is critical for short text clustering due
to the sparse, high-dimensional and noise attributes of short text corpus.
Existing pre-trained models (e.g., Word2vec and BERT) have greatly improved the
expressiveness for short text representations with more condensed,
low-dimensional and continuous features compared to the traditional
Bag-of-Words (BoW) model. However, these models are trained for general
purposes and thus are suboptimal for the short text clustering task. In this
paper, we propose two methods to exploit the unsupervised autoencoder (AE)
framework to further tune the short text representations based on these
pre-trained text models for optimal clustering performance. In our first method
Structural Text Network Graph Autoencoder (STN-GAE), we exploit the structural
text information among the corpus by constructing a text network, and then
adopt graph convolutional network as encoder to fuse the structural features
with the pre-trained text features for text representation learning. In our
second method Soft Cluster Assignment Autoencoder (SCA-AE), we adopt an extra
soft cluster assignment constraint on the latent space of autoencoder to
encourage the learned text representations to be more clustering-friendly. We
tested two methods on seven popular short text datasets, and the experimental
results show that when only using the pre-trained model for short text
clustering, BERT performs better than BoW and Word2vec. However, as long as we
further tune the pre-trained representations, the proposed method like SCA-AE
can greatly increase the clustering performance, and the accuracy improvement
compared to use BERT alone could reach as much as 14\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization in Text-based Games via Hierarchical Reinforcement Learning. (arXiv:2109.09968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09968">
<div class="article-summary-box-inner">
<span><p>Deep reinforcement learning provides a promising approach for text-based
games in studying natural language communication between humans and artificial
agents. However, the generalization still remains a big challenge as the agents
depend critically on the complexity and variety of training tasks. In this
paper, we address this problem by introducing a hierarchical framework built
upon the knowledge graph-based RL agent. In the high level, a meta-policy is
executed to decompose the whole game into a set of subtasks specified by
textual goals, and select one of them based on the KG. Then a sub-policy in the
low level is executed to conduct goal-conditioned reinforcement learning. We
carry out experiments on games with various difficulty levels and show that the
proposed method enjoys favorable generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Kernel-Smoothed Machine Translation with Retrieved Examples. (arXiv:2109.09991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09991">
<div class="article-summary-box-inner">
<span><p>How to effectively adapt neural machine translation (NMT) models according to
emerging cases without retraining? Despite the great success of neural machine
translation, updating the deployed models online remains a challenge. Existing
non-parametric approaches that retrieve similar examples from a database to
guide the translation process are promising but are prone to overfit the
retrieved examples. However, non-parametric methods are prone to overfit the
retrieved examples. In this work, we propose to learn Kernel-Smoothed
Translation with Example Retrieval (KSTER), an effective approach to adapt
neural machine translation models online. Experiments on domain adaptation and
multi-domain machine translation datasets show that even without expensive
retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over
the best existing online adaptation methods. The code and trained models are
released at https://github.com/jiangqn/KSTER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negation-Instance Based Evaluation of End-to-End Negation Resolution. (arXiv:2109.10013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10013">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the task of negation resolution, which includes the
subtasks of cue detection (e.g. "not", "never") and scope resolution. In the
context of previous shared tasks, a variety of evaluation metrics have been
proposed. Subsequent works usually use different subsets of these, including
variations and custom implementations, rendering meaningful comparisons between
systems difficult. Examining the problem both from a linguistic perspective and
from a downstream viewpoint, we here argue for a negation-instance based
approach to evaluating negation resolution. Our proposed metrics correspond to
expectations over per-instance scores and hence are intuitively interpretable.
To render research comparable and to foster future work, we provide results for
a set of current state-of-the-art systems for negation resolution on three
English corpora, and make our implementation of the evaluation scripts publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Comments are Equal: Insights into Comment Moderation from a Topic-Aware Model. (arXiv:2109.10033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10033">
<div class="article-summary-box-inner">
<span><p>Moderation of reader comments is a significant problem for online news
platforms. Here, we experiment with models for automatic moderation, using a
dataset of comments from a popular Croatian newspaper. Our analysis shows that
while comments that violate the moderation rules mostly share common linguistic
and thematic features, their content varies across the different sections of
the newspaper. We therefore make our models topic-aware, incorporating semantic
features from a topic model into the classification decision. Our results show
that topic information improves the performance of the model, increases its
confidence in correct outputs, and helps us understand the model's outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10044">
<div class="article-summary-box-inner">
<span><p>This report describes the parsing problem for Combinatory Categorial Grammar
(CCG), showing how a combination of Transformer-based neural models and a
symbolic CCG grammar can lead to substantial gains over existing approaches.
The report also documents a 20-year research program, showing how NLP methods
have evolved over this time. The staggering accuracy improvements provided by
neural models for CCG parsing can be seen as a reflection of the improvements
seen in NLP more generally. The report provides a minimal introduction to CCG
and CCG parsing, with many pointers to the relevant literature. It then
describes the CCG supertagging problem, and some recent work from Tian et al.
(2020) which applies Transformer-based models to supertagging with great
effect. I use this existing model to develop a CCG multitagger, which can serve
as a front-end to an existing CCG parser. Simply using this new multitagger
provides substantial gains in parsing accuracy. I then show how a
Transformer-based model from the parsing literature can be combined with the
grammar-based CCG parser, setting a new state-of-the-art for the CCGbank
parsing task of almost 93% F-score for labelled dependencies, with complete
sentence accuracies of over 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. (arXiv:2109.10052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10052">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate what types of stereotypical information are
captured by pretrained language models. We present the first dataset comprising
stereotypical attributes of a range of social groups and propose a method to
elicit stereotypes encoded by pretrained language models in an unsupervised
fashion. Moreover, we link the emergent stereotypes to their manifestation as
basic emotions as a means to study their emotional effects in a more
generalized manner. To demonstrate how our methods can be used to analyze
emotion and stereotype shifts due to linguistic experience, we use fine-tuning
on news sources as a case study. Our experiments expose how attitudes towards
different social groups vary across models and how quickly emotions and
stereotypes can shift at the fine-tuning stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10080">
<div class="article-summary-box-inner">
<span><p>Adverse Drug Event (ADE) extraction mod-els can rapidly examine large
collections of so-cial media texts, detecting mentions of drug-related adverse
reactions and trigger medicalinvestigations. However, despite the recent
ad-vances in NLP, it is currently unknown if suchmodels are robust in face
ofnegation, which ispervasive across language varieties.In this paper we
evaluate three state-of-the-artsystems, showing their fragility against
nega-tion, and then we introduce two possible strate-gies to increase the
robustness of these mod-els: a pipeline approach, relying on a
specificcomponent for negation detection; an augmen-tation of an ADE extraction
dataset to artifi-cially create negated samples and further trainthe models.We
show that both strategies bring significantincreases in performance, lowering
the num-ber of spurious entities predicted by the mod-els. Our dataset and code
will be publicly re-leased to encourage research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. (arXiv:2109.10086v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10086">
<div class="article-summary-box-inner">
<span><p>In neural Information Retrieval (IR), ongoing research is directed towards
improving the first retriever in ranking pipelines. Learning dense embeddings
to conduct retrieval using efficient approximate nearest neighbors methods has
proven to work well. Meanwhile, there has been a growing interest in learning
\emph{sparse} representations for documents and queries, that could inherit
from the desirable properties of bag-of-words models such as the exact matching
of terms and the efficiency of inverted indexes. Introduced recently, the
SPLADE model provides highly sparse representations and competitive results
with respect to state-of-the-art dense and sparse approaches. In this paper, we
build on SPLADE and propose several significant improvements in terms of
effectiveness and/or efficiency. More specifically, we modify the pooling
mechanism, benchmark a model solely based on document expansion, and introduce
models trained with distillation. We also report results on the BEIR benchmark.
Overall, SPLADE is considerably improved with more than $9$\% gains on NDCG@10
on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InvBERT: Text Reconstruction from Contextualized Embeddings used for Derived Text Formats of Literary Works. (arXiv:2109.10104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10104">
<div class="article-summary-box-inner">
<span><p>Digital Humanities and Computational Literary Studies apply text mining
methods to investigate literature. Such automated approaches enable
quantitative studies on large corpora which would not be feasible by manual
inspection alone. However, due to copyright restrictions, the availability of
relevant digitized literary works is limited. Derived Text Formats (DTFs) have
been proposed as a solution. Here, textual materials are transformed in such a
way that copyright-critical features are removed, but that the use of certain
analytical methods remains possible. Contextualized word embeddings produced by
transformer-encoders (like BERT) are promising candidates for DTFs because they
allow for state-of-the-art performance on various analytical tasks and, at
first sight, do not disclose the original text. However, in this paper we
demonstrate that under certain conditions the reconstruction of the original
copyrighted text becomes feasible and its publication in the form of
contextualized word representations is not safe. Our attempts to invert BERT
suggest, that publishing parts of the encoder together with the contextualized
embeddings is critical, since it allows to generate data to train a decoder
with a reconstruction accuracy sufficient to violate copyright laws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Difficulty of Segmenting Words with Attention. (arXiv:2109.10107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10107">
<div class="article-summary-box-inner">
<span><p>Word segmentation, the problem of finding word boundaries in speech, is of
interest for a range of tasks. Previous papers have suggested that for
sequence-to-sequence models trained on tasks such as speech translation or
speech recognition, attention can be used to locate and segment the words. We
show, however, that even on monolingual data this approach is brittle. In our
experiments with different input types, data sizes, and segmentation
algorithms, only models trained to predict phones from words succeed in the
task. Models trained to predict words from either phones or speech (i.e., the
opposite direction needed to generalize to new data), yield much worse results,
suggesting that attention-based segmentation is only useful in limited
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review on Summarizing Financial News Using Deep Learning. (arXiv:2109.10118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10118">
<div class="article-summary-box-inner">
<span><p>Investors make investment decisions depending on several factors such as
fundamental analysis, technical analysis, and quantitative analysis. Another
factor on which investors can make investment decisions is through sentiment
analysis of news headlines, the sole purpose of this study. Natural Language
Processing techniques are typically used to deal with such a large amount of
data and get valuable information out of it. NLP algorithms convert raw text
into numerical representations that machines can easily understand and
interpret. This conversion can be done using various embedding techniques. In
this research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,
GloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.
This work aims to evaluate these model's performance to choose the robust model
in identifying the significant factors influencing the prediction. During this
research, it was expected that Deep Leaming would be applied to get the desired
results or achieve better accuracy than the state-of-the-art. The models are
compared to check their outputs to know which one has performed better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvFiT: Conversational Fine-Tuning of Pretrained Language Models. (arXiv:2109.10126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10126">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) pretrained on large text collections
are proven to store a wealth of semantic knowledge. However, 1) they are not
effective as sentence encoders when used off-the-shelf, and 2) thus typically
lag behind conversationally pretrained (e.g., via response selection) encoders
on conversational tasks such as intent detection (ID). In this work, we propose
ConvFiT, a simple and efficient two-stage procedure which turns any pretrained
LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and
task-specialised sentence encoder (after Stage 2). We demonstrate that 1)
full-blown conversational pretraining is not required, and that LMs can be
quickly transformed into effective conversational encoders with much smaller
amounts of unannotated data; 2) pretrained LMs can be fine-tuned into
task-specialised sentence encoders, optimised for the fine-grained semantics of
a particular task. Consequently, such specialised sentence encoders allow for
treating ID as a simple semantic similarity task based on interpretable nearest
neighbours retrieval. We validate the robustness and versatility of the ConvFiT
framework with such similarity-based inference on the standard ID evaluation
sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board,
with particular gains in the most challenging, few-shot setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement. (arXiv:2109.10133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10133">
<div class="article-summary-box-inner">
<span><p>Many recent works have demonstrated that unsupervised sentence
representations of neural networks encode syntactic information by observing
that neural language models are able to predict the agreement between a verb
and its subject. We take a critical look at this line of research by showing
that it is possible to achieve high accuracy on this agreement task with simple
surface heuristics, indicating a possible flaw in our assessment of neural
networks' syntactic ability. Our fine-grained analyses of results on the
long-range French object-verb agreement show that contrary to LSTMs,
Transformers are able to capture a non-trivial amount of grammatical structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with Noisy Labels for Natural Language Understanding. (arXiv:2109.10147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10147">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is extensively used to compress and deploy large
pre-trained language models on edge devices for real-world applications.
However, one neglected area of research is the impact of noisy (corrupted)
labels on KD. We present, to the best of our knowledge, the first study on KD
with noisy labels in Natural Language Understanding (NLU). We document the
scope of the problem and present two methods to mitigate the impact of label
noise. Experiments on the GLUE benchmark show that our methods are effective
even under high noise levels. Nevertheless, our results indicate that more
research is necessary to cope with label noise under the KD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10164">
<div class="article-summary-box-inner">
<span><p>Intermediate layer knowledge distillation (KD) can improve the standard KD
technique (which only targets the output of teacher and student models)
especially over large pre-trained language models. However, intermediate layer
distillation suffers from excessive computational burdens and engineering
efforts required for setting up a proper layer mapping. To address these
problems, we propose a RAndom Intermediate Layer Knowledge Distillation
(RAIL-KD) approach in which, intermediate layers from the teacher model are
selected randomly to be distilled into the intermediate layers of the student
model. This randomized selection enforce that: all teacher layers are taken
into account in the training process, while reducing the computational cost of
intermediate layer distillation. Also, we show that it act as a regularizer for
improving the generalizability of the student model. We perform extensive
experiments on GLUE tasks as well as on out-of-domain test sets. We show that
our proposed RAIL-KD approach outperforms other state-of-the-art intermediate
layer KD methods considerably in both performance and training-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings. (arXiv:2109.10179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10179">
<div class="article-summary-box-inner">
<span><p>How do neural networks "perceive" speech sounds from unknown languages? Does
the typological similarity between the model's training language (L1) and an
unknown language (L2) have an impact on the model representations of L2 speech
signals? To answer these questions, we present a novel experimental design
based on representational similarity analysis (RSA) to analyze acoustic word
embeddings (AWEs) -- vector representations of variable-duration spoken-word
segments. First, we train monolingual AWE models on seven Indo-European
languages with various degrees of typological similarity. We then employ RSA to
quantify the cross-lingual similarity by simulating native and non-native
spoken-word processing using AWEs. Our experiments show that typological
similarity indeed affects the representational similarity of the models in our
study. We further discuss the implications of our work on modeling speech
processing and language similarity with neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranslateLocally: Blazing-fast translation running on the local CPU. (arXiv:2109.10194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10194">
<div class="article-summary-box-inner">
<span><p>Every day, millions of people sacrifice their privacy and browsing habits in
exchange for online machine translation. Companies and governments with
confidentiality requirements often ban online translation or pay a premium to
disable logging. To bring control back to the end user and demonstrate speed,
we developed translateLocally. Running locally on a desktop or laptop CPU,
translateLocally delivers cloud-like translation speed and quality even on 10
year old hardware. The open-source software is based on Marian and runs on
Linux, Windows, and macOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Source, Two Targets: Challenges and Rewards of Dual Decoding. (arXiv:2109.10197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10197">
<div class="article-summary-box-inner">
<span><p>Machine translation is generally understood as generating one target text
from an input source document. In this paper, we consider a stronger
requirement: to jointly generate two texts so that each output side effectively
depends on the other. As we discuss, such a device serves several practical
purposes, from multi-target machine translation to the generation of controlled
variations of the target text. We present an analysis of possible
implementations of dual decoding, and experiment with four applications.
Viewing the problem from multiple angles allows us to better highlight the
challenges of dual decoding and to also thoroughly analyze the benefits of
generating matched, rather than independent, translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blindness to Modality Helps Entailment Graph Mining. (arXiv:2109.10227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10227">
<div class="article-summary-box-inner">
<span><p>Understanding linguistic modality is widely seen as important for downstream
tasks such as Question Answering and Knowledge Graph Population. Entailment
Graph learning might also be expected to benefit from attention to modality. We
build Entailment Graphs using a news corpus filtered with a modality parser,
and show that stripping modal modifiers from predicates in fact increases
performance. This suggests that for some tasks, the pragmatics of modal
modification of predicates allows them to contribute as evidence of entailment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets. (arXiv:2109.10234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10234">
<div class="article-summary-box-inner">
<span><p>We introduce BERTweetFR, the first large-scale pre-trained language model for
French tweets. Our model is initialized using the general-domain French
language model CamemBERT which follows the base architecture of RoBERTa.
Experiments show that BERTweetFR outperforms all previous general-domain French
language models on two downstream Twitter NLP tasks of offensiveness
identification and named entity recognition. The dataset used in the
offensiveness detection task is first created and annotated by our team,
filling in the gap of such analytic datasets in French. We make our model
publicly available in the transformers library with the aim of promoting future
research in analytic tasks for French tweets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10246">
<div class="article-summary-box-inner">
<span><p>Linguistic representations derived from text alone have been criticized for
their lack of grounding, i.e., connecting words to their meanings in the
physical world. Vision-and-Language (VL) models, trained jointly on text and
image or video data, have been offered as a response to such criticisms.
However, while VL pretraining has shown success on multimodal tasks such as
visual question answering, it is not yet known how the internal linguistic
representations themselves compare to their text-only counterparts. This paper
compares the semantic representations learned via VL vs. text-only pretraining
for two recent VL models using a suite of analyses (clustering, probing, and
performance on a commonsense question answering task) in a language-only
setting. We find that the multimodal models fail to significantly outperform
the text-only variants, suggesting that future work is required if multimodal
pretraining is to be pursued as a means of improving NLP in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audiomer: A Convolutional Transformer for Keyword Spotting. (arXiv:2109.10252v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10252">
<div class="article-summary-box-inner">
<span><p>Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or reach
competitive performance after feature extraction through Fourier-based methods,
incurring a loss-floor. In this work, we introduce an architecture, Audiomer,
where we combine 1D Residual Networks with Performer Attention to achieve
state-of-the-art performance in Keyword Spotting with raw audio waveforms,
out-performing all previous methods while also being computationally cheaper,
much more parameter and data-efficient. Audiomer allows for deployment in
compute-constrained devices and training on smaller datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10255">
<div class="article-summary-box-inner">
<span><p>The recognition of hate speech and offensive language (HOF) is commonly
formulated as a classification task to decide if a text contains HOF. We
investigate whether HOF detection can profit by taking into account the
relationships between HOF and similar concepts: (a) HOF is related to sentiment
analysis because hate speech is typically a negative statement and expresses a
negative opinion; (b) it is related to emotion analysis, as expressed hate
points to the author experiencing (or pretending to experience) anger while the
addressees experience (or are intended to experience) fear. (c) Finally, one
constituting element of HOF is the mention of a targeted person or group. On
this basis, we hypothesize that HOF detection shows improvements when being
modeled jointly with these concepts, in a multi-task learning setup. We base
our experiments on existing data sets for each of these concepts (sentiment,
emotion, target of HOF) and evaluate our models as a participant (as team
IMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection
experiments in which we consider multiple available resources and submissions
to the shared task, we find that the combination of the CrowdFlower emotion
corpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target
detection data leads to an F1 =.79 in a multi-head multi-task learning model
based on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test
data, this result is more substantial with an increase by 2pp in F1 and a
considerable increase in recall. Across both data sets (2019, 2021), the recall
is particularly increased for the class of HOF (6pp for the 2019 data and 3pp
for the 2021 data), showing that MTL with emotion, sentiment, and target
identification is an appropriate approach for early warning systems that might
be deployed in social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10274">
<div class="article-summary-box-inner">
<span><p>In this paper, we connect language model adaptation with concepts of machine
learning theory. We consider a training setup with a large out-of-domain set
and a small in-domain set. As a first contribution, we derive how the benefit
of training a model on either set depends on the size of the sets and the
distance between their underlying distribution. As a second contribution, we
present how the most popular data selection techniques -- importance sampling,
intelligent data selection and influence functions -- can be presented in a
common framework which highlights their similarity and also their subtle
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From English to Signal Temporal Logic. (arXiv:2109.10294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10341">
<div class="article-summary-box-inner">
<span><p>Document-level neural machine translation (DocNMT) delivers coherent
translations by incorporating cross-sentence context. However, for most
language pairs there's a shortage of parallel documents, although parallel
sentences are readily available. In this paper, we study whether and how
contextual modeling in DocNMT is transferable from sentences to documents in a
zero-shot fashion (i.e. no parallel documents for student languages) through
multilingual modeling. Using simple concatenation-based DocNMT, we explore the
effect of 3 factors on multilingual transfer: the number of document-supervised
teacher languages, the data schedule for parallel documents at training, and
the data condition of parallel documents (genuine vs. backtranslated). Our
experiments on Europarl-7 and IWSLT-10 datasets show the feasibility of
multilingual transfer for DocNMT, particularly on document-specific metrics. We
observe that more teacher languages and adequate data schedule both contribute
to better transfer quality. Surprisingly, the transfer is less sensitive to the
data condition and multilingual DocNMT achieves comparable performance with
both back-translated and genuine document pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Guided Pre-Training for Open-Domain Question Answering. (arXiv:2109.10346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10346">
<div class="article-summary-box-inner">
<span><p>Answering complex open-domain questions requires understanding the latent
relations between involving entities. However, we found that the existing QA
datasets are extremely imbalanced in some types of relations, which hurts the
generalization performance over questions with long-tail relations. To remedy
this problem, in this paper, we propose a Relation-Guided Pre-Training
(RGPT-QA) framework. We first generate a relational QA dataset covering a wide
range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We
then pre-train a QA model to infer the latent relations from the question, and
then conduct extractive QA to get the target answer entity. We demonstrate that
by pretraining with propoed RGPT-QA techique, the popular open-domain QA model,
Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute
improvement in Exact Match accuracy on Natural Questions, TriviaQA, and
WebQuestions. Particularly, we show that RGPT-QA improves significantly on
questions with long-tail relations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informed Sampling for Diversity in Concept-to-Text NLG. (arXiv:2004.14364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14364">
<div class="article-summary-box-inner">
<span><p>Deep-learning models for language generation tasks tend to produce repetitive
output. Various methods have been proposed to encourage lexical diversity
during decoding, but this often comes at a cost to the perceived fluency and
adequacy of the output. In this work, we propose to ameliorate this cost by
using an Imitation Learning approach to explore the level of diversity that a
language generation model can reliably produce. Specifically, we augment the
decoding process with a meta-classifier trained to distinguish which words at
any given timestep will lead to high-quality output. We focus our experiments
on concept-to-text generation where models are sensitive to the inclusion of
irrelevant words due to the strict relation between input and output. Our
analysis shows that previous methods for diversity underperform in this
setting, while human evaluation suggests that our proposed method achieves a
high level of diversity with minimal effect to the output's fluency and
adequacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models. (arXiv:2009.13267v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13267">
<div class="article-summary-box-inner">
<span><p>The discrepancy between maximum likelihood estimation (MLE) and task measures
such as BLEU score has been studied before for autoregressive neural machine
translation (NMT) and resulted in alternative training algorithms (Ranzato et
al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,
MLE training remains the de facto approach for autoregressive NMT because of
its computational efficiency and stability. Despite this mismatch between the
training objective and task measure, we notice that the samples drawn from an
MLE-based trained NMT support the desired distribution -- there are samples
with much higher BLEU score comparing to the beam decoding output. To benefit
from this observation, we train an energy-based model to mimic the behavior of
the task measure (i.e., the energy-based model assigns lower energy to samples
with higher BLEU score), which is resulted in a re-ranking algorithm based on
the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal
energy models (over target sentence) and joint energy models (over both source
and target sentences). Our EBR with the joint energy model consistently
improves the performance of the Transformer-based NMT: +4 BLEU points on
IWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on
WMT'16 English-German tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not all parameters are born equal: Attention is mostly what you need. (arXiv:2010.11859v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11859">
<div class="article-summary-box-inner">
<span><p>Transformers are widely used in state-of-the-art machine translation, but the
key to their success is still unknown. To gain insight into this, we consider
three groups of parameters: embeddings, attention, and feed forward neural
network (FFN) layers. We examine the relative importance of each by performing
an ablation study where we initialise them at random and freeze them, so that
their weights do not change over the course of the training. Through this, we
show that the attention and FFN are equally important and fulfil the same
functionality in a model. We show that the decision about whether a component
is frozen or allowed to train is at least as important for the final model
performance as its number of parameters. At the same time, the number of
parameters alone is not indicative of a component's importance. Finally, while
the embedding layer is the least essential for machine translation tasks, it is
the most important component for language modelling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation. (arXiv:2101.00421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00421">
<div class="article-summary-box-inner">
<span><p>Machine translation systems are vulnerable to domain mismatch, especially in
a low-resource scenario. Out-of-domain translations are often of poor quality
and prone to hallucinations, due to exposure bias and the decoder acting as a
language model. We adopt two approaches to alleviate this problem: lexical
shortlisting restricted by IBM statistical alignments, and hypothesis
re-ranking based on similarity. The methods are computationally cheap, widely
known, but not extensively experimented on domain adaptation. We demonstrate
success on low-resource out-of-domain test sets, however, the methods are
ineffective when there is sufficient data or too great domain mismatch. This is
due to both the IBM model losing its advantage over the implicitly learned
neural alignment, and issues with subword segmentation of out-of-domain words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Retrieval Conversational Machine Reading. (arXiv:2102.08633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08633">
<div class="article-summary-box-inner">
<span><p>In conversational machine reading, systems need to interpret natural language
rules, answer high-level questions such as "May I qualify for VA health care
benefits?", and ask follow-up clarification questions whose answer is necessary
to answer the original question. However, existing works assume the rule text
is provided for each user question, which neglects the essential retrieval step
in real scenarios. In this work, we propose and investigate an open-retrieval
setting of conversational machine reading. In the open-retrieval setting, the
relevant rule texts are unknown so that a system needs to retrieve
question-relevant evidence from a collection of rule texts, and answer users'
high-level questions according to multiple retrieved rule texts in a
conversational manner. We propose MUDERN, a Multi-passage Discourse-aware
Entailment Reasoning Network which extracts conditions in the rule texts
through discourse segmentation, conducts multi-passage entailment reasoning to
answer user questions directly, or asks clarification follow-up questions to
inquiry more information. On our created OR-ShARC dataset, MUDERN achieves the
state-of-the-art performance, outperforming existing single-passage
conversational machine reading models as well as a new multi-passage
conversational machine reading baseline by a large margin. In addition, we
conduct in-depth analyses to provide new insights into this new setting and our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02258">
<div class="article-summary-box-inner">
<span><p>Mandarin-English code-switching (CS) is frequently used among East and
Southeast Asian people. However, the intra-sentence language switching of the
two very different languages makes recognizing CS speech challenging.
Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the
need for left-to-right beam decoding in autoregressive (AR) models and achieved
outstanding performance and fast inference speed, but it has not been applied
to Mandarin-English CS speech recognition. This paper takes advantage of the
Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We
further propose to change the Mandarin output target of the encoder to Pinyin
for faster encoder training and introduce the Pinyin-to-Mandarin decoder to
learn contextualized information. Moreover, we use word embedding label
smoothing to regularize the decoder with contextualized information and
projection matrix regularization to bridge that gap between the encoder and
decoder. We evaluate these methods on the SEAME corpus and achieved exciting
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders. (arXiv:2104.03630v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03630">
<div class="article-summary-box-inner">
<span><p>Powerful sentence encoders trained for multiple languages are on the rise.
These systems are capable of embedding a wide range of linguistic properties
into vector representations. While explicit probing tasks can be used to verify
the presence of specific linguistic properties, it is unclear whether the
vector representations can be manipulated to indirectly steer such properties.
For efficient learning, we investigate the use of a geometric mapping in
embedding space to transform linguistic properties, without any tuning of the
pre-trained sentence encoder or decoder. We validate our approach on three
linguistic properties using a pre-trained multilingual autoencoder and analyze
the results in both monolingual and cross-lingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condenser: a Pre-training Architecture for Dense Retrieval. (arXiv:2104.08253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08253">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer language models (LM) have become go-to text
representation encoders. Prior research fine-tunes deep LMs to encode text
sequences such as sentences and passages into single dense vector
representations for efficient text comparison and retrieval. However, dense
encoders require a lot of data and sophisticated techniques to effectively
train and suffer in low data situations. This paper finds a key reason is that
standard LMs' internal attention structure is not ready-to-use for dense
encoders, which needs to aggregate text information into the dense
representation. We propose to pre-train towards dense encoder with a novel
Transformer architecture, Condenser, where LM prediction CONditions on DENSE
Representation. Our experiments show Condenser improves over standard LM by
large margins on various text retrieval and similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12227">
<div class="article-summary-box-inner">
<span><p>Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12265">
<div class="article-summary-box-inner">
<span><p>This paper provides a new approach for offensive language and hate speech
detection on social media. Our approach incorporates an offensive lexicon
composed of implicit and explicit offensive and swearing expressions annotated
with binary classes: context-dependent and context-independent offensive. Due
to the severity of the hate speech and offensive comments in Brazil, and the
lack of research in Portuguese, Brazilian Portuguese is the language used to
validate the proposed method. Nevertheless, our proposal may be applied to any
other language or domain. Based on the obtained results, the proposed approach
showed high-performance overcoming the current baselines for European and
Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction. (arXiv:2105.15033v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15033">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph has been proven effective in modeling structured information
and conceptual knowledge, especially in the medical domain. However, the lack
of high-quality annotated corpora remains a crucial problem for advancing the
research and applications on this task. In order to accelerate the research for
domain-specific knowledge graphs in the medical domain, we introduce DiaKG, a
high-quality Chinese dataset for Diabetes knowledge graph, which contains
22,050 entities and 6,890 relations in total. We implement recent typical
methods for Named Entity Recognition and Relation Extraction as a benchmark to
evaluate the proposed dataset thoroughly. Empirical results show that the DiaKG
is challenging for most existing methods and further analysis is conducted to
discuss future research direction for improvements. We hope the release of this
dataset can assist the construction of diabetes knowledge graphs and facilitate
AI-based applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral Personalized Dialogue Generation with Contrastive Learning. (arXiv:2106.07857v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07857">
<div class="article-summary-box-inner">
<span><p>Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
Bilateral Personalized Dialogue Generation (BPDG) method for dyadic
conversation, which integrates user and robot personas into dialogue generation
via designing a dynamic persona-aware fusion method. To bridge the gap between
the learning objective function and evaluation metrics, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted with contrastive learning to
select the proper response from the generated candidates. Moreover, a bilateral
persona accuracy metric is designed to measure the degree of bilateral
personalization. Experimental results demonstrate that, compared with several
state-of-the-art methods, the final results of the proposed method are more
personalized and consistent with bilateral personas in terms of both automatic
and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09700">
<div class="article-summary-box-inner">
<span><p>Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You should evaluate your language model on marginal likelihood over tokenisations. (arXiv:2109.02550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02550">
<div class="article-summary-box-inner">
<span><p>Neural language models typically tokenise input text into sub-word units to
achieve an open vocabulary. The standard approach is to use a single canonical
tokenisation at both train and test time. We suggest that this approach is
unsatisfactory and may bottleneck our evaluation of language model performance.
Using only the one-best tokenisation ignores tokeniser uncertainty over
alternative tokenisations, which may hurt model out-of-domain performance.
</p>
<p>In this paper, we argue that instead, language models should be evaluated on
their marginal likelihood over tokenisations. We compare different estimators
for the marginal likelihood based on sampling, and show that it is feasible to
estimate the marginal likelihood with a manageable number of samples. We then
evaluate pretrained English and German language models on both the
one-best-tokenisation and marginal perplexities, and show that the marginal
perplexity can be significantly better than the one best, especially on
out-of-domain data. We link this difference in perplexity to the tokeniser
uncertainty as measured by tokeniser entropy. We discuss some implications of
our results for language model training and evaluation, particularly with
regard to tokenisation robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08828">
<div class="article-summary-box-inner">
<span><p>Empathy is a complex cognitive ability based on the reasoning of others'
affective states. In order to better understand others and express stronger
empathy in dialogues, we argue that two issues must be tackled at the same
time: (i) identifying which word is the cause for the other's emotion from his
or her utterance and (ii) reflecting those specific words in the response
generation. However, previous approaches for recognizing emotion cause words in
text require sub-utterance level annotations, which can be demanding. Taking
inspiration from social cognition, we leverage a generative estimator to infer
emotion cause words from utterances with no word-level label. Also, we
introduce a novel method based on pragmatics to make dialogue models focus on
targeted words in the input during generation. Our method is applicable to any
dialogue models with no additional training on the fly. We show our approach
improves multiple best-performing dialogue agents on generating more focused
empathetic responses in terms of both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09105">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have been ubiquitously leveraged in various tasks
including spoken language understanding (SLU). Spoken language requires careful
understanding of speaker interactions, dialog states and speech induced
multimodal behaviors to generate a meaningful representation of the
conversation. In this work, we propose to dissect SLU into three representative
properties:conversational (disfluency, pause, overtalk), channel (speaker-type,
turn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based
language models (BERT, RoBERTa) trained on spoken transcripts to investigate
its ability to understand multifarious properties in absence of any speech
cues. Empirical results indicate that LM is surprisingly good at capturing
conversational properties such as pause prediction and overtalk detection from
lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR
errors predictions. Additionally, pre-training the LM on spoken transcripts
restrain its linguistic understanding. Finally, we establish the efficacy and
transferability of the mentioned properties on two benchmark datasets:
Switchboard Dialog Act and Disfluency datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-22 23:09:13.662341255 UTC">2021-09-22 23:09:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>