<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-13T01:30:00Z">09-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04500">
<div class="article-summary-box-inner">
<span><p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for
semantic parsing in task-oriented dialog. Our model consists of an encoder
network that incrementally builds the semantic parse tree by predicting the
non-terminal label and its positions in the linearized tree. At the generation
time, the model constructs the semantic parse tree by recursively inserting the
predicted non-terminal labels at the predicted positions until termination.
RINE achieves state-of-the-art exact match accuracy on low- and high-resource
versions of the conversational semantic parsing benchmark TOP (Gupta et al.,
2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and
transition-based parsers. We also show that our model design is applicable to
nested named entity recognition task, where it performs on par with
state-of-the-art approach designed for that task. Finally, we demonstrate that
our approach is 2-3.5 times faster than the sequence-to-sequence model at
inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04513">
<div class="article-summary-box-inner">
<span><p>We present models which complete missing text given transliterations of
ancient Mesopotamian documents, originally written on cuneiform clay tablets
(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on
contextual cues to manually fill in missing parts in the text in a subjective
and time-consuming process. We identify that this challenge can be formulated
as a masked language modelling task, used mostly as a pretraining objective for
contextualized language models. Following, we develop several architectures
focusing on the Akkadian language, the lingua franca of the time. We find that
despite data scarcity (1M tokens) we can achieve state of the art performance
on missing tokens prediction (89% hit@5) using a greedy decoding scheme and
pretraining on data from other languages and different time periods. Finally,
we conduct human evaluations showing the applicability of our models in
assisting experts to transcribe texts in extinct languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04535">
<div class="article-summary-box-inner">
<span><p>Extracting moral sentiment from text is a vital component in understanding
public opinion, social movements, and policy decisions. The Moral Foundation
Theory identifies five moral foundations, each associated with a positive and
negative polarity. However, moral sentiment is often motivated by its targets,
which can correspond to individuals or collective entities. In this paper, we
introduce morality frames, a representation framework for organizing moral
attitudes directed at different entities, and come up with a novel and
high-quality annotated dataset of tweets written by US politicians. Then, we
propose a relational learning model to predict moral attitudes towards entities
and moral foundations jointly. We do qualitative and quantitative evaluations,
showing that moral sentiment towards entities differs highly across political
ideologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04543">
<div class="article-summary-box-inner">
<span><p>Style transfer aims to rewrite a source text in a different target style
while preserving its content. We propose a novel approach to this task that
leverages generic resources, and without using any task-specific parallel
(source-target) data outperforms existing unsupervised approaches on the two
most popular style transfer tasks: formality transfer and polarity swap. In
practice, we adopt a multi-step procedure which builds on a generic pre-trained
sequence-to-sequence model (BART). First, we strengthen the model's ability to
rewrite by further pre-training BART on both an existing collection of generic
paraphrases, as well as on synthetic pairs created using a general-purpose
lexical resource. Second, through an iterative back-translation approach, we
train two models, each in a transfer direction, so that they can provide each
other with synthetically generated pairs, dynamically in the training process.
Lastly, we let our best reresulting model generate static synthetic pairs to be
used in a supervised training regime. Besides methodology and state-of-the-art
results, a core contribution of this work is a reflection on the nature of the
two tasks we address, and how their differences are highlighted by their
response to our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04546">
<div class="article-summary-box-inner">
<span><p>We study the problem of generating arithmetic math word problems (MWPs) given
a math equation that specifies the mathematical computation and a context that
specifies the problem scenario. Existing approaches are prone to generating
MWPs that are either mathematically invalid or have unsatisfactory language
quality. They also either ignore the context or require manual specification of
a problem template, which compromises the diversity of the generated MWPs. In
this paper, we develop a novel MWP generation approach that leverages i)
pre-trained language models and a context keyword selection model to improve
the language quality of the generated MWPs and ii) an equation consistency
constraint for math equations to improve the mathematical validity of the
generated MWPs. Extensive quantitative and qualitative experiments on three
real-world MWP datasets demonstrate the superior performance of our approach
compared to various baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04550">
<div class="article-summary-box-inner">
<span><p>Temporal Knowledge Graphs store events in the form of subjects, relations,
objects, and timestamps which are often represented by dynamic heterogeneous
graphs. Event forecasting is a critical and challenging task in Temporal
Knowledge Graph reasoning that predicts the subject or object of an event in
the future. To obtain temporal embeddings multi-step away in the future,
existing methods learn generative models that capture the joint distribution of
the observed events. To reduce the high computation costs, these methods rely
on unrealistic assumptions of independence and approximations in training and
inference. In this work, we propose SeDyT, a discriminative framework that
performs sequence modeling on the dynamic entity embeddings to solve the
multi-step event forecasting problem. SeDyT consists of two components: a
Temporal Graph Neural Network that generates dynamic entity embeddings in the
past and a sequence model that predicts the entity embeddings in the future.
Compared with the generative models, SeDyT does not rely on any heuristic-based
probability model and has low computation complexity in both training and
inference. SeDyT is compatible with most Temporal Graph Neural Networks and
sequence models. We also design an efficient training method that trains the
two components in one gradient descent propagation. We evaluate the performance
of SeDyT on five popular datasets. By combining temporal Graph Neural Network
models and sequence models, SeDyT achieves an average of 2.4% MRR improvement
when not using the validation set and more than 10% MRR improvement when using
the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04552">
<div class="article-summary-box-inner">
<span><p>Selective rationalization aims to produce decisions along with rationales
(e.g., text highlights or word alignments between two sentences). Commonly,
rationales are modeled as stochastic binary masks, requiring sampling-based
gradient estimators, which complicates training and requires careful
hyperparameter tuning. Sparse attention mechanisms are a deterministic
alternative, but they lack a way to regularize the rationale extraction (e.g.,
to control the sparsity of a text highlight or the number of alignments). In
this paper, we present a unified framework for deterministic extraction of
structured explanations via constrained inference on a factor graph, forming a
differentiable layer. Our approach greatly eases training and rationale
regularization, generally outperforming previous work on what comes to
performance and plausibility of the extracted rationales. We further provide a
comparative study of stochastic and deterministic methods for rationale
extraction for classification and natural language inference tasks, jointly
assessing their predictive power, quality of the explanations, and model
variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04556">
<div class="article-summary-box-inner">
<span><p>State-of-the-art multilingual systems rely on shared vocabularies that
sufficiently cover all considered languages. To this end, a simple and
frequently used approach makes use of subword vocabularies constructed jointly
over several languages. We hypothesize that such vocabularies are suboptimal
due to false positives (identical subwords with different meanings across
languages) and false negatives (different subwords with similar meanings). To
address these issues, we propose Subword Mapping and Anchoring across Languages
(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts
subword alignments using an unsupervised state-of-the-art mapping technique and
uses them to create cross-lingual anchors based on subword similarities. We
demonstrate the benefits of SMALA for cross-lingual natural language inference
(XNLI), where it improves zero-shot transfer to an unseen language without
task-specific data, but only by sharing subword embeddings. Moreover, in neural
machine translation, we show that joint subword vocabularies obtained with
SMALA lead to higher BLEU scores on sentences that contain many false positives
and false negatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04562">
<div class="article-summary-box-inner">
<span><p>Human conversations naturally evolve around different topics and fluently
move between them. In research on dialog systems, the ability to actively and
smoothly transition to new topics is often ignored. In this paper we introduce
TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human
annotations on topic shifts. Based on TIAGE, we introduce three tasks to
investigate different scenarios of topic-shift modeling in dialog settings:
topic-shift detection, topic-shift triggered response generation and
topic-aware dialog generation. Experiments on these tasks show that the
topic-shift signals in TIAGE are useful for topic-shift response generation. On
the other hand, dialog systems still struggle to decide when to change topic.
This indicates further research is needed in topic-shift aware dialog modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04574">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have gained increasing popularity achieving
state-of-the-art performance in many research fields including speech
translation. However, Transformer's quadratic complexity with respect to the
input sequence length prevents its adoption as is with audio signals, which are
typically represented by long sequences. Current solutions resort to an initial
sub-optimal compression based on a fixed sampling of raw audio features.
Therefore, potentially useful linguistic information is not accessible to
higher-level layers in the architecture. To solve this issue, we propose
Speechformer, an architecture that, thanks to reduced memory usage in the
attention layers, avoids the initial lossy compression and aggregates
information only at a higher level according to more informed linguistic
criteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy
of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and
of up to 4.0 BLEU in a low resource scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04587">
<div class="article-summary-box-inner">
<span><p>The dominant paradigm for semantic parsing in recent years is to formulate
parsing as a sequence-to-sequence task, generating predictions with
auto-regressive sequence decoders. In this work, we explore an alternative
paradigm. We formulate semantic parsing as a dependency parsing task, applying
graph-based decoding techniques developed for syntactic parsing. We compare
various decoding techniques given the same pre-trained Transformer encoder on
the TOP dataset, including settings where training data is limited or contains
only partially-annotated examples. We find that our graph-based approach is
competitive with sequence decoders on the standard setting, and offers
significant improvements in data efficiency and settings where
partially-annotated data is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04588">
<div class="article-summary-box-inner">
<span><p>The success of bidirectional encoders using masked language models, such as
BERT, on numerous natural language processing tasks has prompted researchers to
attempt to incorporate these pre-trained models into neural machine translation
(NMT) systems. However, proposed methods for incorporating pre-trained models
are non-trivial and mainly focus on BERT, which lacks a comparison of the
impact that other pre-trained models may have on translation performance. In
this paper, we demonstrate that simply using the output (contextualized
embeddings) of a tailored and suitable bilingual pre-trained language model
(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art
translation performance. Moreover, we also propose a stochastic layer selection
approach and a concept of dual-directional translation model to ensure the
sufficient utilization of contextualized embeddings. In the case of without
using back translation, our best models achieve BLEU scores of 30.45 for En-&gt;De
and 38.61 for De-&gt;En on the IWSLT'14 dataset, and 31.26 for En-&gt;De and 34.94
for De-&gt;En on the WMT'14 dataset, which exceeds all published numbers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04593">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural machine translation (NMT) have pushed the quality
of machine translation systems to the point where they are becoming widely
adopted to build competitive systems. However, there is still a large number of
languages that are yet to reap the benefits of NMT. In this paper, we provide
the first large-scale case study of the practical application of MT in the
Turkic language family in order to realize the gains of NMT for Turkic
languages under high-resource to extremely low-resource scenarios. In addition
to presenting an extensive analysis that identifies the bottlenecks towards
building competitive systems to ameliorate data scarcity, our study has several
key contributions, including, i) a large parallel corpus covering 22 Turkic
languages consisting of common public datasets in combination with new datasets
of approximately 2 million parallel sentences, ii) bilingual baselines for 26
language pairs, iii) novel high-quality test sets in three different
translation domains and iv) human evaluation scores. All models, scripts, and
data will be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04600">
<div class="article-summary-box-inner">
<span><p>Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work, we present
EVOQUER, a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video, the temporal grounding model predicts the target
interval, and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets,
Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04602">
<div class="article-summary-box-inner">
<span><p>Current language models are usually trained using a self-supervised scheme,
where the main focus is learning representations at the word or sentence level.
However, there has been limited progress in generating useful discourse-level
representations. In this work, we propose to use ideas from predictive coding
theory to augment BERT-style language models with a mechanism that allows them
to learn suitable discourse-level representations. As a result, our proposed
approach is able to predict future sentences using explicit top-down
connections that operate at the intermediate layers of the network. By
experimenting with benchmarks designed to evaluate discourse-related knowledge
using pre-trained sentence representations, we demonstrate that our approach
improves performance in 6 out of 11 tasks by excelling in discourse
relationship detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04604">
<div class="article-summary-box-inner">
<span><p>The general goal of text simplification (TS) is to reduce text complexity for
human consumption. This paper investigates another potential use of neural TS:
assisting machines performing natural language processing (NLP) tasks. We
evaluate the use of neural TS in two ways: simplifying input texts at
prediction time and augmenting data to provide machines with additional
information during training. We demonstrate that the latter scenario provides
positive effects on machine performance on two separate datasets. In
particular, the latter use of TS improves the performances of LSTM (1.82-1.98%)
and SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,
real-world relation extraction task. Further, the same setting yields
improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT
text classifier on MNLI, a practical natural language inference dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04607">
<div class="article-summary-box-inner">
<span><p>We present IndoBERTweet, the first large-scale pretrained model for
Indonesian Twitter that is trained by extending a monolingually-trained
Indonesian BERT model with additive domain-specific vocabulary. We focus in
particular on efficient model adaptation under vocabulary mismatch, and
benchmark different ways of initializing the BERT embedding layer for new word
types. We find that initializing with the average BERT subword embedding makes
pretraining five times faster, and is more effective than proposed methods for
vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Long Dialogue Summarization: What Works and What's Next. (arXiv:2109.04609v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04609">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization helps readers capture salient information from long
conversations in meetings, interviews, and TV series. However, real-world
dialogues pose a great challenge to current summarization models, as the
dialogue length typically exceeds the input limits imposed by recent
transformer-based pre-trained models, and the interactive nature of dialogues
makes relevant information more context-dependent and sparsely distributed than
news articles. In this work, we perform a comprehensive study on long dialogue
summarization by investigating three strategies to deal with the lengthy input
problem and locate relevant information: (1) extended transformer models such
as Longformer, (2) retrieve-then-summarize pipeline models with several
dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding
models such as HMNet. Our experimental results on three long dialogue datasets
(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline
models yield the best performance. We also demonstrate that the summary quality
can be further improved with a stronger retrieval model and pretraining on
proper external summarization datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04611">
<div class="article-summary-box-inner">
<span><p>Transformer-based rankers have shown state-of-the-art performance. However,
their self-attention operation is mostly unable to process long sequences. One
of the common approaches to train these rankers is to heuristically select some
segments of each document, such as the first segment, as training data.
However, these segments may not contain the query-related parts of documents.
To address this problem, we propose query-driven segment selection from long
documents to build training data. The segment selector provides relevant
samples with more accurate labels and non-relevant samples which are harder to
be predicted. The experimental results show that the basic BERT-based ranker
trained with the proposed segment selector significantly outperforms that
trained by the heuristically selected segments, and performs equally to the
state-of-the-art model with localized self-attention that can process longer
input sequences. Our findings open up new direction to design efficient
transformer-based rankers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04620">
<div class="article-summary-box-inner">
<span><p>Current approaches to incorporating terminology constraints in machine
translation (MT) typically assume that the constraint terms are provided in
their correct morphological forms. This limits their application to real-world
scenarios where constraint terms are provided as lemmas. In this paper, we
introduce a modular framework for incorporating lemma constraints in neural MT
(NMT) in which linguistic knowledge and diverse types of NMT models can be
flexibly applied. It is based on a novel cross-lingual inflection module that
inflects the target lemma constraints based on the source context. We explore
linguistically motivated rule-based and data-driven neural-based inflection
modules and design English-German health and English-Lithuanian news test
suites to evaluate them in domain adaptation and low-resource MT settings.
Results show that our rule-based inflection module helps NMT models incorporate
lemma constraints more accurately than a neural module and outperforms the
existing end-to-end approach with lower training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
<div class="article-summary-box-inner">
<span><p>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema(definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5)is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04650">
<div class="article-summary-box-inner">
<span><p>GPT-3 shows remarkable in-context learning ability of large-scale language
models (LMs) trained on hundreds of billion scale data. Here we address some
remaining issues less reported by the GPT-3 paper, such as a non-English LM,
the performances of different sized models, and the effect of recently
introduced prompt optimization on in-context learning. To achieve this, we
introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric
corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA
with our training configuration shows state-of-the-art in-context zero-shot and
few-shot learning performances on various downstream tasks in Korean. Also, we
show the performance benefits of prompt-based learning and demonstrate how it
can be integrated into the prompt engineering pipeline. Then we discuss the
possibility of materializing the No Code AI paradigm by providing AI
prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,
an interactive prompt engineering interface. Lastly, we demonstrate the
potential of our methods with three successful in-house applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04652">
<div class="article-summary-box-inner">
<span><p>Natural language relies on a finite lexicon to express an unbounded set of
emerging ideas. One result of this tension is the formation of new
compositions, such that existing linguistic units can be combined with emerging
items into novel expressions. We develop a framework that exploits the
cognitive mechanisms of chaining and multimodal knowledge to predict emergent
compositional expressions through time. We present the syntactic frame
extension model (SFEM) that draws on the theory of chaining and knowledge from
"percept", "concept", and "language" to infer how verbs extend their frames to
form new compositions with existing and novel nouns. We evaluate SFEM
rigorously on the 1) modalities of knowledge and 2) categorization models of
chaining, in a syntactically parsed English corpus over the past 150 years. We
show that multimodal SFEM predicts newly emerged verb syntax and arguments
substantially better than competing models using purely linguistic or unimodal
knowledge. We find support for an exemplar view of chaining as opposed to a
prototype view and reveal how the joint approach of multimodal chaining may be
fundamental to the creation of literal and figurative language uses including
metaphor and metonymy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04653">
<div class="article-summary-box-inner">
<span><p>Pre-trained language-vision models have shown remarkable performance on the
visual question answering (VQA) task. However, most pre-trained models are
trained by only considering monolingual learning, especially the resource-rich
language like English. Training such models for multilingual setups demand high
computing resources and multilingual language-vision dataset which hinders
their application in practice. To alleviate these challenges, we propose a
knowledge distillation approach to extend an English language-vision model
(teacher) into an equally effective multilingual and code-mixed model
(student). Unlike the existing knowledge distillation methods, which only use
the output from the last layer of the teacher network for distillation, our
student model learns and imitates the teacher from multiple intermediate layers
(language and vision encoders) with appropriately designed distillation
objectives for incremental knowledge extraction. We also create the large-scale
multilingual and code-mixed VQA dataset in eleven different language setups
considering the multiple Indian and European languages. Experimental results
and in-depth analysis show the effectiveness of the proposed VQA model over the
pre-trained language-vision models on eleven diverse language setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04655">
<div class="article-summary-box-inner">
<span><p>Zero-shot transfer learning for dialogue state tracking (DST) enables us to
handle a variety of task-oriented dialogue domains without the expense of
collecting in-domain data. In this work, we propose to transfer the
\textit{cross-task} knowledge from general question answering (QA) corpora for
the zero-shot DST task. Specifically, we propose TransferQA, a transferable
generative QA model that seamlessly combines extractive QA and multi-choice QA
via a text-to-text transformer framework, and tracks both categorical slots and
non-categorical slots in DST. In addition, we introduce two effective ways to
construct unanswerable questions, namely, negative question sampling and
context truncation, which enable our model to handle "none" value slots in the
zero-shot DST setting. The extensive experiments show that our approaches
substantially improve the existing zero-shot and few-shot results on MultiWoz.
Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue
dataset, our approach shows better generalization ability in unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04666">
<div class="article-summary-box-inner">
<span><p>It is a well-known approach for fringe groups and organizations to use
euphemisms -- ordinary-sounding and innocent-looking words with a secret
meaning -- to conceal what they are discussing. For instance, drug dealers
often use "pot" for marijuana and "avocado" for heroin. From a social media
content moderation perspective, though recent advances in NLP have enabled the
automatic detection of such single-word euphemisms, no existing work is capable
of automatically detecting multi-word euphemisms, such as "blue dream"
(marijuana) and "black tar" (heroin). Our paper tackles the problem of
euphemistic phrase detection without human effort for the first time, as far as
we are aware. We first perform phrase mining on a raw text corpus (e.g., social
media posts) to extract quality phrases. Then, we utilize word embedding
similarities to select a set of euphemistic phrase candidates. Finally, we rank
those candidates by a masked language model -- SpanBERT. Compared to strong
baselines, we report 20-50% higher detection accuracies using our algorithm for
detecting euphemistic phrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04672">
<div class="article-summary-box-inner">
<span><p>The transformer-based pre-trained language models have been tremendously
successful in most of the conventional NLP tasks. But they often struggle in
those tasks where numerical understanding is required. Some possible reasons
can be the tokenizers and pre-training objectives which are not specifically
designed to learn and preserve numeracy. Here we investigate the ability of
text-to-text transfer learning model (T5), which has outperformed its
predecessors in the conventional NLP tasks, to learn numeracy. We consider four
numeracy tasks: numeration, magnitude order prediction, finding minimum and
maximum in a series, and sorting. We find that, although T5 models perform
reasonably well in the interpolation setting, they struggle considerably in the
extrapolation setting across all four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04673">
<div class="article-summary-box-inner">
<span><p>Identifying relevant knowledge to be used in conversational systems that are
grounded in long documents is critical to effective response generation. We
introduce a knowledge identification model that leverages the document
structure to provide dialogue-contextualized passage encodings and better
locate knowledge relevant to the conversation. An auxiliary loss captures the
history of dialogue-document connections. We demonstrate the effectiveness of
our model on two document-grounded conversational datasets and provide analyses
showing generalization to unseen documents and long dialogue contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04689">
<div class="article-summary-box-inner">
<span><p>Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04703">
<div class="article-summary-box-inner">
<span><p>The encoder-decoder framework achieves state-of-the-art results in keyphrase
generation (KG) tasks by predicting both present keyphrases that appear in the
source document and absent keyphrases that do not. However, relying solely on
the source document can result in generating uncontrollable and inaccurate
absent keyphrases. To address these problems, we propose a novel graph-based
method that can capture explicit knowledge from related references. Our model
first retrieves some document-keyphrases pairs similar to the source document
from a pre-defined index as references. Then a heterogeneous graph is
constructed to capture relationships of different granularities between the
source document and its references. To guide the decoding process, a
hierarchical attention and copy mechanism is introduced, which directly copies
appropriate words from both the source document and its references based on
their relevance and significance. The experimental results on multiple KG
benchmarks show that the proposed model achieves significant improvements
against other baseline models, especially with regard to the absent keyphrase
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04705">
<div class="article-summary-box-inner">
<span><p>Zero-shot translation, directly translating between language pairs unseen in
training, is a promising capability of multilingual neural machine translation
(NMT). However, it usually suffers from capturing spurious correlations between
the output language and language invariant semantics due to the maximum
likelihood training objective, leading to poor transfer performance on
zero-shot translation. In this paper, we introduce a denoising autoencoder
objective based on pivot language into traditional training objective to
improve the translation accuracy on zero-shot directions. The theoretical
analysis from the perspective of latent variables shows that our approach
actually implicitly maximizes the probability distributions for zero-shot
directions. On two benchmark machine translation datasets, we demonstrate that
the proposed method is able to effectively eliminate the spurious correlations
and significantly outperforms state-of-the-art methods with a remarkable
performance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04707">
<div class="article-summary-box-inner">
<span><p>Meta-learning has achieved great success in leveraging the historical learned
knowledge to facilitate the learning process of the new task. However, merely
learning the knowledge from the historical tasks, adopted by current
meta-learning algorithms, may not generalize well to testing tasks when they
are not well-supported by training tasks. This paper studies a low-resource
text classification problem and bridges the gap between meta-training and
meta-testing tasks by leveraging the external knowledge bases. Specifically, we
propose KGML to introduce additional representation for each sentence learned
from the extracted sentence-specific knowledge graph. The extensive experiments
on three datasets demonstrate the effectiveness of KGML under both supervised
adaptation and unsupervised adaptation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04708">
<div class="article-summary-box-inner">
<span><p>The majority of language domains require prudent use of terminology to ensure
clarity and adequacy of information conveyed. While the correct use of
terminology for some languages and domains can be achieved by adapting
general-purpose MT systems on large volumes of in-domain parallel data, such
quantities of domain-specific data are seldom available for less-resourced
languages and niche domains. Furthermore, as exemplified by COVID-19 recently,
no domain-specific parallel data is readily available for emerging domains.
However, the gravity of this recent calamity created a high demand for reliable
translation of critical information regarding pandemic and infection
prevention. This work is part of WMT2021 Shared Task: Machine Translation using
Terminologies, where we describe Tilde MT systems that are capable of dynamic
terminology integration at the time of translation. Our systems achieve up to
94% COVID-19 term use accuracy on the test set of the EN-FR language pair
without having access to any form of in-domain information during system
training. We conclude our work with a broader discussion considering the Shared
Task itself and terminology translation in MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04711">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated that pre-training in-domain language models can
boost performance when adapting to a new domain. However, the costs associated
with pre-training raise an important question: given a fixed budget, what steps
should an NLP practitioner take to maximize performance? In this paper, we
study domain adaptation under budget constraints, and approach it as a customer
choice problem between data annotation and pre-training. Specifically, we
measure the annotation cost of three procedural text datasets and the
pre-training cost of three in-domain language models. Then we evaluate the
utility of different combinations of pre-training and data annotation under
varying budget constraints to assess which combination strategy works best. We
find that, for small budgets, spending all funds on annotation leads to the
best performance; once the budget becomes large enough, a combination of data
annotation and in-domain pre-training works more optimally. We therefore
suggest that task-specific data annotation should be part of an economical
strategy when adapting an NLP model to a new domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04712">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification is a challenging task because it requires
capturing label dependencies. It becomes even more challenging when class
distribution is long-tailed. Resampling and re-weighting are common approaches
used for addressing the class imbalance problem, however, they are not
effective when there is label dependency besides class imbalance because they
result in oversampling of common labels. Here, we introduce the application of
balancing loss functions for multi-label text classification. We perform
experiments on a general domain dataset with 90 labels (Reuters-21578) and a
domain-specific dataset from PubMed with 18211 labels. We find that a
distribution-balanced loss function, which inherently addresses both the class
imbalance and label linkage problems, outperforms commonly used loss functions.
Distribution balancing methods have been successfully used in the image
recognition field. Here, we show their effectiveness in natural language
processing. Source code is available at
https://github.com/blessu/BalancedLossNLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04715">
<div class="article-summary-box-inner">
<span><p>Reproducible benchmarks are crucial in driving progress of machine
translation research. However, existing machine translation benchmarks have
been mostly limited to high-resource or well-represented languages. Despite an
increasing interest in low-resource machine translation, there are no
standardized reproducible benchmarks for many African languages, many of which
are used by millions of speakers but have less digitized textual data. To
tackle these challenges, we propose AfroMT, a standardized, clean, and
reproducible machine translation benchmark for eight widely spoken African
languages. We also develop a suite of analysis tools for system diagnosis
taking into account the unique properties of these languages. Furthermore, we
explore the newly considered case of low-resource focused pretraining and
develop two novel data augmentation-based strategies, leveraging word-level
alignment information and pseudo-monolingual data for pretraining multilingual
sequence-to-sequence models. We demonstrate significant improvements when
pretraining on 11 languages, with gains of up to 2 BLEU points over strong
baselines. We also show gains of up to 12 BLEU points over cross-lingual
transfer baselines in data-constrained scenarios. All code and pretrained
models will be released as further steps towards larger reproducible benchmarks
for African languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04726">
<div class="article-summary-box-inner">
<span><p>Deep neural models for low-resource named entity recognition (NER) have shown
impressive results by leveraging distant super-vision or other meta-level
information (e.g. explanation). However, the costs of acquiring such additional
information are generally prohibitive, especially in domains where existing
resources (e.g. databases to be used for distant supervision) may not exist. In
this paper, we present a novel two-stage framework (AutoTriggER) to improve NER
performance by automatically generating and leveraging "entity triggers" which
are essentially human-readable clues in the text that can help guide the model
to make better decisions. Thus, the framework is able to both create and
leverage auxiliary supervision by itself. Through experiments on three
well-studied NER datasets, we show that our automatically extracted triggers
are well-matched to human triggers, and AutoTriggER improves performance over a
RoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a
low resource setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04727">
<div class="article-summary-box-inner">
<span><p>Language agnostic and semantic-language information isolation is an emerging
research direction for multilingual representations models. We explore this
problem from a novel angle of geometric algebra and semantic space. A simple
but highly effective method "Language Information Removal (LIR)" factors out
language identity information from semantic related components in multilingual
representations pre-trained on multi-monolingual data. A post-training and
model-agnostic method, LIR only uses simple linear operations, e.g. matrix
factorization and orthogonal projection. LIR reveals that for weak-alignment
multilingual systems, the principal components of semantic spaces primarily
encodes language identity information. We first evaluate the LIR on a
cross-lingual question answer retrieval task (LAReQA), which requires the
strong alignment for the multilingual embedding space. Experiment shows that
LIR is highly effectively on this task, yielding almost 100% relative
improvement in MAP for weak-alignment models. We then evaluate the LIR on
Amazon Reviews and XEVAL dataset, with the observation that removing language
information is able to improve the cross-lingual transfer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04732">
<div class="article-summary-box-inner">
<span><p>Various measures have been proposed to quantify human-like social biases in
word embeddings. However, bias scores based on these measures can suffer from
measurement error. One indication of measurement quality is reliability,
concerning the extent to which a measure produces consistent results. In this
paper, we assess three types of reliability of word embedding gender bias
measures, namely test-retest reliability, inter-rater consistency and internal
consistency. Specifically, we investigate the consistency of bias scores across
different choices of random seeds, scoring rules and words. Furthermore, we
analyse the effects of various factors on these measures' reliability scores.
Our findings inform better design of word embedding gender bias measures.
Moreover, we urge researchers to be more critical about the application of such
measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04733">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that monolingual masked language models learn to
represent data-driven notions of language variation which can be used for
domain-targeted training data selection. Dataset genre labels are already
frequently available, yet remain largely unexplored in cross-lingual setups. We
harness this genre metadata as a weak supervision signal for targeted data
selection in zero-shot dependency parsing. Specifically, we project
treebank-level genre information to the finer-grained sentence level, with the
goal to amplify information implicitly stored in unsupervised contextualized
representations. We demonstrate that genre is recoverable from multilingual
contextual embeddings and that it provides an effective signal for training
data selection in cross-lingual, zero-shot scenarios. For 12 low-resource
language treebanks, six of which are test-only, our genre-specific methods
significantly outperform competitive baselines as well as recent
embedding-based methods for data selection. Moreover, genre-based data
selection provides new state-of-the-art results for three of these target
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04740">
<div class="article-summary-box-inner">
<span><p>It is widely accepted that fine-tuning pre-trained language models usually
brings about performance improvements in downstream tasks. However, there are
limited studies on the reasons behind this effectiveness, particularly from the
viewpoint of structural changes in the embedding space. Trying to fill this
gap, in this paper, we analyze the extent to which the isotropy of the
embedding space changes after fine-tuning. We demonstrate that, even though
isotropy is a desirable geometrical property, fine-tuning does not necessarily
result in isotropy enhancements. Moreover, local structures in pre-trained
contextual word representations (CWRs), such as those encoding token types or
frequency, undergo a massive change during fine-tuning. Our experiments show
dramatic growth in the number of elongated directions in the embedding space,
which, in contrast to pre-trained CWRs, carry the essential linguistic
knowledge in the fine-tuned embedding space, making existing isotropy
enhancement methods ineffective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04762">
<div class="article-summary-box-inner">
<span><p>Text classification systems based on contextual embeddings are not viable
options for many of the low resource languages. On the other hand, recently
introduced capsule networks have shown performance in par with these text
classification models. Thus, they could be considered as a viable alternative
for text classification for languages that do not have pre-trained contextual
embedding models. However, current capsule networks depend upon spatial
patterns without considering the sequential features of the text. They are also
sub-optimal in capturing the context-level information in longer sequences.
This paper presents a novel Dual-State Capsule (DS-Caps) network-based
technique for text classification, which is optimized to mitigate these issues.
Two varieties of states, namely sentence-level and word-level, are integrated
with capsule layers to capture deeper context-level information for language
modeling. The dynamic routing process among capsules was also optimized using
the context-level information obtained through sentence-level states. The
DS-Caps networks outperform the existing capsule network architectures for
multiple datasets, particularly for tasks with longer sequences of text. We
also demonstrate the superiority of DS-Caps in text classification for a low
resource language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04775">
<div class="article-summary-box-inner">
<span><p>Existing black box search methods have achieved high success rate in
generating adversarial attacks against NLP models. However, such search methods
are inefficient as they do not consider the amount of queries required to
generate adversarial attacks. Also, prior attacks do not maintain a consistent
search space while comparing different search methods. In this paper, we
propose a query efficient attack strategy to generate plausible adversarial
examples on text classification and entailment tasks. Our attack jointly
leverages attention mechanism and locality sensitive hashing (LSH) to reduce
the query count. We demonstrate the efficacy of our approach by comparing our
attack with four baselines across three different search spaces. Further, we
benchmark our results across the same search space used in prior attacks. In
comparison to attacks proposed, on an average, we are able to reduce the query
count by 75% across all datasets and target models. We also demonstrate that
our attack achieves a higher success rate when compared to prior attacks in a
limited query setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04778">
<div class="article-summary-box-inner">
<span><p>Multilingual Neural Machine Translation (NMT) enables one model to serve all
translation directions, including ones that are unseen during training, i.e.
zero-shot translation. Despite being theoretically attractive, current models
often produce low quality translations -- commonly failing to even produce
outputs in the right target language. In this work, we observe that off-target
translation is dominant even in strong multilingual systems, trained on massive
multilingual corpora. To address this issue, we propose a joint approach to
regularize NMT models at both representation-level and gradient-level. At the
representation level, we leverage an auxiliary target language prediction task
to regularize decoder outputs to retain information about the target language.
At the gradient level, we leverage a small amount of direct data (in thousands
of sentence pairs) to regularize model gradients. Our results demonstrate that
our approach is highly effective in both reducing off-target translation
occurrences and improving zero-shot translation performance by +5.59 and +10.38
BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our
method also works well when the small amount of direct data is not available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04780">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models, such as BERT, have achieved remarkable
results on machine reading comprehension. However, due to the constraint of
encoding length (e.g., 512 WordPiece tokens), a long document is usually split
into multiple chunks that are independently read. It results in the reading
field being limited to individual chunks without information collaboration for
long document machine reading comprehension. To address this problem, we
propose RoR, a read-over-read method, which expands the reading field from
chunk to document. Specifically, RoR includes a chunk reader and a document
reader. The former first predicts a set of regional answers for each chunk,
which are then compacted into a highly-condensed version of the original
document, guaranteeing to be encoded once. The latter further predicts the
global answers from this condensed document. Eventually, a voting strategy is
utilized to aggregate and rerank the regional and global answers for final
prediction. Extensive experiments on two benchmarks QuAC and TriviaQA
demonstrate the effectiveness of RoR for long document reading. Notably, RoR
ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of
submission (May 17th, 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04783">
<div class="article-summary-box-inner">
<span><p>When a sufficiently large far-field training data is presented, jointly
optimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech
Recognition (ASR) backend shows promising results. Recent literature has shown
traditional beamformer designs, such as MVDR (Minimum Variance Distortionless
Response) or fixed beamformers can be successfully integrated as the frontend
into an E2E ASR system with learnable parameters. In this work, we propose the
self-attention channel combinator (SACC) ASR frontend, which leverages the
self-attention mechanism to combine multichannel audio signals in the magnitude
spectral domain. Experiments conducted on a multichannel playback test data
shows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed
beamformer-based frontend, both jointly optimized with a ContextNet-based ASR
backend. We also demonstrate the connection between the SACC and the
traditional beamformers, and analyze the intermediate outputs of the SACC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04787">
<div class="article-summary-box-inner">
<span><p>Resolving pronouns to their referents has long been studied as a fundamental
natural language understanding problem. Previous works on pronoun coreference
resolution (PCR) mostly focus on resolving pronouns to mentions in text while
ignoring the exophoric scenario. Exophoric pronouns are common in daily
communications, where speakers may directly use pronouns to refer to some
objects present in the environment without introducing the objects first.
Although such objects are not mentioned in the dialogue text, they can often be
disambiguated by the general topics of the dialogue. Motivated by this, we
propose to jointly leverage the local context and global topics of dialogues to
solve the out-of-text PCR problem. Extensive experiments demonstrate the
effectiveness of adding topic regularization for resolving exophoric pronouns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04810">
<div class="article-summary-box-inner">
<span><p>Infusing factual knowledge into pre-trained models is fundamental for many
knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions
(MoP), an infusion approach that can handle a very large knowledge graph (KG)
by partitioning it into smaller sub-graphs and infusing their specific
knowledge into various BERT models using lightweight adapters. To leverage the
overall factual knowledge for a target task, these sub-graph adapters are
further fine-tuned along with the underlying BERT through a mixture layer. We
evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on
six downstream tasks (inc. NLI, QA, Classification), and the results show that
our MoP consistently enhances the underlying BERTs in task performance, and
achieves new SOTA performances on five evaluated datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04817">
<div class="article-summary-box-inner">
<span><p>Style is an integral part of natural language. However, evaluation methods
for style measures are rare, often task-specific and usually do not control for
content. We propose the modular, fine-grained and content-controlled
similarity-based STyle EvaLuation framework (STEL) to test the performance of
any model that can compare two sentences on style. We illustrate STEL with two
general dimensions of style (formal/informal and simple/complex) as well as two
specific characteristics of style (contrac'tion and numb3r substitution). We
find that BERT-based methods outperform simple versions of commonly used style
measures like 3-grams, punctuation frequency and LIWC-based approaches. We
invite the addition of further tasks and task instances to STEL and hope to
facilitate the improvement of style-sensitive measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04825">
<div class="article-summary-box-inner">
<span><p>The impressive capabilities of recent generative models to create texts that
are challenging to distinguish from the human-written ones can be misused for
generating fake news, product reviews, and even abusive content. Despite the
prominent performance of existing methods for artificial text detection, they
still lack interpretability and robustness towards unseen models. To this end,
we propose three novel types of interpretable topological features for this
task based on Topological Data Analysis (TDA) which is currently understudied
in the field of NLP. We empirically show that the features derived from the
BERT model outperform count- and neural-based baselines up to 10\% on three
common datasets, and tend to be the most robust towards unseen GPT-style
generation models as opposed to existing methods. The probing analysis of the
features reveals their sensitivity to the surface and syntactic properties. The
results demonstrate that TDA is a promising line with respect to NLP tasks,
specifically the ones that incorporate surface and structural information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04832">
<div class="article-summary-box-inner">
<span><p>Asking questions about a situation is an inherent step towards understanding
it. To this end, we introduce the task of role question generation, which,
given a predicate mention and a passage, requires producing a set of questions
asking about all possible semantic roles of the predicate. We develop a
two-stage model for this task, which first produces a context-independent
question prototype for each role and then revises it to be contextually
appropriate for the passage. Unlike most existing approaches to question
generation, our approach does not require conditioning on existing answers in
the text. Instead, we condition on the type of information to inquire about,
regardless of whether the answer appears explicitly in the text, could be
inferred from it, or should be sought elsewhere. Our evaluation demonstrates
that we generate diverse and well-formed questions for a large, broad-coverage
ontology of predicates and roles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04834">
<div class="article-summary-box-inner">
<span><p>Multi-turn response selection models have recently shown comparable
performance to humans in several benchmark datasets. However, in the real
environment, these models often have weaknesses, such as making incorrect
predictions based heavily on superficial patterns without a comprehensive
understanding of the context. For example, these models often give a high score
to the wrong response candidate containing several keywords related to the
context but using the inconsistent tense. In this study, we analyze the
weaknesses of the open-domain Korean Multi-turn response selection models and
publish an adversarial dataset to evaluate these weaknesses. We also suggest a
strategy to build a robust model in this adversarial environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04835">
<div class="article-summary-box-inner">
<span><p>In recent years, with the expansion of the Internet and attractive social
media infrastructures, people prefer to follow the news through these media.
Despite the many advantages of these media in the news field, the lack of any
control and verification mechanism has led to the spread of fake news, as one
of the most important threats to democracy, economy, journalism and freedom of
expression. Designing and using automatic methods to detect fake news on social
media has become a significant challenge. In this paper, we examine the
publishers' role in detecting fake news on social media. We also suggest a high
accurate multi-modal framework, namely FR-Detect, using user-related and
content-related features with early detection capability. For this purpose, two
new user-related features, namely Activity Credibility and Influence, have been
introduced for publishers. Furthermore, a sentence-level convolutional neural
network is provided to combine these features with latent textual content
features properly. Experimental results have shown that the publishers'
features can improve the performance of content-based models by up to 13% and
29% in accuracy and F1-score, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04838">
<div class="article-summary-box-inner">
<span><p>Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04847">
<div class="article-summary-box-inner">
<span><p>Labeling data can be an expensive task as it is usually performed manually by
domain experts. This is cumbersome for deep learning, as it is dependent on
large labeled datasets. Active learning (AL) is a paradigm that aims to reduce
labeling effort by only using the data which the used model deems most
informative. Little research has been done on AL in a text classification
setting and next to none has involved the more recent, state-of-the-art NLP
models. Here, we present an empirical study that compares different
uncertainty-based algorithms with BERT$_{base}$ as the used classifier. We
evaluate the algorithms on two NLP classification datasets: Stanford Sentiment
Treebank and KvK-Frontpages. Additionally, we explore heuristics that aim to
solve presupposed problems of uncertainty-based AL; namely, that it is
unscalable and that it is prone to selecting outliers. Furthermore, we explore
the influence of the query-pool size on the performance of AL. Whereas it was
found that the proposed heuristics for AL did not improve performance of AL;
our results show that using uncertainty-based AL with BERT$_{base}$ outperforms
random sampling of data. This difference in performance can decrease as the
query-pool size gets larger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04853">
<div class="article-summary-box-inner">
<span><p>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with
hierarchical label spaces, such as automatic assignment of ICD-9 codes to
discharge summaries. Performance of models in prior art is evaluated with
standard precision, recall, and F1 measures without regard for the rich
hierarchical structure. In this work we argue for hierarchical evaluation of
the predictions of neural LMTC models. With the example of the ICD-9 ontology
we describe a structural issue in the representation of the structured label
space in prior art, and propose an alternative representation based on the
depth of the ontology. We propose a set of metrics for hierarchical evaluation
using the depth-based representation. We compare the evaluation scores from the
proposed metrics with previously used metrics on prior art LMTC models for
ICD-9 coding in MIMIC-III. We also propose further avenues of research
involving the proposed ontological representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04867">
<div class="article-summary-box-inner">
<span><p>As neural language models approach human performance on NLP benchmark tasks,
their advances are widely seen as evidence of an increasingly complex
understanding of syntax. This view rests upon a hypothesis that has not yet
been empirically tested: that word order encodes meaning essential to
performing these tasks. We refute this hypothesis in many cases: in the GLUE
suite and in various genres of English text, the words in a sentence or phrase
can rarely be permuted to form a phrase carrying substantially different
information. Our surprising result relies on inference by iterative shuffling
(IBIS), a novel, efficient procedure that finds the ordering of a bag of words
having the highest likelihood under a fixed language model. IBIS can use any
black-box model without additional training and is superior to existing word
ordering algorithms. Coalescing our findings, we discuss how shuffling
inference procedures such as IBIS can benefit language modeling and constrained
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04870">
<div class="article-summary-box-inner">
<span><p>Readability assessment is the task of determining how difficult or easy a
text is or which level/grade it has. Traditionally, language dependent
readability formula have been used, but these formulae take few text
characteristics into account. However, Natural Language Processing (NLP) tools
that assess the complexity of texts are able to measure more different features
and can be adapted to different languages. In this paper, we present the
MultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over
125 measures of cohesion,language, and readability for English, Spanish and
Basque, but whose architecture is designed to easily adapt other languages;
(ii) readability assessment classifiers that improve the performance of
Coh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a
web tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into
three reading levels (elementary, intermediate, and advanced) in English and
95.50 % in Basque and 90 % in Spanish when classifying into two reading levels
(simple and complex) using a SMO classifier. Using cross-lingual features,
MultiAzterTest also obtains competitive results above all in a complex vs
simple distinction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04876">
<div class="article-summary-box-inner">
<span><p>The problem of representing the atomic elements of language in modern neural
learning systems is one of the central challenges of the field of natural
language processing. I present a survey of the distributional, compositional,
and relational approaches to addressing this task, and discuss various means of
integrating them into systems, with special emphasis on the word level and the
out-of-vocabulary phenomenon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04877">
<div class="article-summary-box-inner">
<span><p>Adapters are light-weight modules that allow parameter-efficient fine-tuning
of pretrained models. Specialized language and task adapters have recently been
proposed to facilitate cross-lingual transfer of multilingual pretrained models
(Pfeiffer et al., 2020b). However, this approach requires training a separate
language adapter for every language one wishes to support, which can be
impractical for languages with limited data. An intuitive solution is to use a
related language adapter for the new language variety, but we observe that this
solution can lead to sub-optimal performance. In this paper, we aim to improve
the robustness of language adapters to uncovered languages without training new
adapters. We find that ensembling multiple existing language adapters makes the
fine-tuned model significantly more robust to other language varieties not
included in these adapters. Building upon this observation, we propose Entropy
Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble
weights of the pretrained language adapters for each test sentence by
minimizing the entropy of its predictions. Experiments on three diverse groups
of language varieties show that our method leads to significant improvements on
both named entity recognition and part-of-speech tagging across all languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04901">
<div class="article-summary-box-inner">
<span><p>Document-level entity-based extraction (EE), aiming at extracting
entity-centric information such as entity roles and entity relations, is key to
automatic knowledge acquisition from text corpora for various domains. Most
document-level EE systems build extractive models, which struggle to model
long-term dependencies among entities at the document level. To address this
issue, we propose a generative framework for two document-level EE tasks:
role-filler entity extraction (REE) and relation extraction (RE). We first
formulate them as a template generation problem, allowing models to efficiently
capture cross-entity dependencies, exploit label semantics, and avoid the
exponential computation complexity of identifying N-ary relations. A novel
cross-attention guided copy mechanism, TopK Copy, is incorporated into a
pre-trained sequence-to-sequence model to enhance the capabilities of
identifying key information in the input document. Experiments done on the
MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),
binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04912">
<div class="article-summary-box-inner">
<span><p>We present ReasonBert, a pre-training method that augments language models
with the ability to reason over long-range relations and multiple, possibly
hybrid contexts. Unlike existing pre-training methods that only harvest
learning signals from local contexts of naturally occurring texts, we propose a
generalized notion of distant supervision to automatically connect multiple
pieces of text and tables to create pre-training examples that require
long-range reasoning. Different types of reasoning are simulated, including
intersecting multiple pieces of evidence, bridging from one piece of evidence
to another, and detecting unanswerable cases. We conduct a comprehensive
evaluation on a variety of extractive question answering datasets ranging from
single-hop to multi-hop and from text-only to table-only to hybrid that require
various reasoning capabilities and show that ReasonBert achieves remarkable
improvement over an array of strong baselines. Few-shot experiments further
demonstrate that our pre-training method substantially improves sample
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04919">
<div class="article-summary-box-inner">
<span><p>The ability to recognise emotions lends a conversational artificial
intelligence a human touch. While emotions in chit-chat dialogues have received
substantial attention, emotions in task-oriented dialogues have been largely
overlooked despite having an equally important role, such as to signal failure
or success. Existing emotion-annotated task-oriented corpora are limited in
size, label richness, and public availability, creating a bottleneck for
downstream tasks. To lay a foundation for studies on emotions in task-oriented
dialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus
of task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain
task-oriented dialogue dataset. It contains more than 11K dialogues with more
than 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz
dialogues from MultiWOZ, we collect human-machine dialogues within the same set
of domains to sufficiently cover the space of various emotions that can happen
during the lifetime of a data-driven dialogue system. To the best of our
knowledge, this is the first large-scale open-source corpus of its kind. We
propose a novel emotion labelling scheme, which is tailored to task-oriented
dialogues. We report a set of experimental results to show the usability of
this corpus for emotion recognition and state tracking in task-oriented
dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04921">
<div class="article-summary-box-inner">
<span><p>State-of-the-art contextual embeddings are obtained from large language
models available only for a few languages. For others, we need to learn
representations using a multilingual model. There is an ongoing debate on
whether multilingual embeddings can be aligned in a space shared across many
languages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\v{c}ek,
2021) allows us to answer this question for specific linguistic features and
learn a projection based only on mono-lingual annotated datasets. We evaluate
syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's
contextual representations for nine diverse languages. We observe that for
languages closely related to English, no transformation is needed. The
evaluated information is encoded in a shared cross-lingual embedding space. For
other languages, it is beneficial to apply orthogonal transformation learned
separately for each language. We successfully apply our findings to zero-shot
and few-shot cross-lingual parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04922">
<div class="article-summary-box-inner">
<span><p>As large-scale, pre-trained language models achieve human-level and
superhuman accuracy on existing language understanding tasks, statistical bias
in benchmark data and probing studies have recently called into question their
true capabilities. For a more informative evaluation than accuracy on text
classification tasks can offer, we propose evaluating systems through a novel
measure of prediction coherence. We apply our framework to two existing
language understanding benchmarks with different properties to demonstrate its
versatility. Our experimental results show that this evaluation framework,
although simple in ideas and implementation, is a quick, effective, and
versatile measure to provide insight into the coherence of machines'
predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04939">
<div class="article-summary-box-inner">
<span><p>In computational linguistics, it has been shown that hierarchical structures
make language models (LMs) more human-like. However, the previous literature
has been agnostic about a parsing strategy of the hierarchical models. In this
paper, we investigated whether hierarchical structures make LMs more
human-like, and if so, which parsing strategy is most cognitively plausible. In
order to address this question, we evaluated three LMs against human reading
times in Japanese with head-final left-branching structures: Long Short-Term
Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars
(RNNGs) with top-down and left-corner parsing strategies as hierarchical
models. Our computational modeling demonstrated that left-corner RNNGs
outperformed top-down RNNGs and LSTM, suggesting that hierarchical and
left-corner architectures are more cognitively plausible than top-down or
sequential architectures. In addition, the relationships between the cognitive
plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be
discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
<div class="article-summary-box-inner">
<span><p>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines' true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines' reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04949">
<div class="article-summary-box-inner">
<span><p>In this paper we examine different meaning representations that are commonly
used in different natural language applications today and discuss their limits,
both in terms of the aspects of the natural language meaning they are modelling
and in terms of the aspects of the application for which they are used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04953">
<div class="article-summary-box-inner">
<span><p>Pretraining techniques leveraging enormous datasets have driven recent
advances in text summarization. While folk explanations suggest that knowledge
transfer accounts for pretraining's benefits, little is known about why it
works or what makes a pretraining task or dataset suitable. In this paper, we
challenge the knowledge transfer story, showing that pretraining on documents
consisting of character n-grams selected at random, we can nearly match the
performance of models pretrained on real corpora. This work holds the promise
of eliminating upstream corpora, which may alleviate some concerns over
offensive language, bias, and copyright issues. To see whether the small
residual benefit of using real data could be accounted for by the structure of
the pretraining task, we design several tasks motivated by a qualitative study
of summarization corpora. However, these tasks confer no appreciable benefit,
leaving open the possibility of a small role for knowledge transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04957">
<div class="article-summary-box-inner">
<span><p>Framing a news article means to portray the reported event from a specific
perspective, e.g., from an economic or a health perspective. Reframing means to
change this perspective. Depending on the audience or the submessage, reframing
can become necessary to achieve the desired effect on the readers. Reframing is
related to adapting style and sentiment, which can be tackled with neural text
generation techniques. However, it is more challenging since changing a frame
requires rewriting entire sentences rather than single phrases. In this paper,
we study how to computationally reframe sentences in news articles while
maintaining their coherence to the context. We treat reframing as a
sentence-level fill-in-the-blank task for which we train neural models on an
existing media frame corpus. To guide the training, we propose three
strategies: framed-language pretraining, named-entity preservation, and
adversarial learning. We evaluate respective models automatically and manually
for topic consistency, coherence, and successful reframing. Our results
indicate that generating properly-framed text works well but with tradeoffs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
<div class="article-summary-box-inner">
<span><p>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04994">
<div class="article-summary-box-inner">
<span><p>Unlike well-structured text, such as news reports and encyclopedia articles,
dialogue content often comes from two or more interlocutors, exchanging
information with each other. In such a scenario, the topic of a conversation
can vary upon progression and the key information for a certain topic is often
scattered across multiple utterances of different speakers, which poses
challenges to abstractly summarize dialogues. To capture the various topic
information of a conversation and outline salient facts for the captured
topics, this work proposes two topic-aware contrastive learning objectives,
namely coherence detection and sub-summary generation objectives, which are
expected to implicitly model the topic change and handle information scattering
challenges for the dialogue summarization task. The proposed contrastive
objectives are framed as auxiliary tasks for the primary dialogue summarization
task, united via an alternative parameter updating strategy. Extensive
experiments on benchmark datasets demonstrate that the proposed simple method
significantly outperforms strong baselines and achieves new state-of-the-art
performance. The code and trained models are publicly available via
\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04997">
<div class="article-summary-box-inner">
<span><p>A major factor contributing to the success of modern representation learning
is the ease of performing various vector operations. Recently, objects with
geometric structures (eg. distributions, complex or hyperbolic vectors, or
regions such as cones, disks, or boxes) have been explored for their
alternative inductive biases and additional representational capacities. In
this work, we introduce Box Embeddings, a Python library that enables
researchers to easily apply and extend probabilistic box embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05003">
<div class="article-summary-box-inner">
<span><p>We study the problem of training named entity recognition (NER) models using
only distantly-labeled data, which can be automatically obtained by matching
entity mentions in the raw text with entity types in a knowledge base. The
biggest challenge of distantly-supervised NER is that the distant supervision
may induce incomplete and noisy labels, rendering the straightforward
application of supervised learning ineffective. In this paper, we propose (1) a
noise-robust learning scheme comprised of a new loss function and a noisy label
removal step, for training NER models on distantly-labeled data, and (2) a
self-training method that uses contextualized augmentations created by
pre-trained language models to improve the generalization ability of the NER
model. On three benchmark datasets, our method achieves superior performance,
outperforming existing distantly-supervised NER models by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05006">
<div class="article-summary-box-inner">
<span><p>An important task in NLP applications such as sentence simplification is the
ability to take a long, complex sentence and split it into shorter sentences,
rephrasing as necessary. We introduce a novel dataset and a new model for this
`split and rephrase' task. Our BiSECT training data consists of 1 million long
English sentences paired with shorter, meaning-equivalent English sentences. We
obtain these by extracting 1-2 sentence alignments in bilingual parallel
corpora and then using machine translation to convert both sides of the corpus
into the same language. BiSECT contains higher quality training examples than
previous Split and Rephrase corpora, with sentence splits that require more
significant modifications. We categorize examples in our corpus, and use these
categories in a novel model that allows us to target specific regions of the
input sentence to be split and edited. Moreover, we show that models trained on
BiSECT can perform a wider variety of split operations and improve upon
previous state-of-the-art approaches in automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05016">
<div class="article-summary-box-inner">
<span><p>We test the natural expectation that using MT in professional translation
saves human processing time. The last such study was carried out by
Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the
translation quality. In contrast, we focus on neural MT (NMT) of high quality,
which has become the state-of-the-art approach since then and also got adopted
by most translation companies.
</p>
<p>Through an experimental study involving over 30 professional translators for
English -&gt; Czech translation, we examine the relationship between NMT
performance and post-editing time and quality. Across all models, we found that
better MT systems indeed lead to fewer changes in the sentences in this
industry setting. The relation between system quality and post-editing time is
however not straightforward and, contrary to the results on phrase-based MT,
BLEU is definitely not a stable predictor of the time or final output quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.02499">
<div class="article-summary-box-inner">
<span><p>We present a model to predict fine-grained emotions along the continuous
dimensions of valence, arousal, and dominance (VAD) with a corpus with
categorical emotion annotations. Our model is trained by minimizing the EMD
(Earth Mover's Distance) loss between the predicted VAD score distribution and
the categorical emotion distributions sorted along VAD, and it can
simultaneously classify the emotion categories and predict the VAD scores for a
given sentence. We use pre-trained RoBERTa-Large and fine-tune on three
different corpora with categorical labels and evaluate on EmoBank corpus with
VAD scores. We show that our approach reaches comparable performance to that of
the state-of-the-art classifiers in categorical emotion classification and
shows significant positive correlations with the ground truth VAD scores. Also,
further training with supervision of VAD labels leads to improved performance
especially when dataset is small. We also present examples of predictions of
appropriate emotion words that are not part of the original annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14876">
<div class="article-summary-box-inner">
<span><p>Word embeddings are powerful representations that form the foundation of many
natural language processing architectures, both in English and in other
languages. To gain further insight into word embeddings, we explore their
stability (e.g., overlap between the nearest neighbors of a word in different
embedding spaces) in diverse languages. We discuss linguistic properties that
are related to stability, drawing out insights about correlations with
affixing, language gender systems, and other features. This has implications
for embedding use, particularly in research that uses them to study language
trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00782">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PTLMs) have achieved impressive performance on
commonsense inference benchmarks, but their ability to employ commonsense to
make robust inferences, which is crucial for effective communications with
humans, is debated. In the pursuit of advancing fluid human-AI communication,
we propose a new challenge, RICA: Robust Inference capability based on
Commonsense Axioms, that evaluates robust commonsense inference despite textual
perturbations. To generate data for this challenge, we develop a systematic and
scalable procedure using commonsense knowledge bases and probe PTLMs across two
different evaluation settings. Extensive experiments on our generated probe
sets with more than 10k statements show that PTLMs perform no better than
random guessing on the zero-shot setting, are heavily impacted by statistical
biases, and are not robust to perturbation attacks. We also find that
fine-tuning on similar statements offer limited gains, as PTLMs still fail to
generalize to unseen inferences. Our new large-scale benchmark exposes a
significant gap between PTLMs and human-level language understanding and offers
a new challenge for PTLMs to demonstrate commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14658">
<div class="article-summary-box-inner">
<span><p>The Transformer translation model is based on the multi-head attention
mechanism, which can be parallelized easily. The multi-head attention network
performs the scaled dot-product attention function in parallel, empowering the
model by jointly attending to information from different representation
subspaces at different positions. In this paper, we present an approach to
learning a hard retrieval attention where an attention head only attends to one
token in the sentence rather than all tokens. The matrix multiplication between
attention probabilities and the value sequence in the standard scaled
dot-product attention can thus be replaced by a simple and efficient retrieval
operation. We show that our hard retrieval attention mechanism is 1.43 times
faster in decoding, while preserving translation quality on a wide range of
machine translation tasks when used in the decoder self- and cross-attention
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06127">
<div class="article-summary-box-inner">
<span><p>Transformers that are pre-trained on multilingual corpora, such as, mBERT and
XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In
the zero-shot transfer setting, only English training data is used, and the
fine-tuned model is evaluated on another target language. While this works
surprisingly well, substantial variance has been observed in target language
performance between different fine-tuning runs, and in the zero-shot setup, no
target-language development data is available to select among multiple
fine-tuned models. Prior work has relied on English dev data to select among
models that are fine-tuned with different learning rates, number of steps and
other hyperparameters, often resulting in suboptimal choices. In this paper, we
show that it is possible to select consistently better models when small
amounts of annotated data are available in auxiliary pivot languages. We
propose a machine learning approach to model selection that uses the fine-tuned
model's own internal representations to predict its cross-lingual capabilities.
In extensive experiments we find that this method consistently selects better
models than English validation data across twenty five languages (including
eight low-resource languages), and often achieves results that are comparable
to model selection using target language development data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
<div class="article-summary-box-inner">
<span><p>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12251">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) is an established component within a
conversational AI or digital assistant system, and it is responsible for
producing semantic understanding of a user request. We propose a scalable and
automatic approach for improving NLU in a large-scale conversational AI system
by leveraging implicit user feedback, with an insight that user interaction
data and dialog context have rich information embedded from which user
satisfaction and intention can be inferred. In particular, we propose a general
domain-agnostic framework for curating new supervision data for improving NLU
from live production traffic. With an extensive set of experiments, we show the
results of applying the framework and improving NLU for a large-scale
production system and show its impact across 10 domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12762">
<div class="article-summary-box-inner">
<span><p>In interpretable NLP, we require faithful rationales that reflect the model's
decision-making process for an explained instance. While prior work focuses on
extractive rationales (a subset of the input words), we investigate their
less-studied counterpart: free-text natural language rationales. We demonstrate
that pipelines, existing models for faithful extractive rationalization on
information-extraction style tasks, do not extend as reliably to "reasoning"
tasks requiring free-text rationales. We turn to models that jointly predict
and rationalize, a class of widely used high-performance models for free-text
rationalization whose faithfulness is not yet established. We define
label-rationale association as a necessary property for faithfulness: the
internal mechanisms of the model producing the label and the rationale must be
meaningfully correlated. We propose two measurements to test this property:
robustness equivalence and feature importance agreement. We find that
state-of-the-art T5-based joint models exhibit both properties for
rationalizing commonsense question-answering and natural language inference,
indicating their potential for producing faithful free-text rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12800">
<div class="article-summary-box-inner">
<span><p>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.
Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,
Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from
55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query
Bank and Relevance Set, where the former contains 1,236 human-paraphrased
queries while the latter contains ~32 human-annotated FAQ items for each query.
We analyze COUGH by testing different FAQ retrieval models built on top of BM25
and BERT, among which the best model achieves 48.8 under P@5, indicating a
great challenge presented by COUGH and encouraging future research for further
improvement. Our COUGH dataset is available at
https://github.com/sunlab-osu/covid-faq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16410">
<div class="article-summary-box-inner">
<span><p>To alleviate human efforts from obtaining large-scale annotations,
Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in
addition to learning from limited samples. Existing self-training methods
suffer from the gradual drift problem, where noisy pseudo labels on unlabeled
data are incorporated during training. To alleviate the noise in pseudo labels,
we propose a method called MetaSRE, where a Relation Label Generation Network
generates quality assessment on pseudo labels by (meta) learning from the
successful and failed attempts on Relation Classification Network as an
additional meta-objective. To reduce the influence of noisy pseudo labels,
MetaSRE adopts a pseudo label selection and exploitation scheme which assesses
pseudo label quality on unlabeled samples and only exploits high-quality pseudo
labels in a self-training fashion to incrementally augment labeled samples for
both robustness and accuracy. Experimental results on two public datasets
demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07497">
<div class="article-summary-box-inner">
<span><p>Codifying commonsense knowledge in machines is a longstanding goal of
artificial intelligence. Recently, much progress toward this goal has been made
with automatic knowledge base (KB) construction techniques. However, such
techniques focus primarily on the acquisition of positive (true) KB statements,
even though negative (false) statements are often also important for
discriminative reasoning over commonsense KBs. As a first step toward the
latter, this paper proposes NegatER, a framework that ranks potential negatives
in commonsense KBs using a contextual language model (LM). Importantly, as most
KBs do not contain negatives, NegatER relies only on the positive knowledge in
the LM and does not require ground-truth negative examples. Experiments
demonstrate that, compared to multiple contrastive data augmentation
approaches, NegatER yields negatives that are more grammatical, coherent, and
informative -- leading to statistically significant accuracy improvements in a
challenging KB completion task and confirming that the positive knowledge in
LMs can be "re-purposed" to generate negative knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14388">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel training method, Conditional Masked Language
Modeling (CMLM), to effectively learn sentence representations on large scale
unlabeled corpora. CMLM integrates sentence representation learning into MLM
training by conditioning on the encoded vectors of adjacent sentences. Our
English CMLM model achieves state-of-the-art performance on SentEval, even
outperforming models learned using supervised signals. As a fully unsupervised
learning method, CMLM can be conveniently extended to a broad range of
languages and domains. We find that a multilingual CMLM model co-trained with
bitext retrieval (BR) and natural language inference (NLI) tasks outperforms
the previous state-of-the-art multilingual models by a large margin, e.g. 10%
improvement upon baseline models on cross-lingual semantic search. We explore
the same language bias of the learned representations, and propose a simple,
post-training and model agnostic approach to remove the language identifying
information from the representation while still retaining sentence semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15562">
<div class="article-summary-box-inner">
<span><p>Massively multilingual language models such as multilingual BERT offer
state-of-the-art cross-lingual transfer performance on a range of NLP tasks.
However, due to limited capacity and large differences in pretraining data
sizes, there is a profound performance gap between resource-rich and
resource-poor target languages. The ultimate challenge is dealing with
under-resourced languages not covered at all by the models and written in
scripts unseen during pretraining. In this work, we propose a series of novel
data-efficient methods that enable quick and effective adaptation of pretrained
multilingual models to such low-resource languages and unseen scripts. Relying
on matrix factorization, our methods capitalize on the existing latent
knowledge about multiple languages already available in the pretrained model's
embedding matrix. Furthermore, we show that learning of the new dedicated
embedding matrix in the target language can be improved by leveraging a small
number of vocabulary items (i.e., the so-called lexically overlapping tokens)
shared between mBERT's and target language vocabulary. Our adaptation
techniques offer substantial performance gains for languages with unseen
scripts. We also demonstrate that they can yield improvements for low-resource
languages written in scripts covered by the pretrained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15710">
<div class="article-summary-box-inner">
<span><p>Transformers (Vaswani et al., 2017) have brought a remarkable improvement in
the performance of neural machine translation (NMT) systems but they could be
surprisingly vulnerable to noise. In this work, we try to investigate how noise
breaks Transformers and if there exist solutions to deal with such issues.
There is a large body of work in the NMT literature on analyzing the behavior
of conventional models for the problem of noise but Transformers are relatively
understudied in this context. Motivated by this, we introduce a novel
data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate
noise during training. This idea is comparable to the well-known fine-tuning
strategy. Moreover, we propose two other novel extensions to the original
Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that
modify the neural architecture as well as the training process to handle noise.
One important characteristic of our techniques is that they only impact the
training phase and do not impose any overhead at inference time. We evaluated
our techniques to translate the English--German pair in both directions and
observed that our models have a higher tolerance to noise. More specifically,
they perform with no deterioration where up to 10% of entire test words are
infected by noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15781">
<div class="article-summary-box-inner">
<span><p>Influence functions approximate the "influences" of training data-points for
test predictions and have a wide variety of applications. Despite the
popularity, their computational cost does not scale well with model and
training data size. We present FastIF, a set of simple modifications to
influence functions that significantly improves their run-time. We use
k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good
candidate data points, identify the configurations that best balance the
speed-quality trade-off in estimating the inverse Hessian-vector product, and
introduce a fast parallel variant. Our proposed method achieves about 80X
speedup while being highly correlated with the original influence values. With
the availability of the fast influence functions, we demonstrate their
usefulness in four applications. First, we examine whether influential
data-points can "explain" test time behavior using the framework of
simulatability. Second, we visualize the influence interactions between
training and test data-points. Third, we show that we can correct model errors
by additional fine-tuning on certain influential data-points, improving the
accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we
experiment with a similar setup but fine-tuning on datapoints not seen during
training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI
datasets respectively. Overall, our fast influence functions can be efficiently
applied to large models and datasets, and our experiments demonstrate the
potential of influence functions in model interpretation and correcting model
errors. Code is available at
https://github.com/salesforce/fast-influence-functions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Commonsense Emergence in Few-shot Knowledge Models. (arXiv:2101.00297v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00297">
<div class="article-summary-box-inner">
<span><p>Recently, commonsense knowledge models - pretrained language models (LM)
fine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of
commonsense knowledge can be encoded in the parameters of large language
models. However, as parallel studies show that LMs are poor hypothesizers of
declarative commonsense relationships on their own, it remains unclear whether
this knowledge is learned during pretraining or from fine-tuning on KG
examples. To investigate this question, we train commonsense knowledge models
in few-shot settings to study the emergence of their commonsense representation
abilities. Our results show that commonsense knowledge models can rapidly adapt
from limited examples, indicating that KG fine-tuning serves to learn an
interface to encoded knowledge learned during pretraining. Importantly, our
analysis of absolute, angular, and distributional parameter changes during
few-shot fine-tuning provides novel insights into how this interface is
learned.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where "too much information" clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01192">
<div class="article-summary-box-inner">
<span><p>We introduce Generative Spoken Language Modeling, the task of learning the
acoustic and linguistic characteristics of a language from raw audio (no text,
no labels), and a set of metrics to automatically evaluate the learned
representations at acoustic and linguistic levels for both encoding and
generation. We set up baseline systems consisting of a discrete speech encoder
(returning pseudo-text units), a generative language model (trained on
pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all
trained without supervision and validate the proposed metrics with human
evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that
the number of discrete units (50, 100, or 200) matters in a task-dependent and
encoder-dependent way, and that some combinations approach text-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09761">
<div class="article-summary-box-inner">
<span><p>Large repositories of products, patents and scientific papers offer an
opportunity for building systems that scour millions of ideas and help users
discover inspirations. However, idea descriptions are typically in the form of
unstructured text, lacking key structure that is required for supporting
creative innovation interactions. Prior work has explored idea representations
that were limited in expressivity, required significant manual effort from
users, or dependent on curated knowledge bases with poor coverage. We explore a
novel representation that automatically breaks up products into fine-grained
functional facets capturing the purposes and mechanisms of ideas, and use it to
support important creative innovation interactions: functional search for
ideas, and exploration of the design space around a focal problem by viewing
related problem perspectives pooled from across many products. In user studies,
our approach boosts the quality of creative search and inspirations,
outperforming strong baselines by 50-60%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01378">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model's decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07352">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation models are sensitive to noise in the input texts,
such as misspelled words and ungrammatical constructions. Existing robustness
techniques generally fail when faced with unseen types of noise and their
performance degrades on clean texts. In this paper, we focus on three types of
realistic noise that are commonly generated by humans and introduce the idea of
visual context to improve translation robustness for noisy texts. In addition,
we describe a novel error correction training regime that can be used as an
auxiliary task to further improve translation robustness. Experiments on
English-French and English-German translation show that both multimodal and
error correction components improve model robustness to noisy texts, while
still retaining translation quality on clean texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11320">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting.
</p>
<p>Numerous natural language processing models have tried injecting commonsense
by using the ConceptNet knowledge base to improve performance on different
tasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect
human biases such as "lawyers are dishonest." It is important that these biases
are not conflated with the notion of commonsense. We study this missing yet
important problem by first defining and quantifying biases in ConceptNet as two
types of representational harms: overgeneralization of polarized perceptions
and representation disparity. We find that ConceptNet contains severe biases
and disparities across four demographic categories. In addition, we analyze two
downstream models that use ConceptNet as a source for commonsense knowledge and
find the existence of biases in those models as well. We further propose a
filtered-based bias-mitigation approach and examine its effectiveness. We show
that our mitigation approach can reduce the issues in both resource and models
but leads to a performance drop, leaving room for future work to build fairer
and stronger commonsense models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16590">
<div class="article-summary-box-inner">
<span><p>Text generation systems are ubiquitous in natural language processing
applications. However, evaluation of these systems remains a challenge,
especially in multilingual settings. In this paper, we propose L'AMBRE -- a
metric to evaluate the morphosyntactic well-formedness of text using its
dependency parse and morphosyntactic rules of the language. We present a way to
automatically extract various rules governing morphosyntax directly from
dependency treebanks. To tackle the noisy outputs from text generation systems,
we propose a simple methodology to train robust parsers. We show the
effectiveness of our metric on the task of machine translation through a
diachronic study of systems translating into morphologically-rich languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01454">
<div class="article-summary-box-inner">
<span><p>We introduce a few-shot transfer learning method for keyword spotting in any
language. Leveraging open speech corpora in nine languages, we automate the
extraction of a large multilingual keyword bank and use it to train an
embedding model. With just five training examples, we fine-tune the embedding
model for keyword spotting and achieve an average F1 score of 0.75 on keyword
classification for 180 new keywords unseen by the embedding model in these nine
languages. This embedding model also generalizes to new languages. We achieve
an average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13
new languages unseen by the embedding model. We investigate streaming accuracy
for our 5-shot models in two contexts: keyword spotting and keyword search.
Across 440 keywords in 22 languages, we achieve an average streaming keyword
spotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe
promising initial results on keyword search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05094">
<div class="article-summary-box-inner">
<span><p>Contrastive Learning has emerged as a powerful representation learning method
and facilitates various downstream tasks especially when supervised data is
limited. How to construct efficient contrastive samples through data
augmentation is key to its success. Unlike vision tasks, the data augmentation
method for contrastive learning has not been investigated sufficiently in
language tasks. In this paper, we propose a novel approach to construct
contrastive samples for language tasks using text summarization. We use these
samples for supervised contrastive learning to gain better text representations
which greatly benefit text classification tasks with limited annotations. To
further improve the method, we mix up samples from different classes and add an
extra regularization, named Mixsum, in addition to the cross-entropy-loss.
Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG
News, and IMDb) demonstrate the effectiveness of the proposed contrastive
learning framework with summarization-based data augmentation and Mixsum
regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WHOSe Heritage: Classification of UNESCO World Heritage "Outstanding Universal Value" Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05547">
<div class="article-summary-box-inner">
<span><p>The UNESCO World Heritage List (WHL) includes the exceptionally valuable
cultural and natural heritage to be preserved for mankind. Evaluating and
justifying the Outstanding Universal Value (OUV) is essential for each site
inscribed in the WHL, and yet a complex task, even for experts, since the
selection criteria of OUV are not mutually exclusive. Furthermore, manual
annotation of heritage values and attributes from multi-source textual data,
which is currently dominant in heritage studies, is knowledge-demanding and
time-consuming, impeding systematic analysis of such authoritative documents in
terms of their implications on heritage management. This study applies
state-of-the-art NLP models to build a classifier on a new dataset containing
Statements of OUV, seeking an explainable and scalable automation tool to
facilitate the nomination, evaluation, research, and monitoring processes of
World Heritage sites. Label smoothing is innovatively adapted to improve the
model performance by adding prior inter-class relationship knowledge to
generate soft labels. The study shows that the best models fine-tuned from BERT
and ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation
on the model prediction shows that the models are sufficiently generalizable.
The study is promising to be further developed and applied in heritage research
and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05837">
<div class="article-summary-box-inner">
<span><p>Relational knowledge bases (KBs) are commonly used to represent world
knowledge in machines. However, while advantageous for their high degree of
precision and interpretability, KBs are usually organized according to
manually-defined schemas, which limit their expressiveness and require
significant human efforts to engineer and maintain. In this review, we take a
natural language processing perspective to these limitations, examining how
they may be addressed in part by training deep contextual language models (LMs)
to internalize and express relational knowledge in more flexible forms. We
propose to organize knowledge representation strategies in LMs by the level of
KB supervision provided, from no KB supervision at all to entity- and
relation-level supervision. Our contributions are threefold: (1) We provide a
high-level, extensible taxonomy for knowledge representation in LMs; (2) Within
our taxonomy, we highlight notable models, evaluation tasks, and findings, in
order to provide an up-to-date review of current knowledge representation
capabilities in LMs; and (3) We suggest future research directions that build
upon the complementary aspects of LMs and KBs as knowledge representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
<div class="article-summary-box-inner">
<span><p>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06022">
<div class="article-summary-box-inner">
<span><p>We propose a parameter sharing method for Transformers (Vaswani et al.,
2017). The proposed approach relaxes a widely used technique, which shares
parameters for one layer with all layers such as Universal Transformers
(Dehghani et al., 2019), to increase the efficiency in the computational time.
We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign
parameters to each layer. Experimental results show that the proposed
strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the
configuration where we use many training data such as the recent WMT
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06644">
<div class="article-summary-box-inner">
<span><p>A possible explanation for the impressive performance of masked language
model (MLM) pre-training is that such models have learned to represent the
syntactic structures prevalent in classical NLP pipelines. In this paper, we
propose a different explanation: MLMs succeed on downstream tasks almost
entirely due to their ability to model higher-order word co-occurrence
statistics. To demonstrate this, we pre-train MLMs on sentences with randomly
shuffled word order, and show that these models still achieve high accuracy
after fine-tuning on many downstream tasks -- including on tasks specifically
designed to be challenging for models that ignore word order. Our models
perform surprisingly well according to some parametric syntactic probes,
indicating possible deficiencies in how we test representations for syntactic
information. Overall, our results show that purely distributional information
largely explains the success of pre-training, and underscore the importance of
curating challenging evaluation datasets that require deeper linguistic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06979">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings often requires a large amount of labeled data.
However, for most tasks and domains, labeled data is seldom available and
creating it is expensive. In this work, we present a new state-of-the-art
unsupervised method based on pre-trained Transformers and Sequential Denoising
Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.
It can achieve up to 93.1% of the performance of in-domain supervised
approaches. Further, we show that TSDAE is a strong domain adaptation and
pre-training method for sentence embeddings, significantly outperforming other
approaches like Masked Language Model.
</p>
<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work
mainly evaluates on the single task of Semantic Textual Similarity (STS), which
does not require any domain knowledge. It is unclear if these proposed methods
generalize to other domains and tasks. We fill this gap and evaluate TSDAE and
other recent approaches on four different datasets from heterogeneous domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07425">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLMs) have contributed to drastic performance
improvements with regard to zero anaphora resolution (ZAR). To further improve
this approach, in this study, we made two proposals. The first is a new
pretraining task that trains MLMs on anaphoric relations with explicit
supervision, and the second proposal is a new finetuning method that remedies a
notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese
ZAR demonstrated that our two proposals boost the state-of-the-art performance,
and our detailed analysis provides new insights on the remaining challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07637">
<div class="article-summary-box-inner">
<span><p>Natural languages display a trade-off among different strategies to convey
syntactic structure, such as word order or inflection. This trade-off, however,
has not appeared in recent simulations of iterated language learning with
neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in
light of three factors that play an important role in comparable experiments
from the Language Evolution field: (i) speaker bias towards efficient
messaging, (ii) non systematic input languages, and (iii) learning bottleneck.
Our simulations show that neural agents mainly strive to maintain the utterance
type distribution observed during learning, instead of developing a more
efficient or systematic language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07789">
<div class="article-summary-box-inner">
<span><p>A health outcome is a measurement or an observation used to capture and
assess the effect of a treatment. Automatic detection of health outcomes from
text would undoubtedly speed up access to evidence necessary in healthcare
decision making. Prior work on outcome detection has modelled this task as
either (a) a sequence labelling task, where the goal is to detect which text
spans describe health outcomes, or (b) a classification task, where the goal is
to classify a text into a pre-defined set of categories depending on an outcome
that is mentioned somewhere in that text. However, this decoupling of span
detection and classification is problematic from a modelling perspective and
ignores global structural correspondences between sentence-level and word-level
information present in a given text. To address this, we propose a method that
uses both word-level and sentence-level information to simultaneously perform
outcome span detection and outcome type classification. In addition to
injecting contextual information to hidden vectors, we use label attention to
appropriately weight both word and sentence level information. Experimental
results on several benchmark datasets for health outcome detection show that
our proposed method consistently outperforms decoupled methods, reporting
competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08247">
<div class="article-summary-box-inner">
<span><p>Intermediate task fine-tuning has been shown to culminate in large transfer
gains across many NLP tasks. With an abundance of candidate datasets as well as
pre-trained language models, it has become infeasible to run the cross-product
of all combinations to find the best transfer setting. In this work we first
establish that similar sequential fine-tuning gains can be achieved in adapter
settings, and subsequently consolidate previously proposed methods that
efficiently identify beneficial tasks for intermediate transfer learning. We
experiment with a diverse set of 42 intermediate and 11 target English
classification, multiple choice, question answering, and sequence tagging
tasks. Our results show that efficient embedding based methods that rely solely
on the respective datasets outperform computational expensive few-shot
fine-tuning approaches. Our best methods achieve an average Regret@3 of less
than 1% across all target tasks, demonstrating that we are able to efficiently
identify the best datasets for intermediate training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Wikily" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08384">
<div class="article-summary-box-inner">
<span><p>We present a simple but effective approach for leveraging Wikipedia for
neural machine translation as well as cross-lingual tasks of image captioning
and dependency parsing without using any direct supervision from external
parallel data or supervised models in the target language. We show that first
sentences and titles of linked Wikipedia pages, as well as cross-lingual image
captions, are strong signals for a seed parallel data to extract bilingual
dictionaries and cross-lingual word embeddings for mining parallel text from
Wikipedia. Our final model achieves high BLEU scores that are close to or
sometimes higher than strong supervised baselines in low-resource languages;
e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.
Moreover, we tailor our wikily supervised translation models to unsupervised
image captioning, and cross-lingual dependency parser transfer. In image
captioning, we train a multi-tasking machine translation and image captioning
pipeline for Arabic and English from which the Arabic training data is a
translated version of the English captioning data, using our wikily-supervised
translation models. Our captioning results on Arabic are slightly better than
that of its supervised model. In dependency parsing, we translate a large
amount of monolingual text, and use it as artificial training data in an
annotation projection framework. We show that our model outperforms recent work
on cross-lingual transfer of dependency parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08597">
<div class="article-summary-box-inner">
<span><p>Cross-lingual named-entity lexica are an important resource to multilingual
NLP tasks such as machine translation and cross-lingual wikification. While
knowledge bases contain a large number of entities in high-resource languages
such as English and French, corresponding entities for lower-resource languages
are often missing. To address this, we propose Lexical-Semantic-Phonetic Align
(LSP-Align), a technique to automatically mine cross-lingual entity lexica from
mined web data. We demonstrate LSP-Align outperforms baselines at extracting
cross-lingual entity pairs and mine 164 million entity pairs from 120 different
languages aligned with English. We release these cross-lingual entity pairs
along with the massively multilingual tagged named entity corpus as a resource
to the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08645">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language encoders, such as multilingual BERT and
XLM-R, show great potential for zero-shot cross-lingual transfer. However,
these multilingual encoders do not precisely align words and phrases across
languages. Especially, learning alignments in the multilingual embedding space
usually requires sentence-level or word-level parallel corpora, which are
expensive to be obtained for low-resource languages. An alternative is to make
the multilingual encoders more robust; when fine-tuning the encoder using
downstream task, we train the encoder to tolerate noise in the contextual
embedding spaces such that even if the representations of different languages
are not aligned well, the model can still achieve good performance on zero-shot
cross-lingual transfer. In this work, we propose a learning strategy for
training robust models by drawing connections between adversarial examples and
the failure cases of zero-shot cross-lingual transfer. We adopt two widely used
robust training methods, adversarial training and randomized smoothing, to
train the desired robust model. The experimental results demonstrate that
robust training improves zero-shot cross-lingual transfer on text
classification tasks. The improvement is more significant in the generalized
cross-lingual transfer setting, where the pair of input sentences belong to two
different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08685">
<div class="article-summary-box-inner">
<span><p>Are pairs of words that tend to occur together also likely to stand in a
linguistic dependency? This empirical question is motivated by a long history
of literature in cognitive science, psycholinguistics, and NLP. In this work we
contribute an extensive analysis of the relationship between linguistic
dependencies and statistical dependence between words. Improving on previous
work, we introduce the use of large pretrained language models to compute
contextualized estimates of the pointwise mutual information between words
(CPMI). For multiple models and languages, we extract dependency trees which
maximize CPMI, and compare to gold standard linguistic dependencies. Overall,
we find that CPMI dependencies achieve an unlabelled undirected attachment
score of at most $\approx 0.5$. While far above chance, and consistently above
a non-contextualized PMI baseline, this score is generally comparable to a
simple baseline formed by connecting adjacent words. We analyze which kinds of
linguistic dependencies are best captured in CPMI dependencies, and also find
marked differences between the estimates of the large pretrained language
models, illustrating how their different training schemes affect the type of
dependencies they capture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08799">
<div class="article-summary-box-inner">
<span><p>Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a
classical task for capturing the central idea from a given document. Based on
Seq2Seq models, the previous reinforcement learning framework on KG tasks
utilizes the evaluation metrics to further improve the well-trained neural
models. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are
only aware of the exact correctness of predictions on phrase-level and ignore
the semantic similarities between similar predictions and targets, which
inhibits the model from learning deep linguistic patterns. In response to this
problem, we propose a new fine-grained evaluation metric to improve the RL
framework, which considers different granularities: token-level $F_1$ score,
edit distance, duplication, and prediction quantities. On the whole, the new
framework includes two reward functions: the fine-grained evaluation score and
the vanilla $F_1$ score. This framework helps the model identifying some
partial match phrases which can be further optimized as the exact match ones.
Experiments on KG benchmarks show that our proposed training framework
outperforms the previous RL training frameworks among all evaluation scores. In
addition, our method can effectively ease the synonym problem and generate a
higher quality prediction. The source code is available at
\url{https://github.com/xuyige/FGRL4KG}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
<div class="article-summary-box-inner">
<span><p>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We find that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we propose a supervised
approach, which incorporates annotated pairs from natural language inference
datasets into our contrastive learning framework, by using "entailment" pairs
as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on
standard semantic textual similarity (STS) tasks, and our unsupervised and
supervised models using BERT base achieve an average of 76.3% and 81.6%
Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to
previous best results. We also show -- both theoretically and empirically --
that contrastive learning objective regularizes pre-trained embeddings'
anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09574">
<div class="article-summary-box-inner">
<span><p>Humans use commonsense reasoning (CSR) implicitly to produce natural and
coherent responses in conversations. Aiming to close the gap between current
response generation (RG) models and human communication abilities, we want to
understand why RG models respond as they do by probing RG model's understanding
of commonsense reasoning that elicits proper responses. We formalize the
problem by framing commonsense as a latent variable in the RG task and using
explanations for responses as textual form of commonsense. We collect 6k
annotated explanations justifying responses from four dialogue datasets and ask
humans to verify them and propose two probing settings to evaluate RG models'
CSR capabilities. Probing results show that models fail to capture the logical
relations between commonsense explanations and responses and fine-tuning on
in-domain data and increasing model sizes do not lead to understanding of CSR
for RG. We hope our study motivates more research in making RG models emulate
the human reasoning process in pursuit of smooth human-AI communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03432">
<div class="article-summary-box-inner">
<span><p>Concept-to-text Natural Language Generation is the task of expressing an
input meaning representation in natural language. Previous approaches in this
task have been able to generalise to rare or unseen instances by relying on a
delexicalisation of the input. However, this often requires that the input
appears verbatim in the output text. This poses challenges in multilingual
settings, where the task expands to generate the output text in multiple
languages given the same input. In this paper, we explore the application of
multilingual models in concept-to-text and propose Language Agnostic
Delexicalisation, a novel delexicalisation method that uses multilingual
pretrained embeddings, and employs a character-level post-editing model to
inflect words in their correct form during relexicalisation. Our experiments
across five datasets and five languages show that multilingual models
outperform monolingual models in concept-to-text and that our framework
outperforms previous approaches, especially for low resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03824">
<div class="article-summary-box-inner">
<span><p>We show that Transformer encoder architectures can be sped up, with limited
accuracy costs, by replacing the self-attention sublayers with simple linear
transformations that "mix" input tokens. These linear mixers, along with
standard nonlinearities in feed-forward layers, prove competent at modeling
semantic relationships in several text classification tasks. Most surprisingly,
we find that replacing the self-attention sublayer in a Transformer encoder
with a standard, unparameterized Fourier Transform achieves 92-97% of the
accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input
lengths, our FNet model is significantly faster: when compared to the
"efficient" Transformers on the Long Range Arena benchmark, FNet matches the
accuracy of the most accurate models, while outpacing the fastest models across
all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).
Finally, FNet has a light memory footprint and is particularly efficient at
smaller model sizes; for a fixed speed and accuracy budget, small FNet models
outperform Transformer counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01198">
<div class="article-summary-box-inner">
<span><p>In this work, we present to the NLP community, and to the wider research
community as a whole, an application for the diachronic analysis of research
corpora. We open source an easy-to-use tool coined: DRIFT, which allows
researchers to track research trends and development over the years. The
analysis methods are collated from well-cited research works, with a few of our
own methods added for good measure. Succinctly put, some of the analysis
methods are: keyword extraction, word clouds, predicting
declining/stagnant/growing trends using Productivity, tracking bi-grams using
Acceleration plots, finding the Semantic Drift of words, tracking trends using
similarity, etc. To demonstrate the utility and efficacy of our tool, we
perform a case study on the cs.CL corpus of the arXiv repository and draw
inferences from the analysis methods. The toolkit and the associated code are
available here: https://github.com/rajaswa/DRIFT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00270">
<div class="article-summary-box-inner">
<span><p>Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user's
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user's comments
conditioned on relevant user's reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Key information extraction (KIE) from document images requires understanding
the contextual and spatial semantics of texts in two-dimensional (2D) space.
Many recent studies try to solve the task by developing pre-training language
models focusing on combining visual features from document images with texts
and their layout. On the other hand, this paper tackles the problem by going
back to the basic: effective combination of text and layout. Specifically, we
propose a pre-trained language model, named BROS (BERT Relying On Spatiality),
that encodes relative positions of texts in 2D space and learns from unlabeled
documents with area-masking strategy. With this optimized training scheme for
understanding texts in 2D space, BROS shows comparable or better performance
compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and
SciTSR) without relying on visual features. This paper also reveals two
real-world challenges in KIE tasks--(1) minimizing the error from incorrect
text ordering and (2) efficient learning from fewer downstream examples--and
demonstrates the superiority of BROS over previous methods. Our code will be
open to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05669">
<div class="article-summary-box-inner">
<span><p>Isolated silos of scientific research and the growing challenge of
information overload limit awareness across the literature and hinder
innovation. Algorithmic curation and recommendation, which often prioritize
relevance, can further reinforce these informational "filter bubbles." In
response, we describe Bridger, a system for facilitating discovery of scholars
and their work, to explore design tradeoffs between relevant and novel
recommendations. We construct a faceted representation of authors with
information gleaned from their papers and inferred author personas, and use it
to develop an approach that locates commonalities ("bridges") and contrasts
between scientists -- retrieving partially similar authors rather than aiming
for strict similarity. In studies with computer science researchers, this
approach helps users discover authors considered useful for generating novel
research directions, outperforming a state-of-art neural model. In addition to
recommending new content, we also demonstrate an approach for displaying it in
a manner that boosts researchers' ability to understand the work of authors
with whom they are unfamiliar. Finally, our analysis reveals that Bridger
connects authors who have different citation profiles, publish in different
venues, and are more distant in social co-authorship networks, raising the
prospect of bridging diverse communities and facilitating discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07971">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11601">
<div class="article-summary-box-inner">
<span><p>Software developers write a lot of source code and documentation during
software development. Intrinsically, developers often recall parts of source
code or code summaries that they had written in the past while implementing
software or documenting them. To mimic developers' code or summary generation
behavior, we propose a retrieval augmented framework, REDCODER, that retrieves
relevant code or summaries from a retrieval database and provides them as a
supplement to code generation or summarization models. REDCODER has a couple of
uniqueness. First, it extends the state-of-the-art dense retrieval technique to
search for relevant code or summaries. Second, it can work with retrieval
databases that include unimodal (only code or natural language description) or
bimodal instances (code-description pairs). We conduct experiments and
extensive analysis on two benchmark datasets of code generation and
summarization in Java and Python, and the promising results endorse the
effectiveness of our proposed retrieval augmented framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications. For example, intent
classification in dialogue systems. The reason is that the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current methods can tackle such problems
reliably in a realistic scenario where zero OOD training data is available. In
this study, we propose ProtoInfoMax, a new architecture that extends
Prototypical Networks to simultaneously process in-domain and OOD sentences via
Mutual Information Maximization (InfoMax) objective. Experimental results show
that our proposed method can substantially improve performance up to 20% for
OOD detection in low resource settings of text classification. We also show
that ProtoInfoMax is less prone to typical overconfidence errors of Neural
Networks, leading to more reliable prediction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12738">
<div class="article-summary-box-inner">
<span><p>Recent advances in summarization provide models that can generate summaries
of higher quality. Such models now exist for a number of summarization tasks,
including query-based summarization, dialogue summarization, and multi-document
summarization. While such models and tasks are rapidly growing in the research
field, it has also become challenging for non-experts to keep track of them. To
make summarization methods more accessible to a wider audience, we develop
SummerTime by rethinking the summarization task from the perspective of an NLP
non-expert. SummerTime is a complete toolkit for text summarization, including
various models, datasets and evaluation metrics, for a full spectrum of
summarization-related tasks. SummerTime integrates with libraries designed for
NLP researchers, and enables users with easy-to-use APIs. With SummerTime,
users can locate pipeline solutions and search for the best model with their
own data, and visualize the differences, all with a few lines of code. We also
provide explanations for models and evaluation metrics to help users understand
the model behaviors and select models that best suit their needs. Our library,
along with a notebook demo, is available at
https://github.com/Yale-LILY/SummerTime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12971">
<div class="article-summary-box-inner">
<span><p>A smart contract is a program executed on a blockchain, based on which many
cryptocurrencies are implemented, and is being used for automating
transactions. Due to the large amount of money that smart contracts deal with,
there is a surging demand for a method that can statically and formally verify
them.
</p>
<p>This article describes our type-based static verification tool HELMHOLTZ for
Michelson, which is a statically typed stack-based language for writing smart
contracts that are executed on the blockchain platform Tezos. HELMHOLTZ is
designed on top of our extension of Michelson's type system with refinement
types. HELMHOLTZ takes a Michelson program annotated with a user-defined
specification written in the form of a refinement type as input; it then
typechecks the program against the specification based on the refinement type
system, discharging the generated verification conditions with the SMT solver
Z3. We briefly introduce our refinement type system for the core calculus
Mini-Michelson of Michelson, which incorporates the characteristic features
such as compound datatypes (e.g., lists and pairs), higher-order functions, and
invocation of another contract. \HELMHOLTZ{} successfully verifies several
practical Michelson programs, including one that transfers money to an account
and that checks a digital signature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
<div class="article-summary-box-inner">
<span><p>Keeping track of scientific challenges, advances and emerging directions is a
fundamental part of research. However, researchers face a flood of papers that
hinders discovery of important knowledge. In biomedicine, this directly impacts
human lives. To address this problem, we present a novel task of extraction and
search of scientific challenges and directions, to facilitate rapid knowledge
discovery. We construct and release an expert-annotated corpus of texts sampled
from full-length papers, labeled with novel semantic categories that generalize
across many types of challenges and directions. We focus on a large corpus of
interdisciplinary work relating to the COVID-19 pandemic, ranging from
biomedicine to areas such as AI and economics. We apply a model trained on our
data to identify challenges and directions across the corpus and build a
dedicated search engine. In experiments with 19 researchers and clinicians
using our system, we outperform a popular scientific search engine in assisting
knowledge discovery. Finally, we show that models trained on our resource
generalize to the wider biomedical domain and to AI papers, highlighting its
broad utility. We make our data, model and search engine publicly available.
https://challenges.apps.allenai.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01850">
<div class="article-summary-box-inner">
<span><p>As the digital news industry becomes the main channel of information
dissemination, the adverse impact of fake news is explosively magnified. The
credibility of a news report should not be considered in isolation. Rather,
previously published news articles on the similar event could be used to assess
the credibility of a news report. Inspired by this, we propose a BERT-based
multimodal unreliable news detection framework, which captures both textual and
visual information from unreliable articles utilising the contrastive learning
strategy. The contrastive learner interacts with the unreliable news classifier
to push similar credible news (or similar unreliable news) closer while moving
news articles with similar content but opposite credibility labels away from
each other in the multimodal embedding space. Experimental results on a
COVID-19 related dataset, ReCOVery, show that our model outperforms a number of
competitive baseline in unreliable news detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
<div class="article-summary-box-inner">
<span><p>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
<div class="article-summary-box-inner">
<span><p>An individual's variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
<div class="article-summary-box-inner">
<span><p>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03858">
<div class="article-summary-box-inner">
<span><p>Recent works have found evidence of gender bias in models of machine
translation and coreference resolution using mostly synthetic diagnostic
datasets. While these quantify bias in a controlled experiment, they often do
so on a small scale and consist mostly of artificial, out-of-distribution
sentences. In this work, we find grammatical patterns indicating stereotypical
and non-stereotypical gender-role assignments (e.g., female nurses versus male
dancers) in corpora from three domains, resulting in a first large-scale gender
bias dataset of 108K diverse real-world English sentences. We manually verify
the quality of our corpus and use it to evaluate gender bias in various
coreference resolution and machine translation models. We find that all tested
models tend to over-rely on gender stereotypes when presented with natural
inputs, which may be especially harmful when deployed in commercial systems.
Finally, we show that our dataset lends itself to finetuning a coreference
resolution model, finding it mitigates bias on a held out set. Our dataset and
models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will
spur future research into gender bias evaluation mitigation techniques in
realistic settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03969">
<div class="article-summary-box-inner">
<span><p>Transformers have recently become very popular for sequence-to-sequence
applications such as machine translation and speech recognition. In this work,
we propose a multi-task learning-based transformer model for low-resource
multilingual speech recognition for Indian languages. Our proposed model
consists of a conformer [1] encoder and two parallel transformer decoders. We
use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme
decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme
recognition task as an auxiliary task for our multi-task learning framework. We
jointly optimize the network for both phoneme and grapheme recognition tasks
using Joint CTC-Attention [2] training. We use a conditional decoding scheme to
inject the language information into the model before predicting the grapheme
sequence. Our experiments show that our proposed approach can obtain
significant improvement over previous approaches [4]. We also show that our
conformer-based dual-decoder approach outperforms both the transformer-based
dual-decoder approach and single decoder approach. Finally, We compare
monolingual ASR models with our proposed multilingual ASR approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04400">
<div class="article-summary-box-inner">
<span><p>In cross-lingual text classification, it is required that task-specific
training data in high-resource source languages are available, where the task
is identical to that of a low-resource target language. However, collecting
such training data can be infeasible because of the labeling cost, task
characteristics, and privacy concerns. This paper proposes an alternative
solution that uses only task-independent word embeddings of high-resource
languages and bilingual dictionaries. First, we construct a dictionary-based
heterogeneous graph (DHG) from bilingual dictionaries. This opens the
possibility to use graph neural networks for cross-lingual transfer. The
remaining challenge is the heterogeneity of DHG because multiple languages are
considered. To address this challenge, we propose dictionary-based
heterogeneous graph neural network (DHGNet) that effectively handles the
heterogeneity of DHG by two-step aggregations, which are word-level and
language-level aggregations. Experimental results demonstrate that our method
outperforms pretrained models even though it does not access to large corpora.
Furthermore, it can perform well even though dictionaries contain many
incorrect translations. Its robustness allows the usage of a wider range of
dictionaries such as an automatically constructed dictionary and crowdsourced
dictionary, which are convenient for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-13 23:09:18.635162273 UTC">2021-09-13 23:09:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>