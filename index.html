<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-14T01:30:00Z">09-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05042">
<div class="article-summary-box-inner">
<span><p>We present a grounded neural dialogue model that successfully collaborates
with people in a partially-observable reference game. We focus on a setting
where two agents each observe an overlapping part of a world context and need
to identify and agree on some object they share. Therefore, the agents should
pool their information and communicate pragmatically to solve the task. Our
dialogue agent accurately grounds referents from the partner's utterances using
a structured reference resolver, conditions on these referents using a
recurrent memory, and uses a pragmatic generation procedure to ensure the
partner can resolve the references the agent produces. We evaluate on the
OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving
a number of dots arranged on a board with continuously varying positions,
sizes, and shades. Our agent substantially outperforms the previous state of
the art for the task, obtaining a 20% relative improvement in successful task
completion in self-play evaluations and a 50% relative improvement in success
in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05052">
<div class="article-summary-box-inner">
<span><p>Knowledge-dependent tasks typically use two sources of knowledge: parametric,
learned at training time, and contextual, given as a passage at inference time.
To understand how models use these sources together, we formalize the problem
of knowledge conflicts, where the contextual information contradicts the
learned information. Analyzing the behaviour of popular models, we measure
their over-reliance on memorized information (the cause of hallucinations), and
uncover important factors that exacerbate this behaviour. Lastly, we propose a
simple method to mitigate over-reliance on parametric knowledge, which
minimizes hallucination, and improves out-of-distribution generalization by
4%-7%. Our findings demonstrate the importance for practitioners to evaluate
model tendency to hallucinate rather than read, and show that our mitigation
strategy encourages generalization to evolving information (i.e.,
time-dependent queries). To encourage these practices, we have released our
framework for generating knowledge conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05056">
<div class="article-summary-box-inner">
<span><p>Dialogue Act (DA) classification is the task of classifying utterances with
respect to the function they serve in a dialogue. Existing approaches to DA
classification model utterances without incorporating the turn changes among
speakers throughout the dialogue, therefore treating it no different than
non-interactive written text. In this paper, we propose to integrate the turn
changes in conversations among speakers when modeling DAs. Specifically, we
learn conversation-invariant speaker turn embeddings to represent the speaker
turns in a conversation; the learned speaker turn embeddings are then merged
with the utterance embeddings for the downstream task of DA classification.
With this simple yet effective mechanism, our model is able to capture the
semantics from the dialogue content while accounting for different speaker
turns in a conversation. Validation on three benchmark public datasets
demonstrates superior performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05074">
<div class="article-summary-box-inner">
<span><p>Transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance across various NLP tasks including the
identification of offensive language and hate speech, an important problem in
social media. In this paper, we present fBERT, a BERT model retrained on SOLID,
the largest English offensive language identification corpus available with
over $1.4$ million offensive instances. We evaluate fBERT's performance on
identifying offensive content on multiple English datasets and we test several
thresholds for selecting instances from SOLID. The fBERT model will be made
freely available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05090">
<div class="article-summary-box-inner">
<span><p>Neural language modelling has progressed the state-of-the-art in different
downstream Natural Language Processing (NLP) tasks. One such area is of
open-domain dialog modelling, neural dialog models based on GPT-2 such as
DialoGPT have shown promising performance in single-turn conversation. However,
such (neural) dialog models have been criticized for generating responses which
although may have relevance to the previous human response, tend to quickly
dissipate human interest and descend into trivial conversation. One reason for
such performance is the lack of explicit conversation strategy being employed
in human-machine conversation. Humans employ a range of conversation strategies
while engaging in a conversation, one such key social strategies is
Self-disclosure(SD). A phenomenon of revealing information about one-self to
others. Social penetration theory (SPT) proposes that communication between two
people moves from shallow to deeper levels as the relationship progresses
primarily through self-disclosure. Disclosure helps in creating rapport among
the participants engaged in a conversation. In this paper, Self-disclosure
enhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic
Model (SDTM) during inference stage of a neural dialog model to re-rank
response candidates to enhance self-disclosure in single-turn responses from
from the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05093">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models for textual data have an unconstrained
output space; at each decoding step, they can produce any of 10,000s of
sub-word tokens. When fine-tuned to target constrained formal languages like
SQL, these models often generate invalid code, rendering it unusable. We
propose PICARD (code and trained models available at
https://github.com/ElementAI/picard), a method for constraining auto-regressive
decoders of language models through incremental parsing. PICARD helps to find
valid output sequences by rejecting inadmissible tokens at each decoding step.
On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that
PICARD transforms fine-tuned T5 models with passable performance into
state-of-the-art solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05097">
<div class="article-summary-box-inner">
<span><p>A hyperbole is an intentional and creative exaggeration not to be taken
literally. Despite its ubiquity in daily life, the computational explorations
of hyperboles are scarce. In this paper, we tackle the under-explored and
challenging task: sentence-level hyperbole generation. We start with a
representative syntactic pattern for intensification and systematically study
the semantic (commonsense and counterfactual) relationships between each
component in such hyperboles. Next, we leverage the COMeT and reverse COMeT
models to do commonsense and counterfactual inference. We then generate
multiple hyperbole candidates based on our findings from the pattern, and train
neural classifiers to rank and select high-quality hyperboles. Automatic and
human evaluations show that our generation method is able to generate
hyperboles creatively with high success rate and intensity scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05105">
<div class="article-summary-box-inner">
<span><p>Can we get existing language models and refine them for zero-shot commonsense
reasoning? This paper presents an initial study exploring the feasibility of
zero-shot commonsense reasoning for the Winograd Schema Challenge by
formulating the task as self-supervised refinement of a pre-trained language
model. In contrast to previous studies that rely on fine-tuning annotated
datasets, we seek to boost conceptualization via loss landscape refinement. To
this end, we propose a novel self-supervised learning approach that refines the
language model utilizing a set of linguistic perturbations of similar concept
relationships. Empirical analysis of our conceptually simple framework
demonstrates the viability of zero-shot commonsense reasoning on multiple
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05108">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has recently attracted considerable attention in the
NLP community for its ability to learn discriminative features using a
contrastive objective. This paper investigates whether contrastive learning can
be extended to Transfomer attention to tackling the Winograd Schema Challenge.
To this end, we propose a novel self-supervised framework, leveraging a
contrastive loss directly at the level of self-attention. Experimental analysis
of our attention-based models on multiple datasets demonstrates superior
commonsense reasoning capabilities. The proposed approach outperforms all
comparable unsupervised approaches while occasionally surpassing supervised
ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05112">
<div class="article-summary-box-inner">
<span><p>For over thirty years, researchers have developed and analyzed methods for
latent tree induction as an approach for unsupervised syntactic parsing.
Nonetheless, modern systems still do not perform well enough compared to their
supervised counterparts to have any practical use as structural annotation of
text. In this work, we present a technique that uses distant supervision in the
form of span constraints (i.e. phrase bracketing) to improve performance in
unsupervised constituency parsing. Using a relatively small number of span
constraints we can substantially improve the output from DIORA, an already
competitive unsupervised parsing system. Compared with full parse tree
annotation, span constraints can be acquired with minimal effort, such as with
a lexicon derived from Wikipedia, to find exact text matches. Our experiments
show span constraints based on entities improves constituency parsing on
English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to
any domain where span constraints are easily attainable, and as a case study we
demonstrate its effectiveness by parsing biomedical text from the CRAFT
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05115">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an approach to improve image captioning solutions
for images with novel objects that do not have caption labels in the training
dataset. Our approach is agnostic to model architecture, and primarily focuses
on training technique that uses existing fully paired image-caption data and
the images with only the novel object detection labels (partially paired data).
We create synthetic paired captioning data for these novel objects by
leveraging context from existing image-caption pairs. We further re-use these
partially paired images with novel objects to create pseudo-label captions that
are used to fine-tune the captioning model. Using a popular captioning model
(Up-Down) as baseline, our approach achieves state-of-the-art results on
held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for
novel object images by 75.8 and 26.6 points respectively, compared to baseline
model that does not use partially paired images during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05125">
<div class="article-summary-box-inner">
<span><p>Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL's performance
matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL's text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05126">
<div class="article-summary-box-inner">
<span><p>Existing research studies on cross-sentence relation extraction in long-form
multi-party conversations aim to improve relation extraction without
considering the explainability of such methods. This work addresses that gap by
focusing on extracting explanations that indicate that a relation exists while
using only partially labeled data. We propose our model-agnostic framework,
D-REX, a policy-guided semi-supervised algorithm that explains and ranks
relations. We frame relation extraction as a re-ranking task and include
relation- and entity-specific explanations as an intermediate step of the
inference process. We find that about 90% of the time, human annotators prefer
D-REX's explanations over a strong BERT-based joint relation extraction and
explanation model. Finally, our evaluations on a dialogue relation extraction
dataset show that our method is simple yet effective and achieves a
state-of-the-art F1 score on relation extraction, improving upon existing
methods by 13.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05140">
<div class="article-summary-box-inner">
<span><p>Many NLG tasks such as summarization, dialogue response, or open domain
question answering focus primarily on a source text in order to generate a
target response. This standard approach falls short, however, when a user's
intent or context of work is not easily recoverable based solely on that source
text -- a scenario that we argue is more of the rule than the exception. In
this work, we argue that NLG systems in general should place a much higher
level of emphasis on making use of additional context, and suggest that
relevance (as used in Information Retrieval) be thought of as a crucial tool
for designing user-oriented text-generating tasks. We further discuss possible
harms and hazards around such personalization, and argue that value-sensitive
design represents a crucial path forward through these challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05149">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new verification style reading comprehension
dataset named VGaokao from Chinese Language tests of Gaokao. Different from
existing efforts, the new dataset is originally designed for native speakers'
evaluation, thus requiring more advanced language understanding skills. To
address the challenges in VGaokao, we propose a novel Extract-Integrate-Compete
approach, which iteratively selects complementary evidence with a novel query
updating mechanism and adaptively distills supportive evidence, followed by a
pairwise competition to push models to learn the subtle difference among
similar text pieces. Experiments show that our methods outperform various
baselines on VGaokao with retrieved complementary evidence, while having the
merits of efficiency and explainability. Our dataset and code are released for
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05153">
<div class="article-summary-box-inner">
<span><p>Addressing the mismatch between natural language descriptions and the
corresponding SQL queries is a key challenge for text-to-SQL translation. To
bridge this gap, we propose an SQL intermediate representation (IR) called
Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities
of SQL, while it simplifies the queries as follows: (1) dispensing with
operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are
usually hard to find counterparts for in the text descriptions; (2) removing
the need for nested subqueries and set operators; and (3) making schema linking
easier by reducing the required number of schema items. On Spider, a
challenging text-to-SQL benchmark that contains complex and nested SQL queries,
we demonstrate that NatSQL outperforms other IRs, and significantly improves
the performance of several previous SOTA models. Furthermore, for existing
models that do not support executable SQL generation, NatSQL easily enables
them to generate executable SQL queries, and achieves the new state-of-the-art
execution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05157">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in studying neural networks for
translating text descriptions into SQL queries under the zero-shot cross-domain
setting. Despite achieving good performance on some public benchmarks, we
observe that existing text-to-SQL models do not generalize when facing domain
knowledge that does not frequently appear in the training data, which may
render the worse prediction performance for unseen domains. In this work, we
investigate the robustness of text-to-SQL models when the questions require
rarely observed domain knowledge. In particular, we define five types of domain
knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),
a human-curated dataset based on the Spider benchmark for text-to-SQL
translation. NL questions in Spider-DK are selected from Spider, and we modify
some samples by adding domain knowledge that reflects real-world question
paraphrases. We demonstrate that the prediction accuracy dramatically drops on
samples that require such domain knowledge, even if the domain knowledge
appears in the training set, and the model provides the correct predictions for
related training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05160">
<div class="article-summary-box-inner">
<span><p>With the explosive growth of livestream broadcasting, there is an urgent need
for new summarization technology that enables us to create a preview of
streamed content and tap into this wealth of knowledge. However, the problem is
nontrivial due to the informal nature of spoken language. Further, there has
been a shortage of annotated datasets that are necessary for transcript
summarization. In this paper, we present StreamHover, a framework for
annotating and summarizing livestream transcripts. With a total of over 500
hours of videos annotated with both extractive and abstractive summaries, our
benchmark dataset is significantly larger than currently existing annotated
corpora. We explore a neural extractive summarization model that leverages
vector-quantized variational autoencoder to learn latent vector representations
of spoken utterances and identify salient utterances from the transcripts to
form summaries. We show that our model generalizes better and improves
performance over strong baselines. The results of this study provide an avenue
for future research to improve summarization solutions for efficient browsing
of livestreams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05168">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (PLMs) have led to great success on various
commonsense question answering (QA) tasks in an end-to-end fashion. However,
little attention has been paid to what commonsense knowledge is needed to
deeply characterize these QA tasks. In this work, we proposed to categorize the
semantics needed for these tasks using the SocialIQA as an example. Building
upon our labeled social knowledge categories dataset on top of SocialIQA, we
further train neural QA models to incorporate such social knowledge categories
and relation information from a knowledge base. Unlike previous work, we
observe our models with semantic categorizations of social knowledge can
achieve comparable performance with a relatively simple model and smaller size
compared to other complex approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05178">
<div class="article-summary-box-inner">
<span><p>We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task
Learning (MSNF-MTCL) to predict 5 important students' retention risks: future
dropout, next semester dropout, type of dropout, duration of dropout and cause
of dropout. First, we develop a general purpose multi-modal neural fusion
network model MSNF for learning students' academic information representation
by fusing spatial and temporal unstructured advising notes with spatiotemporal
structured data. MSNF combines a Bidirectional Encoder Representations from
Transformers (BERT)-based document embedding framework to represent each
advising note, Long-Short Term Memory (LSTM) network to model temporal advising
note embeddings, LSTM network to model students' temporal performance variables
and students' static demographics altogether. The final fused representation
from MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model
towards building MSNF-MTCL for predicting 5 student retention risks. We
evaluate MSNFMTCL on a large educational database consists of 36,445 college
students over 18 years period of time that provides promising performances
comparing with the nearest state-of-art models. Additionally, we test the
fairness of such model given the existence of biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05179">
<div class="article-summary-box-inner">
<span><p>Generating high quality question-answer pairs is a hard but meaningful task.
Although previous works have achieved great results on answer-aware question
generation, it is difficult to apply them into practical application in the
education field. This paper for the first time addresses the question-answer
pair generation task on the real-world examination data, and proposes a new
unified framework on RACE. To capture the important information of the input
passage we first automatically generate(rather than extracting) keyphrases,
thus this task is reduced to keyphrase-question-answer triplet joint
generation. Accordingly, we propose a multi-agent communication model to
generate and optimize the question and keyphrases iteratively, and then apply
the generated question and keyphrases to guide the generation of answers. To
establish a solid benchmark, we build our model on the strong generative
pre-training model. Experimental results show that our model makes great
breakthroughs in the question-answer pair generation task. Moreover, we make a
comprehensive analysis on our model, suggesting new directions for this
challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05182">
<div class="article-summary-box-inner">
<span><p>Dialogue-based relation extraction (DiaRE) aims to detect the structural
information from unstructured utterances in dialogues. Existing relation
extraction models may be unsatisfactory under such a conversational setting,
due to the entangled logic and information sparsity issues in utterances
involving multiple speakers. To this end, we introduce SOLS, a novel model
which can explicitly induce speaker-oriented latent structures for better
DiaRE. Specifically, we learn latent structures to capture the relationships
among tokens beyond the utterance boundaries, alleviating the entangled logic
issue. During the learning process, our speaker-specific regularization method
progressively highlights speaker-related key clues and erases the irrelevant
ones, alleviating the information sparsity issue. Experiments on three public
datasets demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05184">
<div class="article-summary-box-inner">
<span><p>Internet memes have become powerful means to transmit political,
psychological, and socio-cultural ideas. Although memes are typically humorous,
recent days have witnessed an escalation of harmful memes used for trolling,
cyberbullying, and abusing social entities. Detecting such harmful memes is
challenging as they can be highly satirical and cryptic. Moreover, while
previous work has focused on specific aspects of memes such as hate speech and
propaganda, there has been little work on harm in general, and only one
specialized dataset for it. Here, we focus on bridging this gap. In particular,
we aim to solve two novel tasks: detecting harmful memes and identifying the
social entities they target. We further extend the recently released HarMeme
dataset to generalize on two prevalent topics - COVID-19 and US politics and
name the two datasets as Harm-C and Harm-P, respectively. We then propose
MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a
novel multimodal (text + image) deep neural model, which uses global and local
perspectives to detect harmful memes. MOMENTA identifies the object proposals
and attributes and uses a multimodal model to perceive the comprehensive
context in which the objects and the entities are portrayed in a given meme.
MOMENTA is interpretable and generalizable, and it outperforms numerous
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05186">
<div class="article-summary-box-inner">
<span><p>This paper investigates continual learning for semantic parsing. In this
setting, a neural semantic parser learns tasks sequentially without accessing
full training data from previous tasks. Direct application of the SOTA
continual learning algorithms to this problem fails to achieve comparable
performance with re-training models with all seen tasks because they have not
considered the special properties of structured outputs yielded by semantic
parsers. Therefore, we propose TotalRecall, a continual learning method
designed for neural semantic parsers from two aspects: i) a sampling method for
memory replay that diversifies logical form templates and balances
distributions of parse actions in a memory; ii) a two-stage training method
that significantly improves generalization capability of the parsers across
tasks. We conduct extensive experiments to study the research problems involved
in continual semantic parsing and demonstrate that a neural semantic parser
trained with TotalRecall achieves superior performance than the one trained
directly with the SOTA continual learning algorithms and achieve a 3-6 times
speedup compared to re-training from scratch. Code and datasets are available
at: https://github.com/zhuang-li/cl_nsp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05187">
<div class="article-summary-box-inner">
<span><p>A multi-turn dialogue always follows a specific topic thread, and topic shift
at the discourse level occurs naturally as the conversation progresses,
necessitating the model's ability to capture different topics and generate
topic-aware responses. Previous research has either predicted the topic first
and then generated the relevant response, or simply applied the attention
mechanism to all topics, ignoring the joint distribution of the topic
prediction and response generation models and resulting in uncontrollable and
unrelated responses. In this paper, we propose a joint framework with a topic
refinement mechanism to learn these two tasks simultaneously. Specifically, we
design a three-pass iteration mechanism to generate coarse response first, then
predict corresponding topics, and finally generate refined response conditioned
on predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the
topic prediction task respectively, aiming to investigate the effects of joint
learning and the understanding ability of GPT model. Experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance at response generation task and the great potential understanding
capability of GPT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05190">
<div class="article-summary-box-inner">
<span><p>Eliciting knowledge contained in language models via prompt-based learning
has shown great potential in many natural language processing tasks, such as
text classification and generation. Whereas, the applications for more complex
tasks such as event extraction are less studied, since the design of prompt is
not straightforward due to the complicated types and arguments. In this paper,
we explore to elicit the knowledge from pre-trained language models for event
trigger detection and argument extraction. Specifically, we present various
joint trigger/argument prompt methods, which can elicit more complementary
knowledge by modeling the interactions between different triggers or arguments.
The experimental results on the benchmark dataset, namely ACE2005, show the
great advantages of our proposed approach. In particular, our approach is
superior to the recent advanced methods in the few-shot scenario where only a
few samples are used for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05199">
<div class="article-summary-box-inner">
<span><p>The new era of technology has brought us to the point where it is convenient
for people to share their opinions over an abundance of platforms. These
platforms have a provision for the users to express themselves in multiple
forms of representations, including text, images, videos, and audio. This,
however, makes it difficult for users to obtain all the key information about a
topic, making the task of automatic multi-modal summarization (MMS) essential.
In this paper, we present a comprehensive survey of the existing research in
the area of MMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05213">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) aims to extract structural information from
unstructured texts. In practice, long-tailed distributions caused by the
selection bias of a dataset, may lead to incorrect correlations, also known as
spurious correlations, between entities and labels in the conventional
likelihood models. This motivates us to propose counterfactual IE (CFIE), a
novel framework that aims to uncover the main causalities behind data in the
view of causal inference. Specifically, 1) we first introduce a unified
structural causal model (SCM) for various IE tasks, describing the
relationships among variables; 2) with our SCM, we then generate
counterfactuals based on an explicit language structure to better calculate the
direct causal effect during the inference stage; 3) we further propose a novel
debiasing approach to yield more robust predictions. Experiments on three IE
tasks across five public datasets show the effectiveness of our CFIE model in
mitigating the spurious correlation issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05217">
<div class="article-summary-box-inner">
<span><p>In recent years, several high-performance conversational systems have been
proposed based on the Transformer encoder-decoder model. Although previous
studies analyzed the effects of the model parameters and the decoding method on
subjective dialogue evaluations with overall metrics, they did not analyze how
the differences of fine-tuning datasets affect on user's detailed impression.
In addition, the Transformer-based approach has only been verified for English,
not for such languages with large inter-language distances as Japanese. In this
study, we develop large-scale Transformer-based Japanese dialogue models and
Japanese chit-chat datasets to examine the effectiveness of the
Transformer-based approach for building chit-chat dialogue systems. We
evaluated and analyzed the impressions of human dialogues in different
fine-tuning datasets, model parameters, and the use of additional information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05233">
<div class="article-summary-box-inner">
<span><p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large
amountsof fully annotated training data. However, ac-cessible data are often
incompletely annotatedsince the annotators usually lack comprehen-sive
knowledge in the target domain. Normallythe unannotated tokens are regarded as
non-entities by default, while we underline thatthese tokens could either be
non-entities orpart of any entity. Here, we study NER mod-eling with incomplete
annotated data whereonly a fraction of the named entities are la-beled, and the
unlabeled tokens are equiva-lently multi-labeled by every possible label.Taking
multi-labeled tokens into account, thenumerous possible paths can distract the
train-ing model from the gold path (ground truthlabel sequence), and thus
hinders the learn-ing ability. In this paper, we propose AdaK-NER, named the
adaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion
where the gold path is more likely to belocated. We demonstrate the superiority
ofour approach through extensive experimentson both English and Chinese
datasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on
two Chinese datasetscompared with the prior state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05234">
<div class="article-summary-box-inner">
<span><p>Few-shot slot tagging is an emerging research topic in the field of Natural
Language Understanding (NLU). With sufficient annotated data from source
domains, the key challenge is how to train and adapt the model to another
target domain which only has few labels. Conventional few-shot approaches use
all the data from the source domains without considering inter-domain relations
and implicitly assume each sample in the domain contributes equally. However,
our experiments show that the data distribution bias among different domains
will significantly affect the adaption performance. Moreover, transferring
knowledge from dissimilar domains will even introduce some extra noises so that
affect the performance of models. To tackle this problem, we propose an
effective similarity-based method to select data from the source domains. In
addition, we propose a Shared-Private Network (SP-Net) for the few-shot slot
tagging task. The words from the same class would have some shared features. We
extract those shared features from the limited annotated data on the target
domain and merge them together as the label embedding to help us predict other
unlabelled data on the target domain. The experiment shows that our method
outperforms the state-of-the-art approaches with fewer source data. The result
also proves that some training data from dissimilar sources are redundant and
even negative for the adaption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05238">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05244">
<div class="article-summary-box-inner">
<span><p>Cross-attention is an important component of neural machine translation
(NMT), which is always realized by dot-product attention in previous methods.
However, dot-product attention only considers the pair-wise correlation between
words, resulting in dispersion when dealing with long sentences and neglect of
source neighboring relationships. Inspired by linguistics, the above issues are
caused by ignoring a type of cross-attention, called concentrated attention,
which focuses on several central words and then spreads around them. In this
work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention
in cross-attention. Experiments and analyses we conducted on three datasets
show that the proposed method outperforms the baseline and has significant
improvement on alignment quality, N-gram accuracy, and long sentence
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05250">
<div class="article-summary-box-inner">
<span><p>Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain
manually annotated event-centric mentions of events and entities that form
coreference chains with identity relations. ECB+ is a state-of-the-art CDCR
dataset that focuses on the resolution of events and their descriptive
attributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that
annotates coreference chains of both events and entities with a strong variance
of word choice and more loosely-related coreference anaphora, e.g., bridging or
near-identity relations. In this paper, we qualitatively and quantitatively
compare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We
propose a phrasing diversity metric (PD) that compares lexical diversity within
coreference chains on a more detailed level than previously proposed metric,
e.g., a number of unique lemmas. We discuss the different tasks that both CDCR
datasets create, i.e., lexical disambiguation and lexical diversity challenges,
and propose a direction for further CDCR evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05252">
<div class="article-summary-box-inner">
<span><p>Datasets and methods for cross-document coreference resolution (CDCR) focus
on events or entities with strict coreference relations. They lack, however,
annotating and resolving coreference mentions with more abstract or loose
relations that may occur when news articles report about controversial and
polarized events. Bridging and loose coreference relations trigger associations
that may lead to exposing news readers to bias by word choice and labeling. For
example, coreferential mentions of "direct talks between U.S. President Donald
Trump and Kim" such as "an extraordinary meeting following months of heated
rhetoric" or "great chance to solve a world problem" form a more positive
perception of this event. A step towards bringing awareness of bias by word
choice and labeling is the reliable resolution of coreferences with high
lexical diversity. We propose an unsupervised method named XCoref, which is a
CDCR method that capably resolves not only previously prevalent entities, such
as persons, e.g., "Donald Trump," but also abstractly defined concepts, such as
groups of persons, "caravan of immigrants," events and actions, e.g., "marching
to the U.S. border." In an extensive evaluation, we compare the proposed XCoref
to a state-of-the-art CDCR method and a previous method TCA that resolves such
complex coreference relations and find that XCoref outperforms these methods.
Outperforming an established CDCR model shows that the new CDCR models need to
be evaluated on semantically complex mentions with more loose coreference
relations to indicate their applicability of models to resolve mentions in the
"wild" of political news articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05256">
<div class="article-summary-box-inner">
<span><p>Can pre-trained BERT for one language and GPT for another be glued together
to translate texts? Self-supervised training using only monolingual data has
led to the success of pre-trained (masked) language models in many NLP tasks.
However, directly connecting BERT as an encoder and GPT as a decoder can be
challenging in machine translation, for GPT-like models lack a cross-attention
component that is needed in seq2seq decoders. In this paper, we propose
Graformer to graft separately pre-trained (masked) language models for machine
translation. With monolingual data for pre-training and parallel data for
grafting training, we maximally take advantage of the usage of both types of
data. Experiments on 60 directions show that our method achieves average
improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with
the multilingual Transformer of the same size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
<div class="article-summary-box-inner">
<span><p>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05289">
<div class="article-summary-box-inner">
<span><p>A flaw in QA evaluation is that annotations often only provide one gold
answer. Thus, model predictions semantically equivalent to the answer but
superficially different are considered incorrect. This work explores mining
alias entities from knowledge bases and using them as additional gold answers
(i.e., equivalent answers). We incorporate answers for two settings: evaluation
with additional answers and model training with equivalent answers. We analyse
three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion
increases the exact match score on all datasets for evaluation, while
incorporating it helps model training over real-world datasets. We ensure the
additional answers are valid through a human post hoc evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05312">
<div class="article-summary-box-inner">
<span><p>Generating goal-oriented questions in Visual Dialogue tasks is a challenging
and long-standing problem. State-Of-The-Art systems are shown to generate
questions that, although grammatically correct, often lack an effective
strategy and sound unnatural to humans. Inspired by the cognitive literature on
information search and cross-situational word learning, we design Confirm-it, a
model based on a beam search re-ranking algorithm that guides an effective
goal-oriented strategy by asking questions that confirm the model's conjecture
about the referent. We take the GuessWhat?! game as a case-study. We show that
dialogues generated by Confirm-it are more natural and effective than beam
search decoding without re-ranking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05317">
<div class="article-summary-box-inner">
<span><p>Causal inference using observational text data is becoming increasingly
popular in many research areas. This paper presents the Bayesian Topic
Regression (BTR) model that uses both text and numerical information to model
an outcome variable. It allows estimation of both discrete and continuous
treatment effects. Furthermore, it allows for the inclusion of additional
numerical confounding factors next to text data. To this end, we combine a
supervised Bayesian topic model with a Bayesian regression framework and
perform supervised representation learning for the text features jointly with
the regression parameter training, respecting the Frisch-Waugh-Lovell theorem.
Our paper makes two main contributions. First, we provide a regression
framework that allows causal inference in settings when both text and numerical
confounders are of relevance. We show with synthetic and semi-synthetic
datasets that our joint approach recovers ground truth with lower bias than any
benchmark model, when text and numerical features are correlated. Second,
experiments on two real-world datasets demonstrate that a joint and supervised
learning strategy also yields superior prediction results compared to
strategies that estimate regression weights for text and non-text features
separately, being even competitive with more complex deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05322">
<div class="article-summary-box-inner">
<span><p>Hate speech has grown significantly on social media, causing serious
consequences for victims of all demographics. Despite much attention being paid
to characterize and detect discriminatory speech, most work has focused on
explicit or overt hate speech, failing to address a more pervasive form based
on coded or indirect language. To fill this gap, this work introduces a
theoretically-justified taxonomy of implicit hate speech and a benchmark corpus
with fine-grained labels for each message and its implication. We present
systematic analyses of our dataset using contemporary baselines to detect and
explain implicit hate speech, and we discuss key features that challenge
existing models. This dataset will continue to serve as a useful benchmark for
understanding this multifaceted issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05325">
<div class="article-summary-box-inner">
<span><p>Framing has significant but subtle effects on public opinion and policy. We
propose an NLP framework to measure entity-centric frames. We use it to
understand media coverage on police violence in the United States in a new
Police Violence Frames Corpus of 82k news articles spanning 7k police killings.
Our work uncovers more than a dozen framing devices and reveals significant
differences in the way liberal and conservative news sources frame both the
issue of police violence and the entities involved. Conservative sources
emphasize when the victim is armed or attacking an officer and are more likely
to mention the victim's criminal record. Liberal sources focus more on the
underlying systemic injustice, highlighting the victim's race and that they
were unarmed. We discover temporary spikes in these injustice frames near
high-profile shooting events, and finally, we show protest volume correlates
with and precedes media framing decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05327">
<div class="article-summary-box-inner">
<span><p>Numerous government initiatives (e.g. the EU with GDPR) are coming to the
conclusion that the increasing complexity of modern software systems must be
contrasted with some Rights to Explanation and metrics for the Impact
Assessment of these tools, that allow humans to understand and oversee the
output of Automated Decision Making systems. Explainable AI was born as a
pathway to allow humans to explore and understand the inner working of complex
systems. But establishing what is an explanation and objectively evaluating
explainability, are not trivial tasks. With this paper, we present a new
model-agnostic metric to measure the Degree of eXplainability of correct
information in an objective way, exploiting a specific model from Ordinary
Language Philosophy called the Achinstein's Theory of Explanations. In order to
understand whether this metric is actually behaving as explainability is
expected to, we designed a few experiments and a user-study on two realistic
AI-based systems for healthcare and finance, involving famous AI technology
including Artificial Neural Networks and TreeSHAP. The results we obtained are
very encouraging, suggesting that our proposed metric for measuring the Degree
of eXplainability is robust on several scenarios and it can be eventually
exploited for a lawful Impact Assessment of an Automated Decision Making
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05339">
<div class="article-summary-box-inner">
<span><p>The Linac Coherent Light Source (LCLS) is an X- ray free electron laser
(XFEL) facility enabling the study of the structure and dynamics of single
macromolecules. A major upgrade will bring the repetition rate of the X-ray
source from 120 to 1 million pulses per second. Exascale high performance
computing (HPC) capabilities will be required to process the corresponding data
rates. We present SpiniFEL, an application used for structure determination of
proteins from single-particle imaging (SPI) experiments. An emerging technique
for imaging individual proteins and other large molecular complexes by
outrunning radiation damage, SPI breaks free from the need for crystallization
(which is difficult for some proteins) and allows for imaging molecular
dynamics at near ambient conditions. SpiniFEL is being developed to run on
supercomputers in near real-time while an experiment is taking place, so that
the feedback about the data can guide the data collection strategy. We describe
here how we reformulated the mathematical framework for parallelizable
implementation and accelerated the most compute intensive parts of the
application. We also describe the use of Pygion, a Python interface for the
Legion task-based programming model and compare to our existing MPI+GPU
implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05349">
<div class="article-summary-box-inner">
<span><p>Attention is all we need as long as we have enough data. Even so, it is
sometimes not easy to determine how much data is enough while the models are
becoming larger and larger. In this paper, we propose HYDRA heads, lightweight
pretrained linguistic self-attention heads to inject knowledge into transformer
models without pretraining them again. Our approach is a balanced paradigm
between leaving the models to learn unsupervised and forcing them to conform to
linguistic knowledge rigidly as suggested in previous studies. Our experiment
proves that the approach is not only the boost performance of the model but
also lightweight and architecture friendly. We empirically verify our framework
on benchmark datasets to show the contribution of linguistic knowledge to a
transformer model. This is a promising result for a new approach to
transferring knowledge from linguistic resources into transformer-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05357">
<div class="article-summary-box-inner">
<span><p>In this work, we study the problem of named entity recognition (NER) in a low
resource scenario, focusing on few-shot and zero-shot settings. Built upon
large-scale pre-trained language models, we propose a novel NER framework,
namely SpanNER, which learns from natural language supervision and enables the
identification of never-seen entity classes without using in-domain labeled
data. We perform extensive experiments on 5 benchmark datasets and evaluate the
proposed method in the few-shot learning, domain transfer and zero-shot
learning settings. The experimental results show that the proposed method can
bring 10%, 23% and 26% improvements in average over the best baselines in
few-shot learning, domain transfer and zero-shot learning settings
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05358">
<div class="article-summary-box-inner">
<span><p>Enthymemes are defined as arguments where a premise or conclusion is left
implicit. We tackle the task of generating the implicit premise in an
enthymeme, which requires not only an understanding of the stated conclusion
and premise but also additional inferences that could depend on commonsense
knowledge. The largest available dataset for enthymemes (Habernal et al., 2018)
consists of 1.7k samples, which is not large enough to train a neural text
generation model. To address this issue, we take advantage of a similar task
and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).
However, we show that simply using a state-of-the-art seq2seq model fine-tuned
on this data might not generate meaningful implicit premises associated with
the given enthymemes. We demonstrate that encoding discourse-aware commonsense
during fine-tuning improves the quality of the generated implicit premises and
outperforms all other baselines both in automatic and human evaluations on
three different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05361">
<div class="article-summary-box-inner">
<span><p>We introduce COMBO - a fully neural NLP system for accurate part-of-speech
tagging, morphological analysis, lemmatisation, and (enhanced) dependency
parsing. It predicts categorical morphosyntactic features whilst also exposes
their vector representations, extracted from hidden layers. COMBO is an easy to
install Python package with automatically downloadable pre-trained models for
over 40 languages. It maintains a balance between efficiency and quality. As it
is an end-to-end system and its modules are jointly trained, its training is
competitively fast. As its models are optimised for accuracy, they achieve
often better prediction quality than SOTA. The COMBO library is available at:
https://gitlab.clarin-pl.eu/syntactic-tools/combo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05362">
<div class="article-summary-box-inner">
<span><p>Extracting relations across large text spans has been relatively
underexplored in NLP, but it is particularly important for high-value domains
such as biomedicine, where obtaining high recall of the latest findings is
crucial for practical applications. Compared to conventional information
extraction confined to short text spans, document-level relation extraction
faces additional challenges in both inference and learning. Given longer text
spans, state-of-the-art neural architectures are less effective and
task-specific self-supervision such as distant supervision becomes very noisy.
In this paper, we propose decomposing document-level relation extraction into
relation detection and argument resolution, taking inspiration from Davidsonian
semantics. This enables us to incorporate explicit discourse modeling and
leverage modular self-supervision for each sub-problem, which is less
noise-prone and can be further refined end-to-end via variational EM. We
conduct a thorough evaluation in biomedical machine reading for precision
oncology, where cross-paragraph relation mentions are prevalent. Our method
outperforms prior state of the art, such as multi-scale learning and graph
neural networks, by over 20 absolute F1 points. The gain is particularly
pronounced among the most challenging relation instances whose arguments never
co-occur in a paragraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05388">
<div class="article-summary-box-inner">
<span><p>In order to preserve word-order information in a non-autoregressive setting,
transformer architectures tend to include positional knowledge, by (for
instance) adding positional encodings to token embeddings. Several
modifications have been proposed over the sinusoidal positional encodings used
in the original transformer architecture; these include, for instance,
separating position encodings and token embeddings, or directly modifying
attention weights based on the distance between word pairs. We first show that
surprisingly, while these modifications tend to improve monolingual language
models, none of them result in better multilingual language models. We then
answer why that is: Sinusoidal encodings were explicitly designed to facilitate
compositionality by allowing linear projections over arbitrary time steps.
Higher variances in multilingual training distributions requires higher
compression, in which case, compositionality becomes indispensable. Learned
absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal
embeddings in multilingual settings, but more complex positional encoding
architectures lack the inductive bias to effectively learn compositionality and
cross-lingual alignment. In other words, while sinusoidal positional encodings
were originally designed for monolingual applications, they are particularly
useful in multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05395">
<div class="article-summary-box-inner">
<span><p>Single-table text-to-SQL aims to transform a natural language question into a
SQL query according to one single table. Recent work has made promising
progress on this task by pre-trained language models and a multi-submodule
framework. However, zero-shot table, that is, the invisible table in the
training set, is currently the most critical bottleneck restricting the
application of existing approaches to real-world scenarios. Although some work
has utilized auxiliary tasks to help handle zero-shot tables, expensive extra
manual annotation limits their practicality. In this paper, we propose a new
approach for the zero-shot text-to-SQL task which does not rely on any
additional manual annotations. Our approach consists of two parts. First, we
propose a new model that leverages the abundant information of table content to
help establish the mapping between questions and zero-shot tables. Further, we
propose a simple but efficient meta-learning strategy to train our model. The
strategy utilizes the two-step gradient update to force the model to learn a
generalization ability towards zero-shot tables. We conduct extensive
experiments on a public open-domain text-to-SQL dataset WikiSQL and a
domain-specific dataset ESQL. Compared to existing approaches using the same
pre-trained model, our approach achieves significant improvements on both
datasets. Compared to the larger pre-trained model and the tabular-specific
pre-trained model, our approach is still competitive. More importantly, on the
zero-shot subsets of both the datasets, our approach further increases the
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05406">
<div class="article-summary-box-inner">
<span><p>Human conversations consist of reasonable and natural topic flows, which are
observed as the shifts of the mentioned concepts across utterances. Previous
chatbots that incorporate the external commonsense knowledge graph prove that
modeling the concept shifts can effectively alleviate the dull and
uninformative response dilemma. However, there still exists a gap between the
concept relations in the natural conversation and those in the external
commonsense knowledge graph, which is an issue to solve. Specifically, the
concept relations in the external commonsense knowledge graph are not
intuitively built from the conversational scenario but the world knowledge,
which makes them insufficient for the chatbot construction. To bridge the above
gap, we propose the method to supply more concept relations extracted from the
conversational corpora and reconstruct an enhanced concept graph for the
chatbot construction. In addition, we present a novel, powerful, and fast graph
encoding architecture named the Edge-Transformer to replace the traditional GNN
architecture. Experimental results on the Reddit conversation dataset indicate
our proposed method significantly outperforms strong baseline systems and
achieves new SOTA results. Further analysis individually proves the
effectiveness of the enhanced concept graph and the Edge-Transformer
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05424">
<div class="article-summary-box-inner">
<span><p>Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05427">
<div class="article-summary-box-inner">
<span><p>Fine-grained classification involves dealing with datasets with larger number
of classes with subtle differences between them. Guiding the model to focus on
differentiating dimensions between these commonly confusable classes is key to
improving performance on fine-grained tasks. In this work, we analyse the
contrastive fine-tuning of pre-trained language models on two fine-grained text
classification tasks, emotion classification and sentiment analysis. We
adaptively embed class relationships into a contrastive objective function to
help differently weigh the positives and negatives, and in particular,
weighting closely confusable negatives more than less similar negative
examples. We find that Label-aware Contrastive Loss outperforms previous
contrastive methods, in the presence of larger number and/or more confusable
classes, and helps models to produce output distributions that are more
differentiated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05433">
<div class="article-summary-box-inner">
<span><p>Internet search affects people's cognition of the world, so mitigating biases
in search results and learning fair models is imperative for social good. We
study a unique gender bias in image search in this work: the search images are
often gender-imbalanced for gender-neutral natural language queries. We
diagnose two typical image search models, the specialized model trained on
in-domain datasets and the generalized representation model pre-trained on
massive image and text data across the internet. Both models suffer from severe
gender bias. Therefore, we introduce two novel debiasing approaches: an
in-processing fair sampling method to address the gender imbalance issue for
training models, and a post-processing feature clipping method base on mutual
information to debias multimodal representations of pre-trained models.
Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods
significantly reduce the gender bias in image search models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Let Your Characters Tell Their Story": A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05438">
<div class="article-summary-box-inner">
<span><p>When reading a literary piece, readers often make inferences about various
characters' roles, personalities, relationships, intents, actions, etc. While
humans can readily draw upon their past experiences to build such a
character-centric view of the narrative, understanding characters in narratives
can be a challenging task for machines. To encourage research in this field of
character-centric narrative understanding, we present LiSCU -- a new dataset of
literary pieces and their summaries paired with descriptions of characters that
appear in them. We also introduce two new tasks on LiSCU: Character
Identification and Character Description Generation. Our experiments with
several pre-trained language models adapted for these tasks demonstrate that
there is a need for better models of narrative comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05460">
<div class="article-summary-box-inner">
<span><p>Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog data.In this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05463">
<div class="article-summary-box-inner">
<span><p>Post-hoc interpretation aims to explain a trained model and reveal how the
model arrives at a decision. Though research on post-hoc interpretations has
developed rapidly, one growing pain in this field is the difficulty in
evaluating interpretations. There are some crucial logic traps behind existing
evaluation methods, which are ignored by most works. In this opinion piece, we
summarize four kinds evaluation methods and point out the corresponding logic
traps behind them. We argue that we should be clear about these traps rather
than ignore them and draw conclusions assertively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05477">
<div class="article-summary-box-inner">
<span><p>The ability of a dialog system to express consistent language style during
conversations has a direct, positive impact on its usability and on user
satisfaction. Although previous studies have demonstrated that style transfer
is feasible with a large amount of parallel data, it is often impossible to
collect such data for different styles. In this paper, instead of manually
constructing conversation data with a certain style, we propose a flexible
framework that adapts a generic retrieval-based dialogue system to mimic the
language style of a specified persona without any parallel data. Our approach
is based on automatic generation of stylized data by learning the usage of
jargon, and then rewriting the generic conversations to a stylized one by
incorporating the jargon. In experiments we implemented dialogue systems with
five distinct language styles, and the result shows our framework significantly
outperforms baselines in terms of the average score of responses' relevance and
style degree, and content diversity. A/B testing on a commercial chatbot shows
that users are more satisfied with our system. This study demonstrates the
feasibility of building stylistic dialogue systems by simple data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05487">
<div class="article-summary-box-inner">
<span><p>Although pre-training models have achieved great success in dialogue
generation, their performance drops dramatically when the input contains an
entity that does not appear in pre-training and fine-tuning datasets (unseen
entity). To address this issue, existing methods leverage an external knowledge
base to generate appropriate responses. In real-world scenario, the entity may
not be included by the knowledge base or suffer from the precision of knowledge
retrieval. To deal with this problem, instead of introducing knowledge base as
the input, we force the model to learn a better semantic representation by
predicting the information in the knowledge base, only based on the input
context. Specifically, with the help of a knowledge base, we introduce two
auxiliary training objectives: 1) Interpret Masked Word, which conjectures the
meaning of the masked entity given the context; 2) Hypernym Generation, which
predicts the hypernym of the entity based on the context. Experiment results on
two dialogue corpus verify the effectiveness of our methods under both
knowledge available and unavailable settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05494">
<div class="article-summary-box-inner">
<span><p>Building an automatic speech recognition (ASR) system from scratch requires a
large amount of annotated speech data, which is difficult to collect in many
languages. However, there are cases where the low-resource language shares a
common acoustic space with a high-resource language having enough annotated
data to build an ASR. In such cases, we show that the domain-independent
acoustic models learned from the high-resource language through unsupervised
domain adaptation (UDA) schemes can enhance the performance of the ASR in the
low-resource language. We use the specific example of Hindi in the source
domain and Sanskrit in the target domain. We explore two architectures: i)
domain adversarial training using gradient reversal layer (GRL) and ii) domain
separation networks (DSN). The GRL and DSN architectures give absolute
improvements of 6.71% and 7.32%, respectively, in word error rate over the
baseline deep neural network model when trained on just 5.5 hours of data in
the target domain. We also show that choosing a proper language (Telugu) in the
source domain can bring further improvement. The results suggest that UDA
schemes can be helpful in the development of ASR systems for low-resource
languages, mitigating the hassle of collecting large amounts of annotated
speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05522">
<div class="article-summary-box-inner">
<span><p>Multimodal language analysis is a burgeoning field of NLP that aims to
simultaneously model a speaker's words, acoustical annotations, and facial
expressions. In this area, lexicon features usually outperform other modalities
because they are pre-trained on large corpora via Transformer-based models.
Despite their strong performance, training a new self-supervised learning (SSL)
Transformer on any modality is not usually attainable due to insufficient data,
which is the case in multimodal language learning. This work proposes a
Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the
mentioned constraints without training a complete Transformer model. TEASEL
model includes speech modality as a dynamic prefix besides the textual modality
compared to a conventional language model. This method exploits a conventional
pre-trained language model as a cross-modal Transformer model. We evaluated
TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.
Extensive experiments show that our model outperforms unimodal baseline
language models by 4% and outperforms the current multimodal state-of-the-art
(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%
smaller than the SoTA model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05523">
<div class="article-summary-box-inner">
<span><p>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05527">
<div class="article-summary-box-inner">
<span><p>Detecting events and their evolution through time is a crucial task in
natural language understanding. Recent neural approaches to event temporal
relation extraction typically map events to embeddings in the Euclidean space
and train a classifier to detect temporal relations between event pairs.
However, embeddings in the Euclidean space cannot capture richer asymmetric
relations such as event temporal relations. We thus propose to embed events
into hyperbolic spaces, which are intrinsically oriented at modeling
hierarchical structures. We introduce two approaches to encode events and their
temporal relations in hyperbolic spaces. One approach leverages hyperbolic
embeddings to directly infer event relations through simple geometrical
operations. In the second one, we devise an end-to-end architecture composed of
hyperbolic neural units tailored for the temporal relation extraction task.
Thorough experimental assessments on widely used datasets have shown the
benefits of revisiting the tasks on a different geometrical space, resulting in
state-of-the-art performance on several standard metrics. Finally, the ablation
study and several qualitative analyses highlighted the rich event semantics
implicitly encoded into hyperbolic spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05602">
<div class="article-summary-box-inner">
<span><p>This paper asks whether extrapolating the hidden space distribution of text
examples from one class onto another is a valid inductive bias for data
augmentation. To operationalize this question, I propose a simple data
augmentation protocol called "good-enough example extrapolation" (GE3). GE3 is
lightweight and has no hyperparameters. Applied to three text classification
datasets for various data imbalance scenarios, GE3 improves performance more
than upsampling and other hidden-space data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05611">
<div class="article-summary-box-inner">
<span><p>We propose a novel scheme to use the Levenshtein Transformer to perform the
task of word-level quality estimation. A Levenshtein Transformer is a natural
fit for this task: trained to perform decoding in an iterative manner, a
Levenshtein Transformer can learn to post-edit without explicit supervision. To
further minimize the mismatch between the translation task and the word-level
QE task, we propose a two-stage transfer learning procedure on both augmented
data and human post-editing data. We also propose heuristics to construct
reference labels that are compatible with subword-level finetuning and
inference. Results on WMT 2020 QE shared task dataset show that our proposed
method has superior data efficiency under the data-constrained setting and
competitive performance under the unconstrained setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05620">
<div class="article-summary-box-inner">
<span><p>To audit the robustness of named entity recognition (NER) models, we propose
RockNER, a simple yet effective method to create natural adversarial examples.
Specifically, at the entity level, we replace target entities with other
entities of the same semantic class in Wikidata; at the context level, we use
pre-trained language models (e.g., BERT) to generate word substitutions.
Together, the two levels of attack produce natural adversarial examples that
result in a shifted distribution from the training data on which our target
models have been trained. We apply the proposed method to the OntoNotes dataset
and create a new benchmark named OntoRock for evaluating the robustness of
existing NER models via a systematic evaluation protocol. Our experiments and
analysis reveal that even the best model has a significant performance drop,
and these models seem to memorize in-domain entity patterns instead of
reasoning from the context. Our work also studies the effects of a few simple
data augmentation methods to improve the robustness of NER models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03760">
<div class="article-summary-box-inner">
<span><p>Disentanglement is a problem in which multiple conversations occur in the
same channel simultaneously, and the listener should decide which utterance is
part of the conversation he will respond to. We propose a new model, named
Dialogue BERT (DialBERT), which integrates local and global semantics in a
single stream of messages to disentangle the conversations that mixed together.
We employ BERT to capture the matching information in each utterance pair at
the utterance-level, and use a BiLSTM to aggregate and incorporate the
context-level information. With only a 3% increase in parameters, a 12%
improvement has been attained in comparison to BERT, based on the F1-Score. The
model achieves a state-of-the-art result on the a new dataset proposed by IBM
and surpasses previous work by a substantial margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
<div class="article-summary-box-inner">
<span><p>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Ad-dressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus con-firming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12289">
<div class="article-summary-box-inner">
<span><p>We present and make available MedLatinEpi and MedLatinLit, two datasets of
medieval Latin texts to be used in research on computational authorship
analysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,
respectively, labelled by author; MedLatinEpi texts are of epistolary nature,
while MedLatinLit texts consist of literary comments and treatises about
various subjects. As such, these two datasets lend themselves to supporting
research in authorship analysis tasks, such as authorship attribution,
authorship verification, or same-author verification. Along with the datasets
we provide experimental results, obtained on these datasets, for the authorship
verification task, i.e., the task of predicting whether a text of unknown
authorship was written by a candidate author or not. We also make available the
source code of the authorship verification system we have used, thus allowing
our experiments to be reproduced, and to be used as baselines, by other
researchers. We also describe the application of the above authorship
verification system, using these datasets as training data, for investigating
the authorship of two medieval epistles whose authorship has been disputed by
scholars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00049">
<div class="article-summary-box-inner">
<span><p>Language representations are known to carry stereotypical biases and, as a
result, lead to biased predictions in downstream tasks. While existing methods
are effective at mitigating biases by linear projection, such methods are too
aggressive: they not only remove bias, but also erase valuable information from
word embeddings. We develop new measures for evaluating specific information
retention that demonstrate the tradeoff between bias removal and information
retention. To address this challenge, we propose OSCaR (Orthogonal Subspace
Correction and Rectification), a bias-mitigating method that focuses on
disentangling biased associations between concepts instead of removing concepts
wholesale. Our experiments on gender biases show that OSCaR is a well-balanced
approach that ensures that semantic information is retained in the embeddings
and bias is also effectively mitigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06206">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained models have achieved impressive performance on
standard natural language processing benchmarks. However, the resultant model
generalizability remains poorly understood. We do not know, for example, how
excellent performance can lead to the perfection of generalization models. In
this study, we analyze a fine-tuned BERT model from different perspectives
using relation extraction. We also characterize the differences in
generalization techniques according to our proposed improvements. From
empirical experimentation, we find that BERT suffers a bottleneck in terms of
robustness by way of randomizations, adversarial and counterfactual tests, and
biases (i.e., selection and semantic). These findings highlight opportunities
for future improvements. Our open-sourced testbed DiagnoseRE is available in
\url{https://github.com/zjunlp/DiagnoseRE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08366">
<div class="article-summary-box-inner">
<span><p>Pre-trained models for programming language have achieved dramatic empirical
improvements on a variety of code-related tasks such as code search, code
completion, code summarization, etc. However, existing pre-trained models
regard a code snippet as a sequence of tokens, while ignoring the inherent
structure of code, which provides crucial code semantics and would enhance the
code understanding process. We present GraphCodeBERT, a pre-trained model for
programming language that considers the inherent structure of code. Instead of
taking syntactic-level structure of code like abstract syntax tree (AST), we
use data flow in the pre-training stage, which is a semantic-level structure of
code that encodes the relation of "where-the-value-comes-from" between
variables. Such a semantic-level structure is neat and does not bring an
unnecessarily deep hierarchy of AST, the property of which makes the model more
efficient. We develop GraphCodeBERT based on Transformer. In addition to using
the task of masked language modeling, we introduce two structure-aware
pre-training tasks. One is to predict code structure edges, and the other is to
align representations between source code and code structure. We implement the
model in an efficient way with a graph-guided masked attention function to
incorporate the code structure. We evaluate our model on four tasks, including
code search, clone detection, code translation, and code refinement. Results
show that code structure and newly introduced pre-training tasks can improve
GraphCodeBERT and achieves state-of-the-art performance on the four downstream
tasks. We further show that the model prefers structure-level attentions over
token-level attentions in the task of code search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03084">
<div class="article-summary-box-inner">
<span><p>Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11119">
<div class="article-summary-box-inner">
<span><p>Recent neural text-to-SQL models can effectively translate natural language
questions to corresponding SQL queries on unseen databases. Working mostly on
the Spider dataset, researchers have proposed increasingly sophisticated
solutions to the problem. Contrary to this trend, in this paper we focus on
simplifications. We begin by building DuoRAT, a re-implementation of the
state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware
or vanilla transformers as the building blocks. We perform several ablation
experiments using DuoRAT as the baseline model. Our experiments confirm the
usefulness of some techniques and point out the redundancy of others, including
structural SQL features and features that link the question with the schema.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12885">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation has benefited extensively from recent progress in the
designing of training objectives and model architectures. However, previous
explorations have largely focused on supervised methods, which require a large
amount of labeled data that is costly to collect. To address this drawback, we
adopt a transfer learning approach and propose a training pipeline that enables
pre-trained language models to generate high-quality paraphrases in an
unsupervised setting. Our recipe consists of task-adaptation, self-supervision,
and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a
surface form dissimilar from the input, whenever the language model emits a
token contained in the source sequence, DB prevents the model from outputting
the subsequent source token for the next generation step. We show with
automatic and human evaluations that our approach achieves state-of-the-art
performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and
is robust to domain shift between the two datasets of distinct distributions.
We also demonstrate that our model transfers to paraphrasing in other languages
without any additional finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11715">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) systems are increasingly
popular due to their relative architectural simplicity and competitive
performance. However, even though the average accuracy of these systems may be
high, the performance on rare content words often lags behind hybrid ASR
systems. To address this problem, second-pass rescoring is often applied
leveraging upon language modeling. In this paper, we propose a second-pass
system with multi-task learning, utilizing semantic targets (such as intent and
slot prediction) to improve speech recognition performance. We show that our
rescoring model trained with these additional tasks outperforms the baseline
rescoring model, trained with only the language modeling task, by 1.4% on a
general test and by 2.6% on a rare word test set in terms of word-error-rate
relative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR
deduction compared with RNN Transducer only ASR baseline for rare words
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02339">
<div class="article-summary-box-inner">
<span><p>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing the number of unique tokens) is a key factor for improved
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05776">
<div class="article-summary-box-inner">
<span><p>The effectiveness of a language model is influenced by its token
representations, which must encode contextual information and handle the same
word form having a plurality of meanings (polysemy). Currently, none of the
common language modelling architectures explicitly model polysemy. We propose a
language model which not only predicts the next word, but also its sense in
context. We argue that this higher prediction granularity may be useful for end
tasks such as assistive writing, and allow for more a precise linking of
language models with knowledge bases. We find that multi-sense language
modelling requires architectures that go beyond standard language models, and
here propose a structured prediction framework that decomposes the task into a
word followed by a sense prediction task. To aid sense prediction, we utilise a
Graph Attention Network, which encodes definitions and example uses of word
senses. Overall, we find that multi-sense language modelling is a highly
challenging task, and suggest that future work focus on the creation of more
annotated training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14541">
<div class="article-summary-box-inner">
<span><p>Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
https://github.com/IBM/yaso-tsa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15283">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PTLMs) have achieved noticeable success on
many NLP tasks, they still struggle for tasks that require event temporal
reasoning, which is essential for event-centric applications. We present a
continual pre-training approach that equips PTLMs with targeted knowledge about
event temporal relations. We design self-supervised learning objectives to
recover masked-out event and temporal indicators and to discriminate sentences
from their corrupted counterparts (where event or temporal indicators got
replaced). By further pre-training a PTLM with these objectives jointly, we
reinforce its attention to event and temporal information, yielding enhanced
capability on event temporal reasoning. This effective continual pre-training
framework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning
performances across five relation extraction and question answering tasks and
achieves new or on-par state-of-the-art performances in most of our downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% baseline is based on BERT training on the corrected dataset, which is
hard to surpass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11972">
<div class="article-summary-box-inner">
<span><p>The research community has proposed copious modifications to the Transformer
architecture since it was introduced over three years ago, relatively few of
which have seen widespread adoption. In this paper, we comprehensively evaluate
many of these modifications in a shared experimental setting that covers most
of the common uses of the Transformer in natural language processing.
Surprisingly, we find that most modifications do not meaningfully improve
performance. Furthermore, most of the Transformer variants we found beneficial
were either developed in the same codebase that we used or are relatively minor
changes. We conjecture that performance improvements may strongly depend on
implementation details and correspondingly make some recommendations for
improving the generality of experimental results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14583">
<div class="article-summary-box-inner">
<span><p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for
automatic speech recognition (ASR). Yet many endangered languages lack
sufficient data for pre-training such models, or are predominantly oral
vernaculars without a standardised writing system, precluding fine-tuning.
Query-by-example spoken term detection (QbE-STD) offers an alternative for
iteratively indexing untranscribed speech corpora by locating spoken query
terms. Using data from 7 Australian Aboriginal languages and a regional variety
of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can
be improved by leveraging representations developed for ASR (wav2vec 2.0: the
English monolingual model and XLSR53 multilingual model). Surprisingly, the
English model outperformed the multilingual model on 4 Australian language
datasets, raising questions around how to optimally leverage self-supervised
speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0
representations (either English or XLSR53) offer large improvements (56-86%
relative) over state-of-the-art approaches on our endangered language datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01371">
<div class="article-summary-box-inner">
<span><p>Recent advances in text autoencoders have significantly improved the quality
of the latent space, which enables models to generate grammatical and
consistent text from aggregated latent vectors. As a successful application of
this property, unsupervised opinion summarization models generate a summary by
decoding the aggregated latent vectors of inputs. More specifically, they
perform the aggregation via simple average. However, little is known about how
the vector aggregation step affects the generation quality. In this study, we
revisit the commonly used simple average approach by examining the latent space
and generated summaries. We found that text autoencoders tend to generate
overly generic summaries from simply averaged latent vectors due to an
unexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we
refer to as summary vector degeneration. To overcome this issue, we develop a
framework Coop, which searches input combinations for the latent vector
aggregation using input-output word overlap. Experimental results show that
Coop successfully alleviates the summary vector degeneration issue and
establishes new state-of-the-art performance on two opinion summarization
benchmarks. Code is available at \url{https://github.com/megagonlabs/coop}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01477">
<div class="article-summary-box-inner">
<span><p>Several studies have been carried out on revealing linguistic features
captured by BERT. This is usually achieved by training a diagnostic classifier
on the representations obtained from different layers of BERT. The subsequent
classification accuracy is then interpreted as the ability of the model in
encoding the corresponding linguistic property. Despite providing insights,
these studies have left out the potential role of token representations. In
this paper, we provide a more in-depth analysis on the representation space of
BERT in search for distinct and meaningful subspaces that can explain the
reasons behind these probing results. Based on a set of probing tasks and with
the help of attribution methods we show that BERT tends to encode meaningful
knowledge in specific token representations (which are often ignored in
standard classification setups), allowing the model to detect syntactic and
semantic abnormalities, and to distinctively separate grammatical number and
tense subspaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03416">
<div class="article-summary-box-inner">
<span><p>We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06239">
<div class="article-summary-box-inner">
<span><p>Discontinuous constituent parsers have always lagged behind continuous
approaches in terms of accuracy and speed, as the presence of constituents with
discontinuous yield introduces extra complexity to the task. However, a
discontinuous tree can be converted into a continuous variant by reordering
tokens. Based on that, we propose to reduce discontinuous parsing to a
continuous problem, which can then be directly solved by any off-the-shelf
continuous parser. To that end, we develop a Pointer Network capable of
accurately generating the continuous token arrangement for a given input
sentence and define a bijective function to recover the original order.
Experiments on the main benchmarks with two continuous parsers prove that our
approach is on par in accuracy with purely discontinuous state-of-the-art
algorithms, but considerably faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06901">
<div class="article-summary-box-inner">
<span><p>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based
on propositional logic, which has demonstrated competitive performance in many
Natural Language Processing (NLP) tasks, including sentiment analysis, text
classification, and Word Sense Disambiguation. To obtain human-level
interpretability, legacy TM employs Boolean input features such as bag-of-words
(BOW). However, the BOW representation makes it difficult to use any
pre-trained information, for instance, word2vec and GloVe word representations.
This restriction has constrained the performance of TM compared to deep neural
networks (DNNs) in NLP. To reduce the performance gap, in this paper, we
propose a novel way of using pre-trained word representations for TM. The
approach significantly enhances the performance and interpretability of TM. We
achieve this by extracting semantically related words from pre-trained word
representations as input features to the TM. Our experiments show that the
accuracy of the proposed approach is significantly higher than the previous
BOW-based TM, reaching the level of DNN-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07155">
<div class="article-summary-box-inner">
<span><p>Representations from large pretrained models such as BERT encode a range of
features into monolithic vectors, affording strong predictive accuracy across a
multitude of downstream tasks. In this paper we explore whether it is possible
to learn disentangled representations by identifying existing subnetworks
within pretrained models that encode distinct, complementary aspect
representations. Concretely, we learn binary masks over transformer weights or
hidden units to uncover subsets of features that correlate with a specific
factor of variation; this eliminates the need to train a disentangled model
from scratch for a particular task. We evaluate this method with respect to its
ability to disentangle representations of sentiment from genre in movie
reviews, "toxicity" from dialect in Tweets, and syntax from semantics.
</p>
<p>By combining masking with magnitude pruning we find that we can identify
sparse subnetworks within BERT that strongly encode particular aspects (e.g.,
toxicity) while only weakly encoding others (e.g., race). Moreover, despite
only learning masks, we find that disentanglement-via-masking performs as well
as -- and often better than -- previously proposed methods based on variational
autoencoders and adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07467">
<div class="article-summary-box-inner">
<span><p>Stance detection concerns the classification of a writer's viewpoint towards
a target. There are different task variants, e.g., stance of a tweet vs. a full
article, or stance with respect to a claim vs. an (implicit) topic. Moreover,
task definitions vary, which includes the label inventory, the data collection,
and the annotation protocol. All these aspects hinder cross-domain studies, as
they require changes to standard domain adaptation approaches. In this paper,
we perform an in-depth analysis of 16 stance detection datasets, and we explore
the possibility for cross-domain learning from them. Moreover, we propose an
end-to-end unsupervised framework for out-of-domain prediction of unseen,
user-defined labels. In particular, we combine domain adaptation techniques
such as mixture of experts and domain-adversarial training with label
embeddings, and we demonstrate sizable performance gains over strong baselines,
both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for
unseen targets. Finally, we perform an exhaustive analysis of the cross-domain
results, and we highlight the important factors influencing the model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07571">
<div class="article-summary-box-inner">
<span><p>The goal of question answering (QA) is to answer any question. However, major
QA datasets have skewed distributions over gender, profession, and nationality.
Despite that skew, model accuracy analysis reveals little evidence that
accuracy is lower for people based on gender or nationality; instead, there is
more variation on professions (question topic). But QA's lack of representation
could itself hide evidence of bias, necessitating QA datasets that better
represent global diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07814">
<div class="article-summary-box-inner">
<span><p>Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
<div class="article-summary-box-inner">
<span><p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ. The code
is available at https://github.com/microsoft/MoPQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08154">
<div class="article-summary-box-inner">
<span><p>Developing a unified multilingual model has long been a pursuit for machine
translation. However, existing approaches suffer from performance degradation
-- a single multilingual model is inferior to separately trained bilingual ones
on rich-resource languages. We conjecture that such a phenomenon is due to
interference caused by joint training with multiple languages. To accommodate
the issue, we propose CIAT, an adapted Transformer model with a small parameter
overhead for multilingual machine translation. We evaluate CIAT on multiple
benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that
CIAT consistently outperforms strong multilingual baselines on 64 of total 66
language directions, 42 of which see above 0.5 BLEU improvement. Our code is
available at \url{https://github.com/Yaoming95/CIAT}~.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08350">
<div class="article-summary-box-inner">
<span><p>Understanding how events are semantically related to each other is the
essence of reading comprehension. Recent event-centric reading comprehension
datasets focus mostly on event arguments or temporal relations. While these
tasks partially evaluate machines' ability of narrative understanding,
human-like reading comprehension requires the capability to process event-based
information beyond arguments and temporal reasoning. For example, to understand
causality between events, we need to infer motivation or purpose; to establish
event hierarchy, we need to understand the composition of events. To facilitate
these tasks, we introduce ESTER, a comprehensive machine reading comprehension
(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages
natural language queries to reason about the five most common event semantic
relations, provides more than 6K questions and captures 10.1K event relation
pairs. Experimental results show that the current SOTA systems achieve 22.1%,
63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,
which are all significantly below human performances (36.0%, 79.6%, 100%
respectively), highlighting our dataset as a challenging benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08692">
<div class="article-summary-box-inner">
<span><p>Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive
monolingual texts, which has shown promising results on many cross-lingual
tasks. In this paper, we improve multilingual text-to-text transfer Transformer
with translation pairs (mT6). Specifically, we explore three cross-lingual
text-to-text pre-training tasks, namely, machine translation, translation pair
span corruption, and translation span corruption. In addition, we propose a
partially non-autoregressive objective for text-to-text pre-training. We
evaluate the methods on eight multilingual benchmark datasets, including
sentence classification, named entity recognition, question answering, and
abstractive summarization. Experimental results show that the proposed mT6
improves cross-lingual transferability over mT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08727">
<div class="article-summary-box-inner">
<span><p>While day-to-day questions come with a variety of answer types, the current
question-answering (QA) literature has failed to adequately address the answer
diversity of questions. To this end, we present GooAQ, a large-scale dataset
with a variety of answer types. This dataset contains over 5 million questions
and 3 million answers collected from Google. GooAQ questions are collected
semi-automatically from the Google search engine using its autocomplete
feature. This results in naturalistic questions of practical interest that are
nonetheless short and expressed using simple language. GooAQ answers are mined
from Google's responses to our collected questions, specifically from the
answer boxes in the search results. This yields a rich space of answer types,
containing both textual answers (short and long) as well as more structured
ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)
in line with recent work, LM's strong performance on GooAQ's short-answer
questions heavily benefit from annotated data; however, (b) their quality in
generating coherent and accurate responses for questions requiring long
responses (such as 'how' and 'why' questions) is less reliant on observing
annotated data and mainly supported by their pre-training. We release GooAQ to
facilitate further research on improving QA with diverse response types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLI Models Verify QA Systems' Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08731">
<div class="article-summary-box-inner">
<span><p>To build robust question answering systems, we need the ability to verify
whether answers to questions are truly correct, not just "good enough" in the
context of imperfect QA datasets. We explore the use of natural language
inference (NLI) as a way to achieve this goal, as NLI inherently requires the
premise (document context) to contain all necessary information to support the
hypothesis (proposed answer to the question). We leverage large pre-trained
models and recent prior datasets to construct powerful question converter and
decontextualization modules, which can reformulate QA instances as
premise-hypothesis pairs with very high reliability. Then, by combining
standard NLI datasets with NLI examples automatically derived from QA training
data, we can train NLI models to judge the correctness of QA models' proposed
answers. We show that our NLI approach can generally improve the confidence
estimation of a QA model across different domains, evaluated in a selective QA
setting. Careful manual analysis over the predictions of our NLI model shows
that it can further identify cases where the QA model produces the right answer
for the wrong reason, or where the answer cannot be verified as addressing all
aspects of the question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14830">
<div class="article-summary-box-inner">
<span><p>Building ASR models across many languages is a challenging multi-task
learning problem due to large variations and heavily unbalanced data. Existing
work has shown positive transfer from high resource to low resource languages.
However, degradations on high resource languages are commonly observed due to
interference from the heterogeneous multilingual data and reduction in
per-language capacity. We conduct a capacity study on a 15-language task, with
the amount of data per language varying from 7.6K to 53.5K hours. We adopt
GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that
(1) scaling the number of model parameters is an effective way to solve the
capacity bottleneck - our 500M-param model already outperforms monolingual
baselines and scaling it to 1B and 10B brought further quality gains; (2)
larger models are not only more data efficient, but also more efficient in
terms of training cost as measured in TPU days - the 1B-param model reaches the
same accuracy at 34% of training time as the 500M-param model; (3) given a
fixed capacity budget, adding depth works better than width and large encoders
do better than large decoders; (4) with continuous training, they can be
adapted to new languages and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02397">
<div class="article-summary-box-inner">
<span><p>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.
</p>
<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.
</p>
<p>We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06381">
<div class="article-summary-box-inner">
<span><p>The cross-lingual language models are typically pretrained with masked
language modeling on multilingual text or parallel sentences. In this paper, we
introduce denoising word alignment as a new cross-lingual pre-training task.
Specifically, the model first self-labels word alignments for parallel
sentences. Then we randomly mask tokens in a bitext pair. Given a masked token,
the model uses a pointer network to predict the aligned token in the other
language. We alternately perform the above two steps in an
expectation-maximization manner. Experimental results show that our method
improves cross-lingual transferability on various datasets, especially on the
token-level tasks, such as question answering, and structured prediction.
Moreover, the model can serve as a pretrained word aligner, which achieves
reasonably low error rates on the alignment benchmarks. The code and pretrained
parameters are available at https://github.com/CZWin32768/XLM-Align.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02401">
<div class="article-summary-box-inner">
<span><p>This paper introduces WeChat AI's participation in WMT 2021 shared news
translation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and
English-&gt;German. Our systems are based on the Transformer (Vaswani et al.,
2017) with several novel and effective variants. In our experiments, we employ
data filtering, large-scale synthetic data generation (i.e., back-translation,
knowledge distillation, forward-translation, iterative in-domain knowledge
transfer), advanced finetuning approaches, and boosted Self-BLEU based model
ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3
case-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese,
Japanese-&gt;English and English-&gt;German, respectively. The BLEU scores of
English-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among
all submissions, and that of English-&gt;German is the highest among all
constrained submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08676">
<div class="article-summary-box-inner">
<span><p>Existing system dealing with online complaint provides a final decision
without explanations. We propose to analyse the complaint text of internet
fraud in a fine-grained manner. Considering the complaint text includes
multiple clauses with various functions, we propose to identify the role of
each clause and classify them into different types of fraud element. We
construct a large labeled dataset originated from a real finance service
platform. We build an element identification model on top of BERT and propose
additional two modules to utilize the context of complaint text for better
element label classification, namely, global context encoder and label refiner.
Experimental results show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11830">
<div class="article-summary-box-inner">
<span><p>Dialogue models trained on human conversations inadvertently learn to
generate toxic responses. In addition to producing explicitly offensive
utterances, these models can also implicitly insult a group or individual by
aligning themselves with an offensive statement. To better understand the
dynamics of contextually offensive language, we investigate the stance of
dialogue model responses in offensive Reddit conversations. Specifically, we
create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model
responses labeled with offensive language and stance. Our analysis reveals that
42% of human responses agree with toxic comments, whereas only 13% agree with
safe comments. This undesirable behavior is learned by neural dialogue models,
such as DialoGPT, which we show are two times more likely to agree with
offensive comments. To enable automatic detection of offensive language, we
fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for
offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the
effectiveness of controllable text generation (CTG) methods to mitigate the
tendency of neural dialogue models to agree with offensive comments. Compared
to the baseline, our best CTG model achieves a 19% reduction in agreement with
offensive comments and produces 29% fewer offensive replies. Our work
highlights the need for further efforts to characterize and analyze
inappropriate behavior in dialogue models, in order to help make them safer.
Our code and corpus are available at https://github.com/abaheti95/ToxiChat .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work has claimed, our auxiliary experiments suggest that
relation prediction is contributory to named entity prediction in a
non-negligible way. The source code can be found at
https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00055">
<div class="article-summary-box-inner">
<span><p>Representation learning for text via pretraining a language model on a large
corpus has become a standard starting point for building NLP systems. This
approach stands in contrast to autoencoders, also trained on raw text, but with
the objective of learning to encode each input as a vector that allows full
reconstruction. Autoencoders are attractive because of their latent space
structure and generative properties. We therefore explore the construction of a
sentence-level autoencoder from a pretrained, frozen transformer language
model. We adapt the masked language modeling objective as a generative,
denoising one, while only training a sentence bottleneck and a single-layer
modified transformer decoder. We demonstrate that the sentence representations
discovered by our model achieve better quality than previous methods that
extract representations from pretrained transformers on text similarity tasks,
style transfer (an example of controlled generation), and single-sentence
classification tasks in the GLUE benchmark, while using fewer parameters than
large pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00544">
<div class="article-summary-box-inner">
<span><p>Adversarial training, a method for learning robust deep neural networks,
constructs adversarial examples during training. However, recent methods for
generating NLP adversarial examples involve combinatorial search and expensive
sentence encoders for constraining the generated instances. As a result, it
remains challenging to use vanilla adversarial training to improve NLP models'
performance, and the benefits are mainly uninvestigated. This paper proposes a
simple and improved vanilla adversarial training process for NLP models, which
we name Attacking to Training (A2T). The core part of A2T is a new and cheaper
word substitution attack optimized for vanilla adversarial training. We use A2T
to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI
datasets. Our results empirically show that it is possible to train robust NLP
models using a much cheaper adversary. We demonstrate that vanilla adversarial
training with A2T can improve an NLP model's robustness to the attack it was
originally trained with and also defend the model against other types of word
substitution attacks. Furthermore, we show that A2T can improve NLP models'
standard accuracy, cross-domain generalization, and interpretability. Code is
available at https://github.com/QData/Textattack-A2T .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
<div class="article-summary-box-inner">
<span><p>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02102">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates that by fine-tuning an autoregressive language model
(GPT-Neo) on appropriately structured step-by-step demonstrations, it is
possible to teach it to execute a mathematical task that has previously proved
difficult for Transformers - longhand modulo operations - with a relatively
small number of examples. Specifically, we fine-tune GPT-Neo to solve the
numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et
al. (<a href="/abs/1904.01557">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million
training examples. We show that after fine-tuning on 200 appropriately
structured demonstrations of solving long division problems and reporting the
remainders, the smallest available GPT-Neo model achieves over 80% accuracy.
This is achieved by constructing an appropriate dataset for fine-tuning, with
no changes to the learning algorithm. These results suggest that fine-tuning
autoregressive language models on small sets of well-crafted demonstrations may
be a useful paradigm for enabling individuals without training in machine
learning to coax such models to perform some kinds of complex multi-step tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04080">
<div class="article-summary-box-inner">
<span><p>With the rapid increase in the volume of dialogue data from daily life, there
is a growing demand for dialogue summarization. Unfortunately, training a large
summarization model is generally infeasible due to the inadequacy of dialogue
data with annotated summaries. Most existing works for low-resource dialogue
summarization directly pretrain models in other domains, e.g., the news domain,
but they generally neglect the huge difference between dialogues and
conventional articles. To bridge the gap between out-of-domain pretraining and
in-domain fine-tuning, in this work, we propose a multi-source pretraining
paradigm to better leverage the external summary data. Specifically, we exploit
large-scale in-domain non-summary data to separately pretrain the dialogue
encoder and the summary decoder. The combined encoder-decoder model is then
pretrained on the out-of-domain summary data using adversarial critics, aiming
to facilitate domain-agnostic summarization. The experimental results on two
public datasets show that with only limited training data, our approach
achieves competitive performance and generalizes well in different dialogue
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04152">
<div class="article-summary-box-inner">
<span><p>Text classification tasks have improved substantially during the last years
by the usage of transformers. However, the majority of researches focus on
prose texts, with poetry receiving less attention, specially for Spanish
language. In this paper, we propose a semi-supervised learning approach for
inferring 21 psychological categories evoked by a corpus of 4572 sonnets, along
with 10 affective and lexico-semantic multiclass ones. The subset of poems used
for training an evaluation includes 270 sonnets. With our approach, we achieve
an AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65
for 60% on the multiclass ones. The sonnets are modelled using transformers,
through sentence embeddings, along with lexico-semantic and affective features,
obtained by using external lexicons. Consequently, we see that this approach
provides an AUC increase of up to 0.12, as opposed to using transformers alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Different Amounts of Annotation: From Zero to Many Labels. (arXiv:2109.04408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04408">
<div class="article-summary-box-inner">
<span><p>Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
annotation distribution schemes, assigning multiple labels per example for a
small subset of training examples. Introducing such multi label examples at the
cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from training examples with different amount of annotation (with zero,
one, or multiple labels). This algorithm efficiently combines signals from
uneven training data and brings additional gains in low annotation budget and
cross domain settings. Together, our method achieves consistent gains in two
tasks, suggesting distributing labels unevenly among training examples can be
beneficial for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-14 23:08:51.409653586 UTC">2021-09-14 23:08:51 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>