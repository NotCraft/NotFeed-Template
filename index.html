<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-26T01:30:00Z">08-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10904">
<div class="article-summary-box-inner">
<span><p>With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Multisource Feature Fusion for the Text Clustering. (arXiv:2108.10926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10926">
<div class="article-summary-box-inner">
<span><p>The text clustering technique is an unsupervised text mining method which are
used to partition a huge amount of text documents into groups. It has been
reported that text clustering algorithms are hard to achieve better performance
than supervised methods and their clustering performance is highly dependent on
the picked text features. Currently, there are many different types of text
feature generation algorithms, each of which extracts text features from some
specific aspects, such as VSM and distributed word embedding, thus seeking a
new way of obtaining features as complete as possible from the corpus is the
key to enhance the clustering effects. In this paper, we present a hybrid
multisource feature fusion (HMFF) framework comprising three components,
feature representation of multimodel, mutual similarity matrices and feature
fusion, in which we construct mutual similarity matrices for each feature
source and fuse discriminative features from mutual similarity matrices by
reducing dimensionality to generate HMFF features, then k-means clustering
algorithm could be configured to partition input samples into groups. The
experimental tests show our HMFF framework outperforms other recently published
algorithms on 7 of 11 public benchmark datasets and has the leading performance
on the rest 4 benchmark datasets as well. At last, we compare HMFF framework
with those competitors on a COVID-19 dataset from the wild with the unknown
cluster count, which shows the clusters generated by HMFF framework partition
those similar samples much closer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?. (arXiv:2108.10931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10931">
<div class="article-summary-box-inner">
<span><p>We synthesize the reported results and recommendations of recent workshops
and seminars that convened to discuss open questions within the important
intersection of robotics, human-robot interaction, and spoken dialogue systems
research. The goal of this growing area of research interest is to enable
people to more effectively and naturally communicate with robots. To carry
forward opportunities networking and discussion towards concrete, potentially
fundable projects, we encourage interested parties to consider participating in
future virtual and in-person discussions and workshops.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SN Computer Science: Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10939">
<div class="article-summary-box-inner">
<span><p>Offensive Language detection in social media platforms has been an active
field of research over the past years. In non-native English spoken countries,
social media users mostly use a code-mixed form of text in their
posts/comments. This poses several challenges in the offensive content
identification tasks, and considering the low resources available for Tamil,
the task becomes much harder. The current study presents extensive experiments
using multiple deep learning, and transfer learning models to detect offensive
content on YouTube. We propose a novel and flexible approach of selective
translation and transliteration techniques to reap better results from
fine-tuning and ensembling multilingual transformer networks like BERT, Distil-
BERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best
model for this task. The best performing models were ULMFiT and mBERTBiLSTM for
this Tamil code-mix dataset instead of more popular transfer learning models
such as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The
proposed model ULMFiT and mBERTBiLSTM yielded good results and are promising
for effective offensive speech identification in low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Evaluation of Entity Disambiguation Using Prior Probes:the Case of Entity Overshadowing. (arXiv:2108.10949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10949">
<div class="article-summary-box-inner">
<span><p>Entity disambiguation (ED) is the last step of entity linking (EL), when
candidate entities are reranked according to the context they appear in. All
datasets for training and evaluating models for EL consist of convenience
samples, such as news articles and tweets, that propagate the prior probability
bias of the entity distribution towards more frequently occurring entities. It
was previously shown that the performance of the EL systems on such datasets is
overestimated since it is possible to obtain higher accuracy scores by merely
learning the prior. To provide a more adequate evaluation benchmark, we
introduce the ShadowLink dataset, which includes 16K short text snippets
annotated with entity mentions. We evaluate and report the performance of
popular EL systems on the ShadowLink benchmark. The results show a considerable
difference in accuracy between more and less common entities for all of the EL
systems under evaluation, demonstrating the effects of prior probability bias
and entity overshadowing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering. (arXiv:2108.10986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10986">
<div class="article-summary-box-inner">
<span><p>Discovering the logical sequence of events is one of the cornerstones in
Natural Language Understanding. One approach to learn the sequence of events is
to study the order of sentences in a coherent text. Sentence ordering can be
applied in various tasks such as retrieval-based Question Answering, document
summarization, storytelling, text generation, and dialogue systems.
Furthermore, we can learn to model text coherence by learning how to order a
set of shuffled sentences. Previous research has relied on RNN, LSTM, and
BiLSTM architecture for learning text language models. However, these networks
have performed poorly due to the lack of attention mechanisms. We propose an
algorithm for sentence ordering in a corpus of short stories. Our proposed
method uses a language model based on Universal Transformers (UT) that captures
sentences' dependencies by employing an attention mechanism. Our method
improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)
score in the ROCStories dataset, a corpus of nearly 100K short human-made
stories. The proposed model includes three components: Sentence Encoder,
Language Model, and Sentence Arrangement with Brute Force Search. The first
component generates sentence embeddings using SBERT-WK pre-trained model
fine-tuned on the ROCStories data. Then a Universal Transformer network
generates a sentence-level language model. For decoding, the network generates
a candidate sentence as the following sentence of the current sentence. We use
cosine similarity as a scoring function to assign scores to the candidate
embedding and the embeddings of other sentences in the shuffled set. Then a
Brute Force Search is employed to maximize the sum of similarities between
pairs of consecutive sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing Accurately Categorizes Indications, Findings and Pathology Reports from Multicenter Colonoscopy. (arXiv:2108.11034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11034">
<div class="article-summary-box-inner">
<span><p>Colonoscopy is used for colorectal cancer (CRC) screening. Extracting details
of the colonoscopy findings from free text in electronic health records (EHRs)
can be used to determine patient risk for CRC and colorectal screening
strategies. We developed and evaluated the accuracy of a deep learning model
framework to extract information for the clinical decision support system to
interpret relevant free-text reports, including indications, pathology, and
findings notes. The Bio-Bi-LSTM-CRF framework was developed using Bidirectional
Long Short-term Memory (Bi-LSTM) and Conditional Random Fields (CRF) to extract
several clinical features from these free-text reports including indications
for the colonoscopy, findings during the colonoscopy, and pathology of resected
material. We trained the Bio-Bi-LSTM-CRF and existing Bi-LSTM-CRF models on 80%
of 4,000 manually annotated notes from 3,867 patients. These clinical notes
were from a group of patients over 40 years of age enrolled in four Veterans
Affairs Medical Centers. A total of 10% of the remaining annotated notes were
used to train hyperparameter and the remaining 10% were used to evaluate the
accuracy of our model Bio-Bi-LSTM-CRF and compare to Bi-LSTM-CRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How COVID-19 has Impacted American Attitudes Toward China: A Study on Twitter. (arXiv:2108.11040v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11040">
<div class="article-summary-box-inner">
<span><p>Past research has studied social determinants of attitudes toward foreign
countries. Confounded by potential endogeneity biases due to unobserved factors
or reverse causality, the causal impact of these factors on public opinion is
usually difficult to establish. Using social media data, we leverage the
suddenness of the COVID-19 pandemic to examine whether a major global event has
causally changed American views of another country. We collate a database of
more than 297 million posts on the social media platform Twitter about China or
COVID-19 up to June 2020, and we treat tweeting about COVID-19 as a proxy for
individual awareness of COVID-19. Using regression discontinuity and
difference-in-difference estimation, we find that awareness of COVID-19 causes
a sharp rise in anti-China attitudes. Our work has implications for
understanding how self-interest affects policy preference and how Americans
view migrant communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Viola: A Topic Agnostic Generate-and-Rank Dialogue System. (arXiv:2108.11063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11063">
<div class="article-summary-box-inner">
<span><p>We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11126">
<div class="article-summary-box-inner">
<span><p>In this paper we present our open-source neural machine translation (NMT)
toolkit called "Yet Another Neural Machine Translation Toolkit" abbreviated as
YANMTT which is built on top of the Transformers library. Despite the growing
importance of sequence to sequence pre-training there surprisingly few, if not
none, well established toolkits that allow users to easily do pre-training.
Toolkits such as Fairseq which do allow pre-training, have very large codebases
and thus they are not beginner friendly. With regards to transfer learning via
fine-tuning most toolkits do not explicitly allow the user to have control over
what parts of the pre-trained models can be transferred. YANMTT aims to address
these issues via the minimum amount of code to pre-train large scale NMT
models, selectively transfer pre-trained parameters and fine-tune them, perform
translation as well as extract representations and attentions for visualization
and analyses. Apart from these core features our toolkit also provides other
advanced functionalities such as but not limited to document/multi-source NMT,
simultaneous NMT and model compression via distillation which we believe are
relevant to the purpose behind our toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11193">
<div class="article-summary-box-inner">
<span><p>Standard pretrained language models operate on sequences of subword tokens
without direct access to the characters that compose each token's string
representation. We probe the embedding layer of pretrained language models and
show that models learn the internal character composition of whole word and
subword tokens to a surprising extent, without ever seeing the characters
coupled with the tokens. Our results show that the embedding layer of RoBERTa
holds enough information to accurately spell up to a third of the vocabulary
and reach high average character ngram overlap on all token types. We further
test whether enriching subword models with additional character information can
improve language modeling, and observe that this method has a near-identical
learning curve as training without spelling-based enrichment. Overall, our
results suggest that language modeling objectives incentivize the model to
implicitly learn some notion of spelling, and that explicitly teaching the
model how to spell does not enhance its performance on such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Promises of Transformer-Based LMs for the Representation of Normative Claims in the Legal Domain. (arXiv:2108.11215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11215">
<div class="article-summary-box-inner">
<span><p>In this article, we explore the potential of transformer-based language
models (LMs) to correctly represent normative statements in the legal domain,
taking tax law as our use case. In our experiment, we use a variety of LMs as
bases for both word- and sentence-based clusterers that are then evaluated on a
small, expert-compiled test-set, consisting of real-world samples from tax law
research literature that can be clearly assigned to one of four normative
theories. The results of the experiment show that clusterers based on
sentence-BERT-embeddings deliver the most promising results. Based on this main
experiment, we make first attempts at using the best performing models in a
bootstrapping loop to build classifiers that map normative claims on one of
these four normative theories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-Enhanced Slot Filling. (arXiv:2108.11275v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11275">
<div class="article-summary-box-inner">
<span><p>Slot filling is a fundamental task in dialog state tracking in task-oriented
dialog systems. In multi-domain task-oriented dialog system, user utterances
and system responses may mention multiple named entities and attributes values.
A system needs to select those that are confirmed by the user and fill them
into destined slots. One difficulty is that since a dialogue session contains
multiple system-user turns, feeding in all the tokens into a deep model such as
BERT can be challenging due to limited capacity of input word tokens and GPU
memory. In this paper, we investigate an ontology-enhanced approach by matching
the named entities occurred in all dialogue turns using ontology. The matched
entities in the previous dialogue turns will be accumulated and encoded as
additional inputs to a BERT-based dialogue state tracker. In addition, our
improvement includes ontology constraint checking and the correction of slot
name tokenization. Experimental results showed that our ontology-enhanced
dialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%
(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11357">
<div class="article-summary-box-inner">
<span><p>We propose ProoFVer, a proof system for fact verification using natural
logic. The textual entailment model in ProoFVer is a seq2seq model generating
valid natural-logic based logical inferences as its proofs. The generation of
proofs makes ProoFVer an explainable system. The proof consists of iterative
lexical mutations of spans in the claim with spans in a set of retrieved
evidence sentences. Further, each such mutation is marked with an entailment
relation using natural logic operators. The veracity of a claim is determined
solely based on the sequence of natural logic relations present in the proof.
By design, this makes ProoFVer a faithful by construction system that generates
faithful explanations. ProoFVer outperforms existing fact-verification models,
with more than two percent absolute improvements in performance and robustness.
In addition to its explanations being faithful, ProoFVer also scores high on
rationale extraction, with a five point absolute improvement compared to
attention-based rationales in existing models. Finally, we find that humans
correctly simulate ProoFVer's decisions more often using the proofs, than the
decisions of an existing model that directly use the retrieved evidence for
decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00575">
<div class="article-summary-box-inner">
<span><p>In this survey, we provide a comprehensive description of recent neural
entity linking (EL) systems developed since 2015 as a result of the "deep
learning revolution" in NLP. Our goal is to systemize design features of neural
entity linking systems and compare their performance to the prominent classic
methods on common benchmarks. We distill generic architectural components of a
neural EL system, like candidate generation and entity ranking, and summarize
prominent methods for each of them. The vast variety of modifications of this
general neural entity linking architecture are grouped by several common
themes: joint entity recognition and linking, models for global linking,
domain-independent techniques including zero-shot and distant supervision
methods, and cross-lingual approaches. Since many neural models take advantage
of entity and mention/context embeddings to catch semantic meaning of them, we
provide an overview of popular embedding techniques. Finally, we briefly
discuss applications of entity linking, focusing on the recently emerged
use-case of enhancing deep pre-trained masked language models based on the
transformer architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
<div class="article-summary-box-inner">
<span><p>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04980">
<div class="article-summary-box-inner">
<span><p>Most existing image retrieval systems use text queries as a way for the user
to express what they are looking for. However, fine-grained image retrieval
often requires the ability to also express where in the image the content they
are looking for is. The text modality can only cumbersomely express such
localization preferences, whereas pointing is a more natural fit. In this
paper, we propose an image retrieval setup with a new form of multimodal
queries, where the user simultaneously uses both spoken natural language (the
what) and mouse traces over an empty canvas (the where) to express the
characteristics of the desired target image. We then describe simple
modifications to an existing image retrieval model, enabling it to operate in
this setup. Qualitative and quantitative experiments show that our model
effectively takes this spatial guidance into account, and provides
significantly more accurate retrieval results compared to text-only equivalent
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04167">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a
remote location on the basis of natural-language instructions and a set of
photo-realistic panoramas. Most existing methods take the words in the
instructions and the discrete views of each panorama as the minimal unit of
encoding. However, this requires a model to match different nouns (e.g., TV,
table) against the same input view feature. In this work, we propose an
object-informed sequential BERT to encode visual perceptions and linguistic
instructions at the same fine-grained level, namely objects and words. Our
sequential BERT also enables the visual-textual clues to be interpreted in
light of the temporal context, which is crucial to multi-round VLN tasks.
Additionally, we enable the model to identify the relative direction (e.g.,
left/right/front/back) of each navigable location and the room type (e.g.,
bedroom, kitchen) of its current and final navigation goal, as such information
is widely mentioned in instructions implying the desired next and final
locations. We thus enable the model to know-where the objects lie in the
images, and to know-where they stand in the scene. Extensive experiments
demonstrate the effectiveness compared against several state-of-the-art methods
on three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:
https://github.com/YuankaiQi/ORIST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results on some few-class
classification tasks. The core idea of prompt-tuning is to insert text pieces,
i.e., template, to the input and transform a classification task into a masked
language modeling problem. However, as for relation extraction, determining the
appropriate prompt template requires domain expertise, and single label word
handcrafted or auto-searched is cumbersome and time-consuming to verify their
effectiveness in non-few-shot scenarios, which also fails to leverage the
abundant semantic knowledge in the entities and relation labels. To this end,
we focus on incorporating knowledge into prompt-tuning for relation extraction
and propose a knowledge-aware prompt-tuning with synergistic optimization
(KNIGHT) approach. Specifically, we inject entity and relation knowledge into
prompt construction with learnable virtual template words and answer words and
jointly optimize their representation with knowledge constraints. Extensive
experimental results on 5 datasets with standard and low-resource settings
demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Annotated Commodity News Corpus for Event Extraction. (arXiv:2105.08214v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08214">
<div class="article-summary-box-inner">
<span><p>Commodity News contains a wealth of information such as sum-mary of the
recent commodity price movement and notable events that led tothe movement.
Through event extraction, useful information extracted fromcommodity news is
extremely useful in mining for causal relation betweenevents and commodity
price movement, which can be used for commodity priceprediction. To facilitate
the future research, we introduce a new dataset withthe following information
identified and annotated: (i) entities (both nomi-nal and named), (ii) events
(trigger words and argument roles), (iii) eventmetadata: modality, polarity and
intensity and (iv) event-event relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09142">
<div class="article-summary-box-inner">
<span><p>The automatic detection of humor poses a grand challenge for natural language
processing. Transformer-based systems have recently achieved remarkable results
on this task, but they usually (1)~were evaluated in setups where serious vs
humorous texts came from entirely different sources, and (2)~focused on
benchmarking performance without providing insights into how the models work.
We make progress in both respects by training and analyzing transformer-based
humor recognition models on a recently introduced dataset consisting of minimal
pairs of aligned sentences, one serious, the other humorous. We find that,
although our aligned dataset is much harder than previous datasets,
transformer-based models recognize the humorous sentence in an aligned pair
with high accuracy (78%). In a careful error analysis, we characterize easy vs
hard instances. Finally, by analyzing attention weights, we obtain important
insights into the mechanisms by which transformers recognize humor. Most
remarkably, we find clear evidence that one single attention head learns to
recognize the words that make a test sentence humorous, even without access to
this information at training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09395">
<div class="article-summary-box-inner">
<span><p>Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing large-scale
KGs. Over the course of its development, supervision has been considered
necessary for accurate alignments. Inspired by the recent progress of
self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Existing supervised methods for this task
focus on pulling each pair of positive (labeled) entities close to each other.
However, our analysis suggests that the learning of entity alignment can
actually benefit more from pushing sampled (unlabeled) negatives far away than
pulling positive aligned pairs close. We present SelfKG by leveraging this
discovery to design a contrastive learning strategy across two KGs. Extensive
experiments on benchmark datasets demonstrate that SelfKG without supervision
can match or achieve comparable results with state-of-the-art supervised
baselines. The performance of SelfKG demonstrates self-supervised learning
offers great potential for entity alignment in KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01387">
<div class="article-summary-box-inner">
<span><p>We present InferWiki, a Knowledge Graph Completion (KGC) dataset that
improves upon existing benchmarks in inferential ability, assumptions, and
patterns. First, each testing sample is predictable with supportive data in the
training set. To ensure it, we propose to utilize rule-guided train/test
generation, instead of conventional random split. Second, InferWiki initiates
the evaluation following the open-world assumption and improves the inferential
difficulty of the closed-world assumption, by providing manually annotated
negative and unknown triples. Third, we include various inference patterns
(e.g., reasoning path length and types) for comprehensive evaluation. In
experiments, we curate two settings of InferWiki varying in sizes and
structures, and apply the construction process on CoDEx as comparative
datasets. The results and empirical analyses demonstrate the necessity and
high-quality of InferWiki. Nevertheless, the performance gap among various
inferential assumptions and patterns presents the difficulty and inspires
future research direction. Our datasets can be found in
https://github.com/TaoMiner/inferwiki
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09193">
<div class="article-summary-box-inner">
<span><p>Transformer has achieved great success in NLP. However, the quadratic
complexity of the self-attention mechanism in Transformer makes it inefficient
in handling long sequences. Many existing works explore to accelerate
Transformers by computing sparse self-attention instead of a dense one, which
usually attends to tokens at certain positions or randomly selected tokens.
However, manually selected or random tokens may be uninformative for context
modeling. In this paper, we propose Smart Bird, which is an efficient and
effective Transformer with learnable sparse attention. In Smart Bird, we first
compute a sketched attention matrix with a single-head low-dimensional
Transformer, which aims to find potential important interactions between
tokens. We then sample token pairs based on their probability scores derived
from the sketched attention matrix to generate different sparse attention index
matrices for different attention heads. Finally, we select token embeddings
according to the index matrices to form the input of sparse attention networks.
Extensive experiments on six benchmark datasets for different tasks validate
the efficiency and effectiveness of Smart Bird in text modeling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-26 23:09:04.779504661 UTC">2021-08-26 23:09:04 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>